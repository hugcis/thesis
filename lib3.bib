
@article{allenEvolutionEmergenceLearning2003,
  title = {Evolution, Emergence, and Learning in Complex Systems},
  author = {Allen, Peter M. and Strathern, Mark},
  year = {2003},
  journal = {Emergence},
  volume = {5},
  number = {4},
  pages = {8--33},
  publisher = {{Taylor \& Francis}},
  file = {/Users/hugo/Papers/pdf/allenEvolutionEmergenceLearning2003.pdf;/Users/hugo/Zotero/storage/J9DGTJFY/s15327000em0504_4.html}
}

@article{babsonReservoirComputingComplex2019,
  title = {Reservoir {{Computing}} with {{Complex Cellular Automata}}},
  author = {Babson, Neil and Teuscher, Christof and {Portland State University}},
  year = {2019},
  month = dec,
  journal = {Complex Systems},
  volume = {28},
  number = {4},
  pages = {433--455},
  issn = {08912513},
  doi = {10.25088/ComplexSystems.28.4.433},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/babsonReservoirComputingComplex22.pdf}
}

@article{bengioLearningLongtermDependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  year = {1994},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  issn = {1941-0093},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{$<>$}},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,Intelligent networks,Neural networks,Neurofeedback,Production,Recurrent neural networks},
  file = {/Users/hugo/Papers/pdf/bengioLearningLongtermDependencies1994.pdf;/Users/hugo/Zotero/storage/GK3B286S/279181.html}
}

@incollection{bennettLogicalDepthPhysical1995,
  title = {Logical {{Depth}} and {{Physical Complexity}}},
  booktitle = {The {{Universal Turing Machine A Half-Century Survey}}},
  author = {Bennett, Charles H.},
  editor = {Herken, Rolf and Herken, Rolf},
  year = {1995},
  volume = {2},
  pages = {207--235},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  doi = {10.1007/978-3-7091-6597-3_8},
  abstract = {Some mathematical and natural objects (a random sequence, a sequence of zeros, a perfect crystal, a gas) are intuitively trivial, while others (e.g. the human body, the digits of {$\pi$}) contain internal evidence of a nontrivial causal history.},
  isbn = {978-3-211-82637-9 978-3-7091-6597-3},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/EJRQMHI8/bennettLogicalDepthPhysical1995.pdf}
}

@inproceedings{blitzerDomainAdaptationStructural2006,
  title = {Domain {{Adaptation}} with {{Structural Correspondence Learning}}},
  booktitle = {Proceedings of the 2006 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Blitzer, John and McDonald, Ryan and Pereira, Fernando},
  year = {2006},
  month = jul,
  pages = {120--128},
  publisher = {{Association for Computational Linguistics}},
  address = {{Sydney, Australia}},
  file = {/Users/hugo/Papers/pdf/blitzerDomainAdaptationStructural2006.pdf}
}

@book{boccaraModelingComplexSystems2010,
  title = {Modeling {{Complex Systems}}},
  author = {Boccara, Nino},
  year = {2010},
  month = sep,
  publisher = {{Springer Science \& Business Media}},
  abstract = {The essential points of this ?rst chapter are \textbullet{} The de?nition of a complex system \textbullet{} The notion of emergence \textbullet{} The de?nition of a model \textbullet{} The notion of dynamical system This book is about the dynamics of complex systems. Roughly speaking, a system is a collection of interacting elements making up a whole such as, for instance, a mechanical clock. While many systems may be quite complicated, they are not necessarily considered to be complex. Today, most authors agree on the essential properties a system has to possess to be called complex. The ?rst section is devoted to the description of these properties. To interpret the time evolution of a system, scientists build up models, which are simpli?ed mathematical representations of the system. The exact purpose of a model and what its essential features should be is explained in the second section. The mathematical models that will be discussed in this book are dynami- 1 calsystems. A dynamical system is essentially a set of equations whose- lutiondescribesthe evolution,asafunction oftime, ofthe state ofthe system. There exist di?erent types of dynamical systems. Some of them are de?ned in the third section. 1 There is an extensive literature on mathematical modeling. The reader may, for example, consult [11,88,163,233].},
  googlebooks = {boUorPmcbKMC},
  isbn = {978-1-4419-6562-2},
  langid = {english},
  keywords = {Computers / Computer Science,Computers / Computer Simulation,Computers / Programming / Algorithms,Computers / Software Development \& Engineering / General,Mathematics / Applied,Mathematics / Counting \& Numeration,Mathematics / Numerical Analysis,Science / Physics / General,Science / Physics / Mathematical \& Computational}
}

@article{bottouOptimizationMethodsLargescale2018,
  title = {Optimization Methods for Large-Scale Machine Learning},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  journal = {Siam Review},
  volume = {60},
  number = {2},
  pages = {223--311},
  publisher = {{SIAM}},
  file = {/Users/hugo/Papers/pdf/bottouOptimizationMethodsLargescale2018.pdf;/Users/hugo/Zotero/storage/5A4XZPJM/16M1080173.html}
}

@article{brownClassbasedNgramModels1992,
  title = {Class-Based {\emph{n}}-Gram Models of Natural Language},
  author = {Brown, Peter F. and {deSouza}, Peter V. and Mercer, Robert L. and Pietra, Vincent J. Della and Lai, Jenifer C.},
  year = {1992},
  month = dec,
  journal = {Computational Linguistics},
  volume = {18},
  number = {4},
  pages = {467--479},
  issn = {0891-2017},
  abstract = {We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.}
}

@article{buckmanSampleefficientReinforcementLearning2018,
  title = {Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion},
  author = {Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  file = {/Users/hugo/Papers/pdf/buckmanSampleefficientReinforcementLearning2018.pdf;/Users/hugo/Zotero/storage/IDBNPPRQ/f02208a057804ee16ac72ff4d3cec53b-Abstract.html}
}

@inproceedings{caronUnsupervisedLearningVisual2020,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year = {2020},
  eprint = {2006.09882},
  eprinttype = {arxiv},
  address = {{Vancouver, Canada}},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/caronUnsupervisedLearningVisual2021.pdf;/Users/hugo/Zotero/storage/FKQB8CIB/2006.html}
}

@article{chenEmpiricalStudySmoothing1996,
  title = {An {{Empirical Study}} of {{Smoothing Techniques}} for {{Language Modeling}}},
  author = {Chen, Stanley F. and Goodman, Joshua T.},
  year = {1996},
  month = jun,
  journal = {arXiv:cmp-lg/9606011},
  eprint = {cmp-lg/9606011},
  eprinttype = {arxiv},
  abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Zotero/storage/B6QQK2AD/Chen and Goodman - 1996 - An Empirical Study of Smoothing Techniques for Lan.pdf}
}

@inproceedings{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  eprint = {2002.05709},
  eprinttype = {arxiv},
  address = {{Vienna, Austria}},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/chenSimpleFrameworkContrastive2020.pdf;/Users/hugo/Zotero/storage/HZQM6BVB/2002.html}
}

@article{chevalier-boisvertBabyaiPlatformStudy2018,
  title = {Babyai: {{A}} Platform to Study the Sample Efficiency of Grounded Language Learning},
  shorttitle = {Babyai},
  author = {{Chevalier-Boisvert}, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.08272},
  eprint = {1810.08272},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/chevalier-boisvertBabyaiPlatformStudy2018.pdf;/Users/hugo/Zotero/storage/QDXNSPSG/1810.html}
}

@inproceedings{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}\textendash{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of {{SSST-8}}, {{Eighth Workshop}} on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, Kyunghyun and {van Merri{\"e}nboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  pages = {103--111},
  file = {/Users/hugo/Papers/pdf/choPropertiesNeuralMachine2014.pdf}
}

@inproceedings{cisnerosEvolvingStructuresComplex2019,
  title = {Evolving {{Structures}} in {{Complex Systems}}},
  booktitle = {2019 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Cisneros, Hugo and Sivic, Josef and Mikolov, Tomas},
  year = {2019},
  month = dec,
  pages = {230--237},
  publisher = {{IEEE}},
  address = {{Xiamen, China}},
  doi = {10.1109/SSCI44817.2019.9002840},
  isbn = {978-1-72812-485-8},
  file = {/Users/hugo/Papers/pdf/cisnerosEvolvingStructuresComplex2019.pdf}
}

@inproceedings{cisnerosVisualizingComputationLargescale2020,
  title = {Visualizing Computation in Large-Scale Cellular Automata},
  booktitle = {Artificial {{Life Conference Proceedings}}},
  author = {Cisneros, Hugo and Sivic, Josef and Mikolov, Tomas},
  year = {2020},
  month = jul,
  volume = {32},
  pages = {239--247},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00277},
  abstract = {Emergent processes in complex systems such as cellular automata can perform computations of increasing complexity, and could possibly lead to artificial evolution. Such a feat would require scaling up current simulation sizes to allow for enough computational capacity. Understanding complex computations happening in cellular automata and other systems capable of emergence poses many challenges, especially in large-scale systems. We propose methods for coarse-graining cellular automata based on frequency analysis of cell states, clustering and autoencoders. These innovative techniques facilitate the discovery of large-scale structure formation and complexity analysis in those systems. They emphasize interesting behaviors in elementary cellular automata while filtering out background patterns. Moreover, our methods reduce large 2D automata to smaller sizes and enable identifying systems that behave interestingly at multiple scales.},
  file = {/Users/hugo/Papers/pdf/cisnerosVisualizingComputationLargescale2020.pdf;/Users/hugo/Zotero/storage/I4WSGJAQ/isal_a_00277.html}
}

@article{cookUniversalityElementaryCellular2004,
  title = {Universality in {{Elementary Cellular Automata}}},
  author = {Cook, Matthew},
  year = {2004},
  journal = {Complex Systems},
  pages = {40},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/9ACRWMAX/cookUniversalityElementaryCellular2004.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals, and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  langid = {english}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  year = {1990},
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402\_1},
  file = {/Users/hugo/Papers/pdf/elmanFindingStructureTime1990.pdf;/Users/hugo/Zotero/storage/FRIQ94YD/s15516709cog1402_1.html}
}

@article{gallicchioFrontiersReservoirComputing2020,
  title = {Frontiers in {{Reservoir Computing}}},
  author = {Gallicchio, Claudio and Luko{\v s}evi{\v c}ius, Mantas and Scardapane, Simone},
  year = {2020},
  journal = {Computational Intelligence},
  pages = {8},
  abstract = {Reservoir computing (RC) studies the properties of large recurrent networks of artificial neurons, with either fixed or random connectivity. Over the last years, reservoirs have become a key tool for pattern recognition and neuroscience problems, being able to develop a rich representation of the temporal information even if left untrained. The common paradigm has been instantiated into several models, among which the Echo State Network and the Liquid State Machine represent the most widely known ones. Nowadays, RC represents the de facto state-of-the-art approach for efficient learning in the temporal domain. Besides, theoretical studies in RC area can contribute to the broader field of Recurrent Neural Networks research by enabling a deeper understanding of the fundamental capabilities of dynamical recurrent models, even in the absence of training of the recurrent connections. RC paradigm also allows using different dynamical systems, including hardware, for computation.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/ESEJ7FNJ/Gallicchio et al. - 2020 - Frontiers in Reservoir Computing.pdf}
}

@article{gardnerMathematicalGames1970,
  title = {Mathematical {{Games}}},
  author = {Gardner, Martin},
  year = {1970},
  month = oct,
  journal = {Scientific American},
  volume = {223},
  number = {4},
  pages = {120--123},
  issn = {0036-8733},
  doi = {10.1038/scientificamerican1070-120},
  file = {/Users/hugo/Papers/pdf/gardnerMathematicalGames1970.pdf}
}

@article{gepperthBioInspiredIncrementalLearning2016,
  title = {A {{Bio-Inspired Incremental Learning Architecture}} for {{Applied Perceptual Problems}}},
  author = {Gepperth, Alexander and Karaoguz, Cem},
  year = {2016},
  month = oct,
  journal = {Cognitive Computation},
  volume = {8},
  number = {5},
  pages = {924--934},
  issn = {1866-9964},
  doi = {10.1007/s12559-016-9389-5},
  abstract = {We present a biologically inspired architecture for incremental learning that remains resource-efficient even in the face of very high data dimensionalities ({$>$}1000) that are typically associated with perceptual problems. In particular, we investigate how a new perceptual (object) class can be added to a trained architecture without retraining, while avoiding the well-known catastrophic forgetting effects typically associated with such scenarios. At the heart of the presented architecture lies a generative description of the perceptual space by a self-organized approach which at the same time approximates the neighborhood relations in this space on a two-dimensional plane. This approximation, which closely imitates the topographic organization of the visual cortex, allows an efficient local update rule for incremental learning even in the face of very high dimensionalities, which we demonstrate by tests on the well-known MNIST benchmark. We complement the model by adding a biologically plausible short-term memory system, allowing it to retain excellent classification accuracy even under incremental learning in progress. The short-term memory is additionally used to reinforce new data statistics by replaying previously stored samples during dedicated ``sleep'' phases.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/gepperthBioInspiredIncrementalLearning2016.pdf}
}

@article{gilpinCellularAutomataConvolutional2018,
  title = {Cellular Automata as Convolutional Neural Networks},
  author = {Gilpin, William},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.02942 [cond-mat, physics:nlin, physics:physics]},
  eprint = {1809.02942},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:nlin, physics:physics},
  abstract = {Deep learning techniques have recently demonstrated broad success in predicting complex dynamical systems ranging from turbulence to human speech, motivating broader questions about how neural networks encode and represent dynamical rules. We explore this problem in the context of cellular automata (CA), simple dynamical systems that are intrinsically discrete and thus difficult to analyze using standard tools from dynamical systems theory. We show that any CA may readily be represented using a convolutional neural network with a network-in-network architecture. This motivates our development of a general convolutional multilayer perceptron architecture, which we find can learn the dynamical rules for arbitrary CA when given videos of the CA as training data. In the limit of large network widths, we find that training dynamics are strongly stereotyped across replicates, and that common patterns emerge in the structure of networks trained on different CA rulesets. We train ensembles of networks on randomly-sampled CA, and we probe how the trained networks internally represent the CA rules using an information-theoretic technique based on distributions of layer activation patterns. We find that CA with simpler rule tables produce trained networks with hierarchical structure and layer specialization, while more complex CA tend to produce shallower representations---illustrating how the underlying complexity of the CA's rules influences the specificity of these internal representations. Our results suggest how the entropy of a physical process can affect its representation when learned by neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Cellular Automata and Lattice Gases,Physics - Computational Physics},
  file = {/Users/hugo/Papers/pdf/gilpinCellularAutomataConvolutional22.pdf}
}

@inproceedings{gloverDynamicalLandscapeReservoir2021,
  title = {The {{Dynamical Landscape}} of {{Reservoir Computing}} with {{Elementary Cellular Automata}}},
  booktitle = {{{ALIFE}} 2021: {{The}} 2021 {{Conference}} on {{Artificial Life}}},
  author = {Glover, Tom Eivind and Lind, Pedro and Yazidi, Anis and Osipov, Evgeny and Nichele, Stefano},
  year = {2021},
  publisher = {{MIT Press}}
}

@article{goldsteinEmergenceComplexSystems2011,
  title = {Emergence in Complex Systems},
  author = {Goldstein, Jeffrey},
  year = {2011},
  journal = {The sage handbook of complexity and management},
  pages = {65--78},
  publisher = {{Sage Thousand Oaks, CA}},
  file = {/Users/hugo/Papers/pdf/goldsteinEmergenceComplexSystems2011.pdf;/Users/hugo/Zotero/storage/RLWFB2QL/books.html}
}

@inproceedings{goodfellowEmpiricalInvestigationCatastrophic2014,
  title = {An {{Empirical Investigation}} of {{Catastrophic Forgeting}} in {{Gradient-Based Neural Networks}}},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2014, {{Banff}}, {{AB}}, {{Canada}}, {{April}} 14-16, 2014, {{Conference Track Proceedings}}},
  author = {Goodfellow, Ian J. and Mirza, Mehdi and Da, Xia and Courville, Aaron C. and Bengio, Yoshua},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014}
}

@article{gravesNeuralTuringMachines2014,
  title = {Neural Turing Machines},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  journal = {arXiv preprint arXiv:1410.5401},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Zotero/storage/K8996JNR/1410.html}
}

@inproceedings{grillBootstrapYourOwn2020,
  title = {Bootstrap {{Your Own Latent A New Approach}} to {{Self-Supervised Learning}}},
  booktitle = {Proceedings of the 34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  pages = {14},
  address = {{Vancouver, Canada}},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/N3GQZLNJ/Grill et al. - Bootstrap Your Own Latent A New Approach to Self-S.pdf}
}

@article{hansonAttractorbasinPortraitCellular1992,
  title = {The Attractor-Basin Portrait of a Cellular Automaton},
  shorttitle = {The Attractor?},
  author = {Hanson, James E. and Crutchfield, James P.},
  year = {1992},
  month = mar,
  journal = {Journal of Statistical Physics},
  volume = {66},
  number = {5-6},
  pages = {1415--1462},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/BF01054429},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hansonAttractorbasinPortraitCellular1992.pdf}
}

@article{hansonComputationalMechanicsCellular1997,
  title = {Computational Mechanics of Cellular Automata: {{An}} Example},
  shorttitle = {Computational Mechanics of Cellular Automata},
  author = {Hanson, James E. and Crutchfield, James P.},
  year = {1997},
  month = apr,
  journal = {Physica D: Nonlinear Phenomena},
  series = {Lattice {{Dynamics}}},
  volume = {103},
  number = {1},
  pages = {169--189},
  issn = {0167-2789},
  doi = {10.1016/S0167-2789(96)00259-X},
  abstract = {We illustrate and extend the techniques of computational mechanics in explicating the structures that emerge in the space-time behavior of elementary one-dimensional cellular automaton rule 54. The dominant regular domain of the cellular automation is identified and a domain filter is constructed to locate and classify defects in the domain. The primary particles are identified and a range of interparticle interactions is studied. The deterministic equation of motion of the filtered space-time behavior is derived. Filters of increasing sophistication are constructed for the efficient gathering of particle statistics and for the identification of higher-level defects, particle interactions, and secondary domains. We define the emergence time at which the space-time behavior condenses into configurations consisting only of domains, particles, and particle interactions. Taken together, these techniques serve as the basis for the investigation of pattern evolution and self-organization in this representative system.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hansonComputationalMechanicsCellular1997.pdf;/Users/hugo/Zotero/storage/BDFIKY33/S016727899600259X.html}
}

@inproceedings{hintonUsingFastWeights1987,
  title = {Using Fast Weights to Deblur Old Memories},
  booktitle = {Proceedings of the Ninth Annual Conference of the {{Cognitive Science Society}}},
  author = {Hinton, Geoffrey E. and Plaut, David C.},
  year = {1987},
  pages = {177--186},
  file = {/Users/hugo/Papers/pdf/hintonUsingFastWeights1987.pdf;/Users/hugo/Zotero/storage/L95ZXCVI/books.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/hugo/Papers/pdf/hochreiterLongShortTermMemory1997.pdf}
}

@article{jaegerEchoStateApproach2001,
  title = {The ``Echo State'' Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note},
  author = {Jaeger, Herbert},
  year = {2001},
  journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  volume = {148},
  number = {34},
  pages = {13},
  publisher = {{Bonn}},
  file = {/Users/hugo/Papers/pdf/jaegerEchoStateApproach2001.pdf}
}

@techreport{jaegerLongShortTermMemory2012,
  title = {Long {{Short-Term Memory}} in {{Echo State Networks}}: {{Details}} of a {{Simulation Study}}},
  shorttitle = {Long {{Short-Term Memory}} in {{Echo State Networks}}},
  author = {Jaeger, Herbert},
  year = {2012},
  month = feb,
  institution = {{Jacobs University Bremen}},
  abstract = {Echo State Networks (ESNs) is an approach to design and train recurrent neural networks in supervised learning tasks. An important objective in many such tasks is to learn to exploit long-time dependencies in the processed signals ("long short-term memory" performance). Here we expose ESNs to a series of synthetic benchmark tasks that have been used in the literature to study the learnability of long-range temporal dependencies. This report provides all the detail necessary to replicate these experiments. It is intended to serve as the technical companion to a journal submission paper where the findings are analysed and compared to results obtained elsewhere with other learning paradigms.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jaegerLongShortTermMemory2012.pdf;/Users/hugo/Zotero/storage/FSWQAVUD/638.html}
}

@book{jaegerTutorialTrainingRecurrent2002,
  title = {Tutorial on Training Recurrent Neural Networks, Covering {{BPPT}}, {{RTRL}}, {{EKF}} and the" Echo State Network" Approach},
  author = {Jaeger, Herbert},
  year = {2002},
  volume = {5},
  publisher = {{GMD-Forschungszentrum Informationstechnik Bonn}},
  file = {/Users/hugo/Papers/pdf/jaegerTutorialTrainingRecurrent2002.pdf}
}

@inproceedings{joulinInferringAlgorithmicPatterns2015,
  title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = dec,
  series = {{{NIPS}}'15},
  pages = {190--198},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
  file = {/Users/hugo/Papers/pdf/joulinInferringAlgorithmicPatterns22.pdf}
}

@article{kanazawaGeneralIntelligenceDomainspecific2004,
  title = {General Intelligence as a Domain-Specific Adaptation.},
  author = {Kanazawa, Satoshi},
  year = {2004},
  journal = {Psychological review},
  volume = {111},
  number = {2},
  pages = {512},
  publisher = {{American Psychological Association}},
  file = {/Users/hugo/Papers/pdf/kanazawaGeneralIntelligenceDomainspecific2004.pdf;/Users/hugo/Zotero/storage/S7FXKD8B/2004-12248-010.html}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015}
}

@article{kleykoCellularAutomataCan2020,
  title = {Cellular {{Automata Can Reduce Memory Requirements}} of {{Collective-State Computing}}},
  author = {Kleyko, Denis and Frady, E. Paxon and Sommer, Friedrich T.},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.03585 [cs]},
  eprint = {2010.03585},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Various non-classical approaches of distributed information processing, such as neural networks, computation with Ising models, reservoir computing, vector symbolic architectures, and others, employ the principle of collective-state computing. In this type of computing, the variables relevant in a computation are superimposed into a single high-dimensional state vector, the collective-state. The variable encoding uses a fixed set of random patterns, which has to be stored and kept available during the computation. Here we show that an elementary cellular automaton with rule 90 (CA90) enables space-time tradeoff for collective-state computing models that use random dense binary representations, i.e., memory requirements can be traded off with computation running CA90. We investigate the randomization behavior of CA90, in particular, the relation between the length of the randomization period and the size of the grid, and how CA90 preserves similarity in the presence of the initialization noise. Based on these analyses we discuss how to optimize a collective-state computing model, in which CA90 expands representations on the fly from short seed patterns - rather than storing the full set of random patterns. The CA90 expansion is applied and tested in concrete scenarios using reservoir computing and vector symbolic architectures. Our experimental results show that collective-state computing with CA90 expansion performs similarly compared to traditional collective-state models, in which random patterns are generated initially by a pseudo-random number generator and then stored in a large memory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/kleykoCellularAutomataCan2020.pdf;/Users/hugo/Zotero/storage/6LA4U5ED/2010.html}
}

@inproceedings{kobayashiContinualLearningExploiting2019,
  title = {Continual {{Learning Exploiting Structure}} of {{Fractal Reservoir Computing}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2019: {{Workshop}} and {{Special Sessions}}},
  author = {Kobayashi, Taisuke and Sugino, Toshiki},
  editor = {Tetko, Igor V. and K{\r{u}}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {35--47},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30493-5_4},
  abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be fired corresponding to each task, since only readout weights are updated according to the degree of firing of neurons. We therefore propose the way to design reservoir computing such that the firing neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely different networks.},
  isbn = {978-3-030-30493-5},
  langid = {english},
  keywords = {Continual learning,Fractal network,Reservoir computing},
  file = {/Users/hugo/Papers/pdf/kobayashiContinualLearningExploiting2019.pdf}
}

@article{koppelAlmostMachineindependentTheory1991,
  title = {An Almost Machine-Independent Theory of Program-Length Complexity, Sophistication, and Induction},
  author = {Koppel, Moshe and Atlan, Henri},
  year = {1991},
  month = aug,
  journal = {Information Sciences},
  volume = {56},
  number = {1},
  pages = {23--33},
  issn = {0020-0255},
  doi = {10.1016/0020-0255(91)90021-L},
  abstract = {The purpose of this paper is to use a variant of program-length complexity to formally define the structure of a binary string, where the structure of an object is taken to mean the aggregate of its projectible properties.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/4B8K5XSI/002002559190021L.html}
}

@techreport{krizhevskyLearningMultipleLayers2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  year = {2009},
  institution = {{University of Toronto}},
  file = {/Users/hugo/Papers/pdf/krizhevskyLearningMultipleLayers2009.pdf}
}

@article{litjensSurveyDeepLearning2017,
  title = {A Survey on Deep Learning in Medical Image Analysis},
  author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and {van der Laak}, Jeroen A. W. M. and {van Ginneken}, Bram and S{\'a}nchez, Clara I.},
  year = {2017},
  month = dec,
  journal = {Medical Image Analysis},
  volume = {42},
  pages = {60--88},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.07.005},
  abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
  langid = {english},
  keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
  file = {/Users/hugo/Papers/pdf/litjensSurveyDeepLearning2017.pdf;/Users/hugo/Zotero/storage/Z6FVT7B2/S1361841517301135.html}
}

@inproceedings{maasLearningWordVectors2011,
  title = {Learning {{Word Vectors}} for {{Sentiment Analysis}}},
  booktitle = {The 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Proceedings}} of the {{Conference}}, 19-24 {{June}}, 2011, {{Portland}}, {{Oregon}}, {{USA}}},
  author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  editor = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
  year = {2011},
  pages = {142--150},
  publisher = {{The Association for Computer Linguistics}}
}

@article{maassRealTimeComputingStable2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  journal = {Neural Computation},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/maassRealTimeComputingStable2002.pdf}
}

@techreport{margemExperimentalStudyCellular2017,
  title = {An Experimental Study on Cellular Automata Reservoir in Pathological Sequence Learning Tasks},
  author = {Margem, Mrwan and Yilmaz, Ozg{\"u}r},
  year = {2017},
  institution = {{Jul}},
  abstract = {In this study, we are providing a recurrent architecture based on Cellular Automata state evolution. The states of the cells are used as the reservoir of activities as in Echo State Networks. The projection of the input onto this reservoir medium provides a systematic way of remembering previous inputs and combining the memory with a continuous stream of inputs. This is an essential capability for a wide collection of intelligence tasks such as language, continuous vision (i.e. video), symbolic manipulation in a knowledge base etc. We devise intuitive methods for enlarging the memory capacity of the cellular autmata state space and reducing the interference between memory traces. Cellular Automata reservoir constructs a novel bridge between computational theory of automata and neural architectures. The proposed framework is tested on classical synthetic pathological tasks that are widely used in evaluating recurrent algorithms. We show that the proposed algorithm achieves zero error in all tasks, giving a similar performance with Echo State Networks, but even better in many different aspects. Two other methods are also introduced to training recurrent neural networks; ``Covariance representation'' that has second order attribute statistics and ``Stack representation'' that has a local representation, and compare them with Cellular Automata based Reservoir Comuting framework that has higher attribute statistics and distributed representation. This raises the question of whether real valued neuron units are essential for solving complex problems that are distributed over time. Our results suggest that, even very sparsely connected binary units with simple computational rules can provide the required computation for intelligent behavior.}
}

@article{margemReservoirComputingBased2018,
  title = {Reservoir {{Computing}} Based on {{Cellular Automata}} ({{ReCA}}) in {{Sequence Learning}}},
  author = {Margem, Mrwan and Gedik, Osman},
  year = {2018},
  month = dec,
  journal = {Journal of cellular automata},
  volume = {14},
  pages = {153--170},
  abstract = {ReCA is a reservoir computing architecture based on cellular automata in which the inputs pass on a cellular automaton instead of a recurrent neural network reservoir. ReCA has been tested using pathological synthetic sequence tasks (well-known benchmark tasks within the reservoir computing (RC) community) and has been showing promising results that reduce complexity compared with other RC approaches such as echo state networks (ESNs). In this paper, a number of methods for feature extraction from the cellular automata (CA) reservoir are introduced to improve ReCA by reducing its complexity while maintaining accuracy. The proposed method reduces the feature dimension by using a few states from every time step (EACH) in the reservoir and/or using only one side of the CA evolution (HALF) and/or reducing the CA evolution in space (expansion ratio {$f$}). Due to the rich dynamics of the CA reservoir, the three methods of reduction (EACH, HALF, and {$f$}) can be used together to reduce the feature dimension by up to 98\% in some pathological tasks compared with the state-of-the-art ReCA results.}
}

@inproceedings{mcdonaldReservoirComputingExtreme2017a,
  title = {Reservoir Computing \& Extreme Learning Machines Using Pairs of Cellular Automata Rules},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {McDonald, Nathan},
  year = {2017},
  month = may,
  pages = {2429--2436},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966151},
  abstract = {A framework for implementing reservoir computing (RC) and extreme learning machines (ELMs), two types of artificial neural networks, based on 1D elementary Cellular Automata (CA) is presented, in which two separate CA rules explicitly implement the minimum computational requirements of the reservoir layer: hyperdimensional projection and short-term memory. CAs are cell-based state machines, which evolve in time in accordance with local rules based on a cell's current state and those of its neighbors. Notably, simple single cell shift rules as the memory rule in a fixed edge CA afforded reasonable success in conjunction with a variety of projection rules, potentially significantly reducing the optimal solution search space. Optimal iteration counts for the CA rule pairs can be estimated for some tasks based upon the category of the projection rule. Initial results support future hardware realization, where CAs potentially afford orders of magnitude reduction in size, weight, and power (SWaP) requirements compared with floating point RC implementations.},
  keywords = {Automata,cellular automata (CA),cellular automata based reservoirs (ReCA),Chaos,Encoding,extreme learning machine (ELM),Hardware,Learning automata,Neurons,reservoir computing (RC),Reservoirs},
  file = {/Users/hugo/Zotero/storage/R5A4KXJ7/7966151.html}
}

@inproceedings{mikolovDistributedRepresentationsWords2013a,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = dec,
  series = {{{NIPS}}'13},
  pages = {3111--3119},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}
}

@article{millidgePredictiveCodingApproximates2020,
  title = {Predictive {{Coding Approximates Backprop}} along {{Arbitrary Computation Graphs}}},
  author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  year = {2020},
  month = oct,
  journal = {arXiv:2006.04182 [cs]},
  eprint = {2006.04182},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayerperceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Zotero/storage/YVSFMHP3/Millidge et al. - 2020 - Predictive Coding Approximates Backprop along Arbi.pdf}
}

@book{minskyPerceptronsIntroductionComputational1972,
  title = {Perceptrons: {{An}} Introduction to Computational Geometry},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour A.},
  year = {1972},
  edition = {2nd edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge MA}},
  doi = {10.7551/mitpress/11301.001.0001},
  abstract = {The first systematic study of parallelism in computation by two pioneers in the field},
  isbn = {0-262-63022-2},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/L6AGHT8Q/PerceptronsAn-Introduction-to-Computational.html}
}

@inproceedings{mitchellEvolvingCellularAutomata1996,
  title = {Evolving {{Cellular Automata}} with {{Genetic Algorithms}}: {{A Review}} of {{Recent Work}}},
  booktitle = {Proceedings of the {{First International Conference}} on {{Evolutionary Computation}} and {{Its Applications}}},
  author = {Mitchell, Melanie and Road, Hyde Park and Das, Rajarshi and Box, P O},
  year = {1996},
  pages = {14},
  abstract = {We review recent work done by our group on applying genetic algorithms (GAs) to the design of cellular automata (CAs) that can perform computations requiring global coordination. A GA was used to evolve CAs for two computational tasks: density classi cation and synchronization. In both cases, the GA discovered rules that gave rise to sophisticated emergent computational strategies. These strategies can be analyzed using a \textbackslash computational mechanics" framework in which \textbackslash particles" carry information and interactions between particles e ects information processing. This framework can also be used to explain the process by which the strategies were designed by the GA. The work described here is a rst step in employing GAs to engineer useful emergent computation in decentralized multi-processor systems. It is also a rst step in understanding how an evolutionary process can produce complex systems with sophisticated collective computational abilities.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mitchellEvolvingCellularAutomata3.pdf}
}

@article{moranReservoirComputingHardware2018,
  title = {Reservoir {{Computing Hardware}} with {{Cellular Automata}}},
  author = {Mor{\'a}n, Alejandro and Frasser, Christiam F. and Rossell{\'o}, Josep L.},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.04932 [nlin]},
  eprint = {1806.04932},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  abstract = {Elementary cellular automata (ECA) is a widely studied one-dimensional processing methodology where the successive iteration of the automaton may lead to the recreation of a rich pattern dynamic. Recently, cellular automata have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automata rule is fixed and the training is performed using a linear regression. In this work we perform an exhaustive study of the performance of the different ECA rules when applied to pattern recognition of time-independent input signals using a RC scheme. Once the different ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates and shift-registers, thus representing a high-performance alternative for RC hardware implementation in terms of processing time, circuit area, power dissipation and system accuracy. The model (both in software and its hardware implementation) has been tested using a pattern recognition task of handwritten numbers (the MNIST database) for which we obtained competitive results in terms of accuracy, speed and power dissipation. The proposed model can be considered to be a low-cost method to implement fast pattern recognition digital circuits.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/moranReservoirComputingHardware22.pdf}
}

@article{mordvintsevGrowingNeuralCellular2020,
  title = {Growing {{Neural Cellular Automata}}},
  author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
  year = {2020},
  month = feb,
  journal = {Distill},
  volume = {5},
  number = {2},
  pages = {e23},
  issn = {2476-0757},
  doi = {10.23915/distill.00023},
  abstract = {Differentiable Self-Organisation: A Cellular Automata model of Morphogenesis.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/DQXWVUX2/growing-ca.html}
}

@article{natschlagerLiquidComputerNovel2002,
  title = {The" Liquid Computer": {{A}} Novel Strategy for Real-Time Computing on Time Series},
  shorttitle = {The" Liquid Computer"},
  author = {Natschl{\"a}ger, Thomas and Maass, Wolfgang and Markram, Henry},
  year = {2002},
  journal = {Special issue on Foundations of Information Processing of TELEMATIK},
  volume = {8},
  number = {ARTICLE},
  pages = {39--43},
  file = {/Users/hugo/Zotero/storage/BVQ9BAUI/117806.html}
}

@inproceedings{ngSpectralClusteringAnalysis2001,
  title = {On Spectral Clustering: Analysis and an Algorithm},
  shorttitle = {On Spectral Clustering},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Neural Information Processing Systems}}: {{Natural}} and {{Synthetic}}},
  author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
  year = {2001},
  month = jan,
  series = {{{NIPS}}'01},
  pages = {849--856},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {Despite many empirical successes of spectral clustering methods\textemdash{} algorithms that cluster points using eigenvectors of matrices derived from the data\textemdash there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.}
}

@article{nguyenVariationalContinualLearning2017,
  title = {Variational {{Continual Learning}}},
  author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
  year = {2017},
  journal = {CoRR},
  volume = {abs/1710.10628},
  eprint = {1710.10628},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{nicheleDeepLearningCellular2017,
  title = {Deep Learning with Cellular Automaton-Based Reservoir Computing},
  author = {Nichele, Stefano and Molund, Andreas},
  year = {2017},
  journal = {0891-2513},
  publisher = {{Complex Systems Publications Inc}},
  issn = {0891-2513},
  doi = {http://dx.doi.org/10.25088/ComplexSystems.26.4.319},
  abstract = {Recurrent neural networks (RNNs) have been a prominent concept wiithin artificial intelligence. They are inspired by biological neural net works (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the   more generic artificial neural networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech   recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, echo state Networks and liquid state machines have been proposed as   possible RNN alternatives, under the name of reservoir computing (RC). Reservoir computers are far easier to train. In this paper, cellular automata (CAs) are used as a reservoir and are tested on the five-bit memory task (a well known benchmark   within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata   and a recurrent architecture for handling the sequential aspects. Furthermore, a layered (deep) reservoir architecture   is proposed. Performances are compared to earlier work, in addition to the single-layer version. Results show that the single   cellular automaton (CA) reservoir system yields similar results to state-of-the-art work. The system comprised of two layered   reservoirs does show a noticeable improvement compared to a single CA reservoir. This work lays the foundation for   implementations of deep learning with CA-based reservoir systems.},
  langid = {english},
  annotation = {Accepted: 2018-05-09T07:58:01Z},
  file = {/Users/hugo/Papers/pdf/nicheleDeepLearningCellular2017.pdf;/Users/hugo/Zotero/storage/UMTI75FY/6164.html}
}

@article{nicheleDeepReservoirComputing2017,
  title = {Deep {{Reservoir Computing Using Cellular Automata}}},
  author = {Nichele, Stefano and Molund, Andreas},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.02806 [cs]},
  eprint = {1703.02806},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recurrent Neural Networks (RNNs) have been a prominent concept within artificial intelligence. They are inspired by Biological Neural Networks (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more generic Artificial Neural Networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, Echo State Networks and Liquid State Machines have been proposed as possible RNN alternatives, under the name of Reservoir Computing (RC). RCs are far more easy to train. In this paper, Cellular Automata are used as reservoir, and are tested on the 5-bit memory task (a well known benchmark within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata, and a recurrent architecture for handling the sequential aspects of it. Furthermore, a layered (deep) reservoir architecture is proposed. Performances are compared towards earlier work, in addition to its single-layer version. Results show that the single CA reservoir system yields similar results to state-of-the-art work. The system comprised of two layered reservoirs do show a noticeable improvement compared to a single CA reservoir. This indicates potential for further research and provides valuable insight on how to design CA reservoir systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/nicheleDeepReservoirComputing2017.pdf;/Users/hugo/Zotero/storage/SQT8XITZ/1703.html}
}

@article{nicheleReservoirComputingUsing2017,
  title = {Reservoir {{Computing Using Non-Uniform Binary Cellular Automata}}},
  author = {Nichele, Stefano and Gundersen, Magnus S.},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.03812 [cs]},
  eprint = {1702.03812},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The Reservoir Computing (RC) paradigm utilizes a dynamical system, i.e., a reservoir, and a linear classifier, i.e., a read-out layer, to process data from sequential classification tasks. In this paper the usage of Cellular Automata (CA) as a reservoir is investigated. The use of CA in RC has been showing promising results. In this paper, selected state-of-the-art experiments are reproduced. It is shown that some CA-rules perform better than others, and the reservoir performance is improved by increasing the size of the CA reservoir itself. In addition, the usage of parallel loosely coupled CA-reservoirs, where each reservoir has a different CA-rule, is investigated. The experiments performed on quasi-uniform CA reservoir provide valuable insights in CA reservoir design. The results herein show that some rules do not work well together, while other combinations work remarkably well. This suggests that non-uniform CA could represent a powerful tool for novel CA reservoir implementations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Emerging Technologies},
  file = {/Users/hugo/Papers/pdf/nicheleReservoirComputingUsing2017.pdf;/Users/hugo/Zotero/storage/D96INVWY/1702.html}
}

@article{rafayelyanLargescaleOpticalReservoir2020,
  title = {Large-Scale Optical Reservoir Computing for Spatiotemporal Chaotic Systems Prediction},
  author = {Rafayelyan, Mushegh and Dong, Jonathan and Tan, Yongqi and Krzakala, Florent and Gigan, Sylvain},
  year = {2020},
  journal = {Physical Review X},
  volume = {10},
  number = {4},
  pages = {041037},
  publisher = {{APS}},
  file = {/Users/hugo/Papers/pdf/rafayelyanLargescaleOpticalReservoir2020.pdf;/Users/hugo/Zotero/storage/A6G4TC86/PhysRevX.10.html}
}

@inproceedings{rebuffiIcarlIncrementalClassifier2017,
  title = {Icarl: {{Incremental}} Classifier and Representation Learning},
  shorttitle = {Icarl},
  booktitle = {Proceedings of the {{IEEE}} Conference on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
  year = {2017},
  pages = {2001--2010},
  file = {/Users/hugo/Papers/pdf/rebuffiIcarlIncrementalClassifier2017.pdf;/Users/hugo/Zotero/storage/JUTEPPXE/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.html}
}

@incollection{rendellTuringUniversalityGame2002,
  title = {Turing Universality of the Game of Life},
  booktitle = {Collision-Based Computing},
  author = {Rendell, Paul},
  year = {2002},
  pages = {513--539},
  publisher = {{Springer}},
  file = {/Users/hugo/Zotero/storage/54VJPFQ5/978-1-4471-0129-1_18.html}
}

@inproceedings{richardsonMCTestChallengeDataset2013,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open-Domain Machine Comprehension}} of {{Text}}},
  shorttitle = {{{MCTest}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Richardson, Matthew and Burges, Christopher J.C. and Renshaw, Erin},
  year = {2013},
  month = oct,
  pages = {193--203},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington, USA}},
  file = {/Users/hugo/Papers/pdf/richardsonMCTestChallengeDataset2013.pdf}
}

@inproceedings{richardsonProbingNaturalLanguage2020,
  title = {Probing Natural Language Inference Models through Semantic Fragments},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Richardson, Kyle and Hu, Hai and Moss, Lawrence and Sabharwal, Ashish},
  year = {2020},
  volume = {34},
  pages = {8713--8721},
  file = {/Users/hugo/Papers/pdf/richardsonProbingNaturalLanguage2020.pdf;/Users/hugo/Zotero/storage/HLZD7KJS/6397.html}
}

@inproceedings{robinsCatastrophicForgettingNeural1993a,
  title = {Catastrophic Forgetting in Neural Networks: The Role of Rehearsal Mechanisms},
  shorttitle = {Catastrophic Forgetting in Neural Networks},
  booktitle = {Proceedings 1993 {{The First New Zealand International Two-Stream Conference}} on {{Artificial Neural Networks}} and {{Expert Systems}}},
  author = {Robins, Anthony},
  year = {1993},
  pages = {65--68},
  publisher = {{IEEE}},
  file = {/Users/hugo/Zotero/storage/F2KKFALC/323080.html}
}

@techreport{rumelhartLearningInternalRepresentations1985,
  title = {Learning Internal Representations by Error Propagation},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1985},
  institution = {{California Univ San Diego La Jolla Inst for Cognitive Science}},
  file = {/Users/hugo/Papers/pdf/rumelhartLearningInternalRepresentations1985.pdf;/Users/hugo/Zotero/storage/YB4KYT27/ADA164453.html}
}

@article{schulzeNewMonotonicCloneindependent2011,
  title = {A New Monotonic, Clone-Independent, Reversal Symmetric, and Condorcet-Consistent Single-Winner Election Method},
  author = {Schulze, Markus},
  year = {2011},
  month = feb,
  journal = {Social Choice and Welfare},
  volume = {36},
  number = {2},
  pages = {267--303},
  issn = {1432-217X},
  doi = {10.1007/s00355-010-0475-4},
  abstract = {In recent years, the Pirate Party of Sweden, the Wikimedia Foundation, the Debian project, the ``Software in the Public Interest'' project, the Gentoo project, and many other private organizations adopted a new single-winner election method for internal elections and referendums. In this article, we will introduce this method, demonstrate that it satisfies, e.g., resolvability, Condorcet, Pareto, reversal symmetry, monotonicity, and independence of clones and present an O(C\^3) algorithm to calculate the winner, where C is the number of alternatives.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/schulzeNewMonotonicCloneindependent2011.pdf}
}

@article{shaliziAutomaticFiltersDetection2006,
  title = {Automatic Filters for the Detection of Coherent Structure in Spatiotemporal Systems},
  author = {Shalizi, Cosma Rohilla and Haslinger, Robert and Rouquier, Jean-Baptiste and Klinkner, Kristina Lisa and Moore, Cristopher},
  year = {2006},
  month = mar,
  journal = {Physical Review E},
  volume = {73},
  number = {3},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.73.036104},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/shaliziAutomaticFiltersDetection2006.pdf}
}

@inproceedings{siegelmannComputationalPowerNeural1992,
  title = {On the Computational Power of Neural Nets},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
  year = {1992},
  pages = {440--449},
  file = {/Users/hugo/Papers/pdf/siegelmannComputationalPowerNeural1992.pdf;/Users/hugo/Zotero/storage/LPZMVTJZ/130385.html}
}

@inproceedings{srivastavaCompeteCompute2013a,
  title = {Compete to {{Compute}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26: 27th {{Annual Conference}} on {{Neural Information Processing Systems}} 2013. {{Proceedings}} of a Meeting Held {{December}} 5-8, 2013, {{Lake Tahoe}}, {{Nevada}}, {{United States}}},
  author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino J. and Schmidhuber, J{\"u}rgen},
  editor = {Burges, Christopher J. C. and Bottou, L{\'e}on and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  year = {2013},
  pages = {2310--2318}
}

@article{tanakaRecentAdvancesPhysical2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  langid = {english},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing},
  file = {/Users/hugo/Papers/pdf/tanakaRecentAdvancesPhysical2019.pdf;/Users/hugo/Zotero/storage/LWYFUFGG/S0893608019300784.html}
}

@inproceedings{taylorCrossdomainTransferReinforcement2007,
  title = {Cross-Domain Transfer for Reinforcement Learning},
  booktitle = {Machine {{Learning}}, {{Proceedings}} of the {{Twenty-Fourth International Conference}} ({{ICML}} 2007), {{Corvallis}}, {{Oregon}}, {{USA}}, {{June}} 20-24, 2007},
  author = {Taylor, Matthew E. and Stone, Peter},
  editor = {Ghahramani, Zoubin},
  year = {2007},
  series = {{{ACM International Conference Proceeding Series}}},
  volume = {227},
  pages = {879--886},
  publisher = {{ACM}},
  doi = {10.1145/1273496.1273607},
  file = {/Users/hugo/Papers/pdf/taylorCrossdomainTransferReinforcement2007.pdf}
}

@article{taylorTransferLearningInterTask2007,
  title = {Transfer {{Learning}} via {{Inter-Task Mappings}} for {{Temporal Difference Learning}}},
  author = {Taylor, Matthew E. and Stone, Peter and Liu, Yaxin},
  year = {2007},
  journal = {J. Mach. Learn. Res.},
  volume = {8},
  pages = {2125--2167}
}

@inproceedings{ulamMathematicalProblemsConnected1962,
  title = {On Some Mathematical Problems Connected with Patterns of Growth of Figures},
  booktitle = {Proceedings of {{Symposia}} in {{Applied Mathematics}}},
  author = {Ulam, Stanislaw},
  year = {1962},
  volume = {14},
  pages = {215--224},
  publisher = {{Am. Math. Soc. Vol. 14, Providence}},
  file = {/Users/hugo/Zotero/storage/HSM72C8W/books.html}
}

@article{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/vaswaniAttentionAllYou2017.pdf;/Users/hugo/Zotero/storage/CXLW8VZD/1706.html}
}

@article{vonneumannTheorySelfreproducingAutomata1966,
  title = {Theory of Self-Reproducing Automata},
  author = {Von Neumann, John and Burks, Arthur W.},
  year = {1966},
  journal = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {1},
  pages = {3--14}
}

@article{wangGeneralizingFewExamples2020,
  title = {Generalizing from a Few Examples: {{A}} Survey on Few-Shot Learning},
  shorttitle = {Generalizing from a Few Examples},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  year = {2020},
  journal = {ACM computing surveys (csur)},
  volume = {53},
  number = {3},
  pages = {1--34},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/hugo/Papers/pdf/wangGeneralizingFewExamples2020.pdf;/Users/hugo/Zotero/storage/2AFHB2F3/3386252.html}
}

@inproceedings{westonAICompleteQuestionAnswering2016,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tom{\'a}s},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016}
}

@book{wolframNewKindScience2002,
  title = {A New Kind of Science},
  author = {Wolfram, Stephen},
  year = {2002},
  publisher = {{Wolfram Media}},
  address = {{Champaign, IL}},
  isbn = {978-1-57955-008-0},
  lccn = {QA267.5.C45 W67 2002},
  keywords = {Cellular automata,Computational complexity}
}

@article{wolframStatisticalMechanicsCellular1983,
  title = {Statistical Mechanics of Cellular Automata},
  author = {Wolfram, Stephen},
  year = {1983},
  month = jul,
  journal = {Reviews of Modern Physics},
  volume = {55},
  number = {3},
  pages = {601--644},
  issn = {0034-6861},
  doi = {10.1103/RevModPhys.55.601},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/wolframStatisticalMechanicsCellular12.pdf}
}

@article{wuenscheClassifyingCellularAutomata1999,
  title = {Classifying Cellular Automata Automatically: {{Finding}} Gliders, Filtering, and Relating Space-Time Patterns, Attractor Basins, and the {{Z}} Parameter},
  shorttitle = {Classifying Cellular Automata Automatically},
  author = {Wuensche, Andrew},
  year = {1999},
  journal = {Complexity},
  volume = {4},
  number = {3},
  pages = {47--66},
  issn = {1099-0526},
  doi = {10.1002/(SICI)1099-0526(199901/02)4:3<47::AID-CPLX9>3.0.CO;2-V},
  abstract = {Cellular automata (CA) rules can be classified automatically for a spectrum of ordered, complex, and chaotic dynamics by a measure of the variance of input-entropy over time. Rules that support interacting gliders and related complex dynamics can be identified, giving an unlimited source for further study. The distribution of rule classes in rule-space can be shown. A byproduct of the method allows the automatic ``filtering'' of CA space-time patterns to show up gliders and related emergent configurations more clearly. The classification seems to correspond to our subjective judgment of space-time dynamics. There are also approximate correlations with global measures on convergence in attractor basins, characterized by the distribution of in-degree sizes in their branching structure, and to the rule parameter, Z. Based on computer experiments using the software Discrete Dynamics Lab (DDLab), this article explains the methods and presents results for 1D CA. \textcopyright{} 1999 John Wiley \& Sons, Inc.},
  copyright = {Copyright \textcopyright{} 1999 John Wiley \& Sons, Inc.},
  langid = {english},
  keywords = {attractor basins,cellular automata,filtering,glider dynamics,Z parameter},
  file = {/Users/hugo/Papers/pdf/wuenscheClassifyingCellularAutomata1999.pdf;/Users/hugo/Zotero/storage/Q2RUEDH4/02)4347AID-CPLX93.0.html}
}

@inproceedings{wulffLearningCellularAutomaton1993,
  title = {Learning Cellular Automaton Dynamics with Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wulff, N. H. and Hertz, J. A.},
  year = {1993},
  pages = {631--638},
  file = {/Users/hugo/Zotero/storage/XFIPA3TI/wulffLearningCellularAutomaton1993.pdf}
}

@article{yaratsImprovingSampleEfficiency2019,
  title = {Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},
  author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  year = {2019},
  journal = {arXiv preprint arXiv:1910.01741},
  eprint = {1910.01741},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/yaratsImprovingSampleEfficiency2019.pdf;/Users/hugo/Zotero/storage/L2WJRRQJ/1910.html}
}

@article{yilmazReservoirComputingUsing2014,
  title = {Reservoir {{Computing}} Using {{Cellular Automata}}},
  author = {Yilmaz, Ozg{\"u}r},
  year = {2014},
  month = oct,
  journal = {arXiv:1410.0162 [cs]},
  eprint = {1410.0162},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long shortterm memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efficient compared to state-of-the-art approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/yilmazReservoirComputingUsing22.pdf}
}

@article{youngMinAtarAtariInspiredTestbed2019,
  title = {{{MinAtar}}: {{An Atari-Inspired Testbed}} for {{Thorough}} and {{Reproducible Reinforcement Learning Experiments}}},
  shorttitle = {{{MinAtar}}},
  author = {Young, Kenny and Tian, Tian},
  year = {2019},
  month = jun,
  journal = {arXiv:1903.03176 [cs]},
  eprint = {1903.03176},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/youngMinAtarAtariInspiredTestbed2019.pdf;/Users/hugo/Zotero/storage/QSPA85QX/1903.html}
}

@inproceedings{yuSampleEfficientReinforcement2018,
  title = {Towards {{Sample Efficient Reinforcement Learning}}.},
  booktitle = {{{IJCAI}}},
  author = {Yu, Yang},
  year = {2018},
  pages = {5739--5743},
  file = {/Users/hugo/Papers/pdf/yuSampleEfficientReinforcement2018.pdf}
}


