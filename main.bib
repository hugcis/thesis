@unpublished{aachGeneralizationDifferentCellular2021,
  title = {Generalization over Different Cellular Automata Rules Learned by a Deep Feed-Forward Neural Network},
  author = {Aach, Marcel and Goebbert, Jens Henrik and Jitsev, Jenia},
  date = {2021-03-27},
  eprint = {2103.14886},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/2103.14886},
  urldate = {2021-04-21},
  abstract = {To test generalization ability of a class of deep neural networks, we randomly generate a large number of different rule sets for 2-D cellular automata (CA), based on John Conway’s Game of Life. Using these rules, we compute several trajectories for each CA instance. A deep convolutional encoder-decoder network with short and long range skip connections is trained on various generated CA trajectories to predict the next CA state given its previous states. Results show that the network is able to learn the rules of various, complex cellular automata and generalize to unseen configurations. To some extent, the network shows generalization to rule sets and neighborhood sizes that were not seen during the training at all.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {/Users/hugo/Papers/pdf/aachGeneralizationDifferentCellular2021.pdf}
}

@article{aageGigavoxelComputationalMorphogenesis2017,
  title = {Giga-Voxel Computational Morphogenesis for Structural Design},
  author = {Aage, Niels and Andreassen, Erik and Lazarov, Boyan S. and Sigmund, Ole},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7674},
  pages = {84--86},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature23911},
  url = {https://www.nature.com/articles/nature23911},
  urldate = {2020-06-03},
  abstract = {Giga-voxel-resolution computational morphogenesis is used to optimize the internal structure of a full-scale aeroplane wing, yielding light-weight designs with more similarities to animal bone structures than to current aeroplane wing designs.},
  issue = {7674},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/aageGigavoxelComputationalMorphogenesis2017.pdf;/Users/hugo/Zotero/storage/YZ5JJKJ8/nature23911.html}
}

@article{abelsonAmorphousComputing2000,
  title = {Amorphous Computing},
  author = {Abelson, Harold and Allen, Don and Coore, Daniel and Hanson, Chris and Homsy, George and Knight Jr, Thomas F. and Nagpal, Radhika and Rauch, Erik and Sussman, Gerald Jay and Weiss, Ron},
  date = {2000},
  journaltitle = {Communications of the ACM},
  volume = {43},
  number = {6},
  pages = {74--82},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/hugo/Papers/pdf/abelsonAmorphousComputing2000.pdf}
}

@article{adamiEvolutionBiologicalComplexity,
  title = {Evolution of Biological Complexity},
  author = {Adami, Christoph and Ofria, Charles and Collier, Travis C},
  pages = {6},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/adamiEvolutionBiologicalComplexity2.pdf}
}

@article{adamopoulosEvolvingCellularAutomata2010,
  title = {Evolving Cellular Automata Rules for Multiple-Step-Ahead Prediction of Complex Binary Sequences},
  author = {Adamopoulos, A.V. and Pavlidis, N.G. and Vrahatis, M.N.},
  date = {2010-02},
  journaltitle = {Mathematical and Computer Modelling},
  volume = {51},
  number = {3-4},
  pages = {229--238},
  issn = {08957177},
  doi = {10.1016/j.mcm.2009.08.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0895717709002696},
  urldate = {2019-05-26},
  abstract = {Complex binary sequences are generated through the application of simple threshold, linear transformations to the logistic iterative map. Depending primarily on the value of its non-linearity parameter, the logistic map exhibits a great variety of behavior, including stable states, cycling and periodical activity and the period doubling phenomenon that leads to high-order chaos. From the real data sequences, binary sequences are derived. Consecutive L bit sequences are given as input to a cellular automaton with the task to regenerate the subsequent L bits of the binary sequence in precisely L evolution steps. To perform this task a genetic algorithm is employed to evolve cellular automaton rules. Various complex binary sequences are examined, for a variety of initial values and a wide range of values of the non-linearity parameter. The proposed hybrid multiple-step-ahead prediction algorithm, based on a combination of genetic algorithms and cellular automata proved efficient and effective.},
  langid = {english}
}

@article{adamsFormalDefinitionsUnbounded2017,
  title = {Formal {{Definitions}} of {{Unbounded Evolution}} and {{Innovation Reveal Universal Mechanisms}} for {{Open-Ended Evolution}} in {{Dynamical Systems}}},
  author = {Adams, Alyssa and Zenil, Hector and Davies, Paul C. W. and Walker, Sara Imari},
  date = {2017-04-20},
  journaltitle = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {997},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-00810-8},
  url = {https://www.nature.com/articles/s41598-017-00810-8},
  urldate = {2019-06-27},
  abstract = {Open-ended evolution (OEE) is relevant to a variety of biological, artificial and technological systems, but has been challenging to reproduce in silico. Most theoretical efforts focus on key aspects of open-ended evolution as it appears in biology. We recast the problem as a more general one in dynamical systems theory, providing simple criteria for open-ended evolution based on two hallmark features: unbounded evolution and innovation. We define unbounded evolution as patterns that are non-repeating within the expected Poincare recurrence time of an isolated system, and innovation as trajectories not observed in isolated systems. As a case study, we implement novel variants of cellular automata (CA) where the update rules are allowed to vary with time in three alternative ways. Each is capable of generating conditions for open-ended evolution, but vary in their ability to do so. We find that state-dependent dynamics, regarded as a hallmark of life, statistically out-performs other candidate mechanisms, and is the only mechanism to produce open-ended evolution in a scalable manner, essential to the notion of ongoing evolution. This analysis suggests a new framework for unifying mechanisms for generating OEE with features distinctive to life and its artifacts, with broad applicability to biological and artificial systems.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/adamsFormalDefinitionsUnbounded22.pdf;/Users/hugo/Zotero/storage/JUE7LI4I/s41598-017-00810-8.html}
}

@article{adamsPhysicalUniversalityStateDependent2017,
  title = {Physical {{Universality}}, {{State-Dependent Dynamical Laws}} and {{Open-Ended Novelty}}},
  author = {Adams, Alyssa and Berner, Angelica and Davies, Paul and Walker, Sara},
  date = {2017-09-01},
  journaltitle = {Entropy},
  volume = {19},
  number = {9},
  pages = {461},
  issn = {1099-4300},
  doi = {10.3390/e19090461},
  url = {http://www.mdpi.com/1099-4300/19/9/461},
  urldate = {2020-07-30},
  abstract = {A major conceptual step forward in understanding the logical architecture of living systems was advanced by von Neumann with his universal constructor, a physical device capable of self-reproduction. A necessary condition for a universal constructor to exist is that the laws of physics permit physical universality, such that any transformation (consistent with the laws of physics and availability of resources) can be caused to occur. While physical universality has been demonstrated in simple cellular automata models, so far these have not displayed a requisite feature of life—namely open-ended evolution—the explanation of which was also a prime motivator in von Neumann’s formulation of a universal constructor. Current examples of physical universality rely on reversible dynamical laws, whereas it is well-known that living processes are dissipative. Here we show that physical universality and open-ended dynamics should both be possible in irreversible dynamical systems if one entertains the possibility of state-dependent laws. We demonstrate with simple toy models how the accessibility of state space can yield open-ended trajectories, defined as trajectories that do not repeat within the expected Poincaré recurrence time and are not reproducible by an isolated system. We discuss implications for physical universality, or an approximation to it, as a foundational framework for developing a physics for life.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/adamsPhysicalUniversalityStateDependent2017.pdf}
}

@book{adkinsAssetEconomyProperty2020,
  title = {The Asset Economy: Property Ownership and the New Logic of Inequality},
  shorttitle = {The Asset Economy},
  author = {Adkins, Lisa and Konings, Martijn and Cooper, Melinda},
  date = {2020},
  publisher = {{Polity Press}},
  location = {{Medford}},
  abstract = {"How assets dictate the new class system"--},
  isbn = {978-1-5095-4422-6 978-1-5095-4347-2},
  pagetotal = {1},
  keywords = {Finance,Generation Y,Home ownership,Social aspects,Social conditions,Social stratification,Time}
}

@article{aguiarTwoElementaryCellular2015,
  title = {Two {{Elementary Cellular Automata}} with a {{New Kind}} of {{Dynamic}}},
  author = {Aguiar, Isabel and Severino, Ricardo},
  date = {2015-06-15},
  journaltitle = {Complex Systems},
  volume = {24},
  number = {2},
  pages = {113--125},
  issn = {08912513},
  doi = {10.25088/ComplexSystems.24.2.113},
  url = {http://www.complex-systems.com/abstracts/v24_i02_a02.html},
  urldate = {2019-06-10},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/WTUAPYEW/aguiarTwoElementaryCellular2015.pdf}
}

@article{aguilarPresentFutureArtificial2014,
  title = {The {{Past}}, {{Present}}, and {{Future}} of {{Artificial Life}}},
  author = {Aguilar, Wendy and Santamaría-Bonfil, Guillermo and Froese, Tom and Gershenson, Carlos},
  date = {2014},
  journaltitle = {Frontiers in Robotics and AI},
  shortjournal = {Front. Robot. AI},
  volume = {1},
  publisher = {{Frontiers}},
  issn = {2296-9144},
  doi = {10.3389/frobt.2014.00008},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2014.00008/full},
  urldate = {2021-05-16},
  abstract = {For millennia people have wondered what makes the living different from the non-living. Beginning in the mid-1980s, artificial life has studied living systems using a synthetic approach: build life in order to understand it better, be it by means of software, hardware, or wetware. This review provides a summary of the advances that led to the development of artificial life, its current research topics, and open problems and opportunities. We classify artificial life research into fourteen themes: origins of life, autonomy, self-organization, adaptation (including evolution, development, and learning), ecology, artificial societies, behavior, computational biology, artificial chemistries, information, living technology, art, and philosophy. Being interdisciplinary, artificial life seems to be losing its boundaries and merging with other fields.},
  langid = {english},
  keywords = {adaptation,artificial intelligence,Artificial Life,Cognitive Science,Philosophy,Robotics,self-organization,Synthetic Biology},
  file = {/Users/hugo/Papers/pdf/aguilarPresentFutureArtificial2014.pdf}
}

@misc{ainsworthGitReBasinMerging2022,
  title = {Git {{Re-Basin}}: {{Merging Models}} modulo {{Permutation Symmetries}}},
  shorttitle = {Git {{Re-Basin}}},
  author = {Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddhartha},
  date = {2022-09-11},
  number = {arXiv:2209.04836},
  eprint = {2209.04836},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.04836},
  url = {http://arxiv.org/abs/2209.04836},
  urldate = {2022-09-15},
  abstract = {The success of deep learning is thanks to our ability to solve certain massive non-convex optimization problems with relative ease. Despite non-convex optimization being NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin, after accounting for all possible permutation symmetries of hidden units. We introduce three algorithms to permute the units of one model to bring them into alignment with units of a reference model. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10 and CIFAR-100. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity across a variety of models and datasets. Finally, we discuss shortcomings of a single basin theory, including a counterexample to the linear mode connectivity hypothesis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/ainsworthGitReBasinMerging2022.pdf;/Users/hugo/Zotero/storage/QX7M3INH/2209.html}
}

@unpublished{aitkenGeometryIntegrationText2020,
  title = {The Geometry of Integration in Text Classification {{RNNs}}},
  author = {Aitken, Kyle and Ramasesh, Vinay V. and Garg, Ankush and Cao, Yuan and Sussillo, David and Maheswaranathan, Niru},
  date = {2020-10-28},
  eprint = {2010.15114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2010.15114},
  urldate = {2020-11-02},
  abstract = {Despite the widespread application of recurrent neural networks (RNNs) across a variety of tasks, a unified understanding of how RNNs solve these tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of a specific natural language processing task: text classification. Using tools from dynamical systems analysis, we study recurrent networks trained on a battery of both natural and synthetic text classification tasks. We find the dynamics of these trained RNNs to be both interpretable and lowdimensional. Specifically, across architectures and datasets, RNNs accumulate evidence for each class as they process the text, using a low-dimensional attractor manifold as the underlying mechanism. Moreover, the dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset; in particular, we describe how simple word-count statistics computed on the training dataset can be used to predict these properties. Our observations span multiple architectures and datasets, reflecting a common mechanism RNNs employ to perform text classification. To the degree that integration of evidence towards a decision is a common computational primitive, this work lays the foundation for using dynamical systems techniques to study the inner workings of RNNs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/X6PPP8MQ/Aitken et al. - 2020 - The geometry of integration in text classification.pdf}
}

@article{aizawaSolitonTurbulenceOnedimensional1990,
  title = {Soliton Turbulence in One-Dimensional Cellular Automata},
  author = {Aizawa, Y. and Nishikawa, I. and Kaneko, K.},
  date = {1990-09-02},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {45},
  number = {1},
  pages = {307--327},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(90)90191-Q},
  url = {http://www.sciencedirect.com/science/article/pii/016727899090191Q},
  urldate = {2020-04-13},
  abstract = {Statistical properties of cellular automata which support simple solitary waves are numerically studied. Under these rules, spatio-temporal patterns are sensitively dependent on the collision processes of solitons, and other elementary excitations such as breathers, kinks, and nuclei. Patterns typically become randomized as collisions proceed. Turbulent states are realized after many collisions. The resulting global patterns can be classified by the types of elementary excitations they contain. In many turbulent states, the spectra reveal a long-range order with a k−v anomaly for k ⪡ 1. The irreversible process leading to turbulent equilibrium is characterized by the transient spectrum together with the Allan variance. The mean free motion of a soliton in the turbulent states is measured by the mutual information flow, and the information loss is shown to obey an inverse power law. The transient time to reach an attractor grows exponentially with system size, which suggests that in the thermodynamic limit soliton turbulence is not an attractor but rather a transient state.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/aizawaSolitonTurbulenceOnedimensional1990.pdf;/Users/hugo/Zotero/storage/MFZ4PTL6/016727899090191Q.html}
}

@misc{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  date = {2022-04-29},
  number = {arXiv:2204.14198},
  eprint = {2204.14198},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.14198},
  urldate = {2022-07-26},
  abstract = {Building models that can be rapidly adapted to numerous tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. Flamingo models include key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of the proposed Flamingo models, exploring and measuring their ability to rapidly adapt to a variety of image and video understanding benchmarks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple choice visual question-answering. For tasks lying anywhere on this spectrum, we demonstrate that a single Flamingo model can achieve a new state of the art for few-shot learning, simply by prompting the model with task-specific examples. On many of these benchmarks, Flamingo actually surpasses the performance of models that are fine-tuned on thousands of times more task-specific data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/alayracFlamingoVisualLanguage2022.pdf;/Users/hugo/Zotero/storage/S25Y9QLV/2204.html}
}

@article{aldousInteractingParticleSystems2013,
  title = {Interacting Particle Systems as Stochastic Social Dynamics},
  author = {Aldous, David},
  date = {2013-09-01},
  journaltitle = {Bernoulli},
  shortjournal = {Bernoulli},
  volume = {19},
  number = {4},
  eprint = {1309.6766},
  eprinttype = {arxiv},
  issn = {1350-7265},
  doi = {10.3150/12-BEJSP04},
  url = {http://arxiv.org/abs/1309.6766},
  urldate = {2021-12-13},
  abstract = {The style of mathematical models known to probabilists as Interacting Particle Systems and exemplified by the Voter, Exclusion and Contact processes have found use in many academic disciplines. In many such disciplines the underlying conceptual picture is of a social network, where individuals meet pairwise and update their "state" (opinion, activity etc) in a way depending on the two previous states. This picture motivates a precise general setup we call Finite Markov Information Exchange (FMIE) processes. We briefly describe a few less familiar models (Averaging, Compulsive Gambler, Deference, Fashionista) suggested by the social network picture, as well as a few familiar ones.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/Users/hugo/Zotero/storage/RL4ECW89/Aldous - 2013 - Interacting particle systems as stochastic social .pdf}
}

@unpublished{alemiTherMLThermodynamicsMachine2018,
  title = {{{TherML}}: {{Thermodynamics}} of {{Machine Learning}}},
  shorttitle = {{{TherML}}},
  author = {Alemi, Alexander A. and Fischer, Ian},
  date = {2018-07-11},
  eprint = {1807.04162},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, stat},
  url = {http://arxiv.org/abs/1807.04162},
  urldate = {2019-05-26},
  abstract = {In this work we offer an information-theoretic framework for representation learning that connects with a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/WYQEXJNR/alemiTherMLThermodynamicsMachine2018.pdf}
}

@unpublished{alexanderAutodidacticUniverse2021,
  title = {The {{Autodidactic Universe}}},
  author = {Alexander, Stephon and Cunningham, William J. and Lanier, Jaron and Smolin, Lee and Stanojevic, Stefan and Toomey, Michael W. and Wecker, Dave},
  date = {2021-09-02},
  eprint = {2104.03902},
  eprinttype = {arxiv},
  primaryclass = {gr-qc, physics:hep-th, physics:physics, physics:quant-ph},
  url = {http://arxiv.org/abs/2104.03902},
  urldate = {2021-09-26},
  abstract = {We present an approach to cosmology in which the Universe learns its own physical laws. It does so by exploring a landscape of possible laws, which we express as a certain class of matrix models. We discover maps that put each of these matrix models in correspondence with both a gauge/gravity theory and a mathematical model of a learning machine, such as a deep recurrent, cyclic neural network. This establishes a correspondence between each solution of the physical theory and a run of a neural network.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,General Relativity and Quantum Cosmology,High Energy Physics - Theory,Physics - History and Philosophy of Physics,Quantum Physics},
  file = {/Users/hugo/Papers/pdf/alexanderAutodidacticUniverse2021.pdf}
}

@article{alfaroGeneratingInterestingPatterns,
  title = {Generating {{Interesting Patterns}} in {{Conway}}’s {{Game}} of {{Life Through}} a {{Genetic Algorithm}}},
  author = {Alfaro, Hector and Mendoza, Francisco and Tice, Chris},
  pages = {8},
  abstract = {In this paper we describe the application of a genetic algorithm to John Conway’s Game of Life, a popular form of Cellular Automata. Our intent is to create an arrangement of cellular automata (CA) that will produce “interesting” behavior when placed into Conway’s Game of Life – where “interesting” behavior includes repeated patterns, reproducing groups of cells, and patterns that change their location in Life. We also discuss techniques for improving algorithm performance by applying methods developed in previous research.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/alfaroGeneratingInterestingPatterns2.pdf}
}

@article{allenEvolutionEmergenceLearning2003,
  title = {Evolution, Emergence, and Learning in Complex Systems},
  author = {Allen, Peter M. and Strathern, Mark},
  date = {2003},
  journaltitle = {Emergence},
  volume = {5},
  number = {4},
  pages = {8--33},
  publisher = {{Taylor \& Francis}},
  file = {/Users/hugo/Papers/pdf/allenEvolutionEmergenceLearning2003.pdf;/Users/hugo/Zotero/storage/J9DGTJFY/s15327000em0504_4.html}
}

@unpublished{altschulerMassivelyScalableSinkhorn2018,
  title = {Massively Scalable {{Sinkhorn}} Distances via the {{Nystr}}\textbackslash "om Method},
  author = {Altschuler, Jason and Bach, Francis and Rudi, Alessandro and Weed, Jonathan},
  date = {2018-12-12},
  eprint = {1812.05189},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1812.05189},
  urldate = {2018-12-17},
  abstract = {The Sinkhorn distance, a variant of the Wasserstein distance with entropic regularization, is an increasingly popular tool in machine learning and statistical inference. We give a simple, practical, parallelizable algorithm Nys-Sink, based on Nystr¨om approximation, for computing Sinkhorn distances on a massive scale. As we show in numerical experiments, our algorithm easily computes Sinkhorn distances on data sets hundreds of times larger than can be handled by state-of-the-art approaches. We also give provable guarantees establishing that the running time and memory requirements of our algorithm adapt to the intrinsic dimension of the underlying data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/altschulerMassivelyScalableSinkhorn22.pdf}
}

@book{andersonEconomyEvolvingComplex1996,
  title = {The Economy as an Evolving Complex System: The Proceedings of the {{Evolutionary Paths}} of the {{Global Economy Workshop}}, Held {{September}}, 1987 in {{Santa Fe}}, {{New Mexico}}},
  shorttitle = {The Economy as an Evolving Complex System},
  editor = {Anderson, Philip W. and Evolutionary Paths of the Global Economy Workshop},
  date = {1996},
  series = {Santa {{Fe Institute}} Studies in the Sciences of Complexity},
  edition = {8. print},
  number = {5},
  publisher = {{Addison-Wesley Publ. Co}},
  location = {{Reading, Mass.}},
  eventtitle = {Evolutionary {{Paths}} of the {{Global Economy Workshop}}},
  isbn = {978-0-201-15685-0},
  langid = {english},
  pagetotal = {317},
  annotation = {OCLC: 258655934},
  file = {/Users/hugo/Papers/pdf/andersonEconomyEvolvingComplex1996.pdf}
}

@article{andersonMoreDifferent1972,
  title = {More {{Is Different}}},
  author = {Anderson, P. W.},
  date = {1972-08-04},
  journaltitle = {Science},
  volume = {177},
  number = {4047},
  eprint = {17796623},
  eprinttype = {pmid},
  pages = {393--396},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.177.4047.393},
  url = {https://science.sciencemag.org/content/177/4047/393},
  urldate = {2020-04-14},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/andersonMoreDifferent1972.pdf}
}

@inproceedings{andreDiscoveryGeneticProgramming1996,
  title = {Discovery by Genetic Programming of a Cellular Automata Rule That Is Better than Any Known Rule for the Majority Classification Problem},
  booktitle = {Proceedings of the 1st Annual Conference on Genetic Programming},
  author = {Andre, David and Bennett, Forrest H. and Koza, John R.},
  date = {1996-07-28},
  pages = {3--11},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {It is difficult to program cellular automata. This is especially true when the desired computation requires global communication and global integration of information across great distances in the cellular space. Various human-written algorithms have appeared in the past two decades for the vexatious majority classification task for one-dimensional two-state cellular automata. This paper describes how genetic programming with automatically defined functions evolved a rule for this task with an accuracy of 82.326\%. This level of accuracy exceeds that of the original 1978 Gacs-Kurdyumov-Levin (GKL) rule, all other known human-written rules, and all other known rules produced by automated methods. The rule evolved by genetic programming is qualitatively different from all previous rules in that it employs a larger and more intricate repertoire of domains and particles to represent and communicate information across the cellular space.},
  isbn = {978-0-262-61127-5},
  file = {/Users/hugo/Papers/pdf/andreDiscoveryGeneticProgramming1996.pdf}
}

@article{angelineEvolutionaryAlgorithmThat1994,
  title = {An Evolutionary Algorithm That Constructs Recurrent Neural Networks},
  author = {Angeline, P.J. and Saunders, G.M. and Pollack, J.B.},
  date = {1994-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {1},
  pages = {54--65},
  issn = {1941-0093},
  doi = {10.1109/72.265960},
  abstract = {Standard methods for simultaneously inducing the structure and weights of recurrent neural networks limit every task to an assumed class of architectures. Such a simplification is necessary since the interactions between network structure and function are not well understood. Evolutionary computations, which include genetic algorithms and evolutionary programming, are population-based search methods that have shown promise in many similarly complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. GNARL's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods.{$<>$}},
  keywords = {Artificial intelligence,Ash,Computer architecture,evolutionary algorithm,Evolutionary computation,evolutionary programming,genetic algorithms,Genetic algorithms,Genetic programming,GNARL,Induction generators,Network topology,optimisation,population-based search methods,recurrent neural nets,recurrent neural networks,Recurrent neural networks,Search methods},
  file = {/Users/hugo/Zotero/storage/XND5KS9I/angelineEvolutionaryAlgorithmThat1994.pdf;/Users/hugo/Zotero/storage/XKPJRRZ3/265960.html}
}

@online{AnnouncingAI21Studio,
  title = {Announcing {{AI21 Studio}} and {{Jurassic-1 Language Models}}},
  url = {https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1},
  urldate = {2022-07-26},
  abstract = {AI21 Labs’ new developer platform offers instant access to our 178B-parameter language model, to help you build sophisticated text-based AI applications at scale},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/W6TUY65S/announcing-ai21-studio-and-jurassic-1.html}
}

@inproceedings{arasuLargeScaleDeduplicationConstraints2009,
  title = {Large-{{Scale Deduplication}} with {{Constraints Using Dedupalog}}},
  author = {Arasu, Arvind and Ré, Christopher and Suciu, Dan},
  date = {2009-03},
  pages = {952--963},
  publisher = {{IEEE}},
  doi = {10.1109/ICDE.2009.43},
  url = {http://ieeexplore.ieee.org/document/4812468/},
  urldate = {2018-05-07},
  abstract = {We present a declarative framework for collective deduplication of entity references in the presence of constraints. Constraints occur naturally in many data cleaning domains and can improve the quality of deduplication. An example of a constraint is “each paper has a unique publication venue”; if two paper references are duplicates, then their associated conference references must be duplicates as well. Our framework supports collective deduplication, meaning that we can dedupe both paper references and conference references collectively in the example above. Our framework is based on a simple declarative Datalogstyle language with precise semantics. Most previous work on deduplication either ignore constraints or use them in an ad-hoc domain-specific manner. We also present efficient algorithms to support the framework. Our algorithms have precise theoretical guarantees for a large subclass of our framework. We show, using a prototype implementation, that our algorithms scale to very large datasets. We provide thorough experimental results over real-world data demonstrating the utility of our framework for high-quality and scalable deduplication.},
  isbn = {978-1-4244-3422-0},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/arasuLargeScaleDeduplicationConstraints22.pdf}
}

@inproceedings{arataFreeformShapeModeling1999,
  title = {Free-Form Shape Modeling by {{3D}} Cellular Automata},
  booktitle = {Proceedings {{Shape Modeling International}}'99. {{International Conference}} on {{Shape Modeling}} and {{Applications}}},
  author = {Arata, Hideki and Takai, Yoshiaki and Takai, Nami K. and Yamamoto, Tsuyoshi},
  date = {1999},
  pages = {242--247},
  publisher = {{IEEE}},
  file = {/Users/hugo/Zotero/storage/XE7ZE633/749346.html}
}

@unpublished{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017-01-26},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.07875},
  urldate = {2019-01-01},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/arjovskyWassersteinGAN22.pdf;/Users/hugo/Zotero/storage/W3ZAVRJ8/1701.html}
}

@unpublished{atenieseHackingSmartMachines2013,
  title = {Hacking {{Smart Machines}} with {{Smarter Ones}}: {{How}} to {{Extract Meaningful Data}} from {{Machine Learning Classifiers}}},
  shorttitle = {Hacking {{Smart Machines}} with {{Smarter Ones}}},
  author = {Ateniese, Giuseppe and Felici, Giovanni and Mancini, Luigi V. and Spognardi, Angelo and Villani, Antonio and Vitali, Domenico},
  date = {2013-06-19},
  eprint = {1306.4447},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1306.4447},
  urldate = {2020-12-02},
  abstract = {Machine Learning (ML) algorithms are used to train computers to perform a variety of complex tasks and improve with experience. Computers learn how to recognize patterns, make unintended decisions, or react to a dynamic environment. Certain trained machines may be more effective than others because they are based on more suitable ML algorithms or because they were trained through superior training sets. Although ML algorithms are known and publicly released, training sets may not be reasonably ascertainable and, indeed, may be guarded as trade secrets. While much research has been performed about the privacy of the elements of training sets, in this paper we focus our attention on ML classifiers and on the statistical information that can be unconsciously or maliciously revealed from them. We show that it is possible to infer unexpected but useful information from ML classifiers. In particular, we build a novel meta-classifier and train it to hack other classifiers, obtaining meaningful information about their training sets. This kind of information leakage can be exploited, for example, by a vendor to build more effective classifiers or to simply acquire trade secrets from a competitor’s apparatus, potentially violating its intellectual property rights.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/atenieseHackingSmartMachines22.pdf}
}

@inproceedings{auerDBpediaNucleusWeb2007,
  title = {{{DBpedia}}: {{A}} Nucleus for a {{Web}} of Open Data},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  date = {2007},
  volume = {4825 LNCS},
  pages = {722--735},
  doi = {10.1007/978-3-540-76298-0_52},
  url = {http://www.cis.upenn.edu/ zives/research/dbpedia.pdf},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
  isbn = {3-540-76297-3},
  file = {/Users/hugo/Papers/pdf/auerDBpediaNucleusWeb22.pdf}
}

@article{aurenhammerMinkowskiTypeTheoremsLeastSquares1998,
  title = {Minkowski-{{Type Theorems}} and {{Least-Squares Clustering}}},
  author = {Aurenhammer, F. and Hoffmann, F. and Aronov, B.},
  date = {1998-01},
  journaltitle = {Algorithmica},
  volume = {20},
  number = {1},
  pages = {61--76},
  issn = {0178-4617},
  doi = {10.1007/PL00009187},
  url = {http://link.springer.com/10.1007/PL00009187},
  urldate = {2019-01-02},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/aurenhammerMinkowskiTypeTheoremsLeastSquares1998.pdf}
}

@article{babsonReservoirComputingComplex2019,
  title = {Reservoir {{Computing}} with {{Complex Cellular Automata}}},
  author = {Babson, Neil and Teuscher, Christof and {Portland State University}},
  date = {2019-12-15},
  journaltitle = {Complex Systems},
  volume = {28},
  number = {4},
  pages = {433--455},
  issn = {08912513},
  doi = {10.25088/ComplexSystems.28.4.433},
  url = {https://www.complex-systems.com/abstracts/v28_i04_a03/},
  urldate = {2020-01-10},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/babsonReservoirComputingComplex22.pdf}
}

@unpublished{bachLearningStructureGenerative2017,
  title = {Learning the {{Structure}} of {{Generative Models}} without {{Labeled Data}}},
  author = {Bach, Stephen H. and He, Bryan and Ratner, Alexander and Ré, Christopher},
  date = {2017-03-02},
  eprint = {1703.00854},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.00854},
  urldate = {2018-04-16},
  abstract = {Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the l-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100 times faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/bachLearningStructureGenerative22.pdf;/Users/hugo/Zotero/storage/SPMV5JY7/1703.html}
}

@article{backOverviewEvolutionaryAlgorithms1993,
  title = {An Overview of Evolutionary Algorithms for Parameter Optimization},
  author = {Bäck, Thomas and Schwefel, Hans-Paul},
  date = {1993},
  journaltitle = {Evolutionary computation},
  volume = {1},
  number = {1},
  pages = {1--23},
  publisher = {{mit Press}},
  file = {/Users/hugo/Papers/pdf/backOverviewEvolutionaryAlgorithms1993.pdf;/Users/hugo/Zotero/storage/6I5ZPEXE/6791438.html}
}

@unpublished{baetensIntroducingLyapunovProfiles2015,
  title = {Introducing {{Lyapunov}} Profiles of Cellular Automata},
  author = {Baetens, Jan M. and Gravner, Janko},
  date = {2015-09-22},
  eprint = {1509.06639},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1509.06639},
  urldate = {2019-06-05},
  abstract = {In line with the stability theory of continuous dynamical systems, Lyapunov exponents of cellular automata (CAs) have been conceived two decades ago to quantify to what extent their dynamics changes following a perturbation of their initial configuration. More precisely, Lyapunov exponents of CAs have either been understood as the rate by which the resulting defect cone widens as these dynamical systems are evolved from a perturbed initial configuration, or as the rate by which defects accumulate during their evolution. The former viewpoint yields insight into the extent of the affected region, whereas the latter tells us something about the intensity of the defect propagation. In this paper, we will show how these viewpoints can be united by relying on Lyapunov profiles of CAs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/baetensIntroducingLyapunovProfiles22.pdf}
}

@article{baetensPhenomenologicalStudyIrregular2010b,
  title = {Phenomenological Study of Irregular Cellular Automata Based on {{Lyapunov}} Exponents and {{Jacobians}}},
  author = {Baetens, Jan M. and De Baets, Bernard},
  date = {2010-09},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  shortjournal = {Chaos},
  volume = {20},
  number = {3},
  pages = {033112},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3460362},
  url = {http://aip.scitation.org/doi/10.1063/1.3460362},
  urldate = {2021-01-18},
  langid = {english}
}

@article{bagleySpontaneousEmergenceMetabolism1991,
  title = {Spontaneous {{Emergence}} of a {{Metabolism}}},
  author = {Bagley, R.J. and Farmer, J.D.},
  date = {1991},
  journaltitle = {Artificial Life II},
  volume = {X},
  pages = {93--140},
  abstract = {Networks of catalysed reactions with nonlinear feedback have been proposed to play an important role in the origin of life. We investigate this possibility in a polymer chemistry with catalysed cleavage and condensation reactions, studying the properties of a well-stirred reactor driven away from equilibrium by the flow of mass. Near equilibrium the distribution of material is uninteresting; it favours short polymers but is otherwise homogenous. However, under appropriate non-equilibrium conditions, the situation changes radically: The nonlinear feedback of the reaction network focuses the material of the system into a few specific polymer species, whose concentrations can be orders of magnitude about the background. Like a metabolism, the network of catalytic reactions "digests" the material of its environment, incorporating it into its own form. For this reason we call it an autocatalytic metabolism. We vary the diet of an autocatalytic metabolism, and demonstrate that under some variations it persists almost unchanged, while in other cases it dies. We argue that the dynamical stability of autocatalytic metabolisms gives them regenerative properties that allow them to repair themselves and to propagate through time.}
}

@article{balcanLifelongLearningCostly2020,
  title = {Lifelong Learning in Costly Feature Spaces},
  author = {Balcan, Maria-Florina and Blum, Avrim and Nagarajan, Vaishnavh},
  date = {2020-02-12},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Special {{Issue}} on {{Algorithmic Learning Theory}}},
  volume = {808},
  pages = {14--37},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2019.11.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397519307224},
  urldate = {2022-08-24},
  abstract = {An important long-term goal in machine learning systems is to build learning agents that, like humans, can learn many tasks over their lifetime, and moreover use information from these tasks to improve their ability to do so efficiently. In this work, our goal is to provide new theoretical insights into the potential of this paradigm. In particular, we propose a lifelong learning framework that adheres to a novel notion of resource efficiency that is critical in many real-world domains where feature evaluations are costly. That is, our learner aims to reuse information from previously learned related tasks to learn future tasks in a feature-efficient manner. Furthermore, we consider novel combinatorial ways in which learning tasks can relate. Specifically, we design lifelong learning algorithms for two structurally different and widely used families of target functions: decision trees/lists and monomials/polynomials. We also provide strong feature-efficiency guarantees for these algorithms; in fact, we show that in order to learn future targets, we need only slightly more feature evaluations per training example than what is needed to predict on an arbitrary example using those targets. We also provide algorithms with guarantees in an agnostic model where not all the targets are related to each other. Finally, we also provide lower bounds on the performance of a lifelong learner in these models, which are in fact tight under some conditions.},
  langid = {english},
  keywords = {Costly features,Lifelong learning,Representation learning},
  file = {/Users/hugo/Papers/pdf/balcanLifelongLearningCostly2020.pdf;/Users/hugo/Zotero/storage/UA2WPRHX/S0304397519307224.html}
}

@article{baldanDistributedFastShapeletTransform2019,
  title = {Distributed {{FastShapelet Transform}}: A {{Big Data}} Time Series Classification Algorithm},
  shorttitle = {Distributed {{FastShapelet Transform}}},
  author = {Baldán, Francisco J. and Benítez, José M.},
  date = {2019-09-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {496},
  pages = {451--463},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2018.10.028},
  url = {https://www.sciencedirect.com/science/article/pii/S0020025518308454},
  urldate = {2022-06-24},
  abstract = {The classification of time series is a central problem in a wide range of disciplines. In this field, the state-of-the-art algorithm is COTE (Collective of Transformation-Based Ensembles) which is a combination of classifiers of different domains: time, autocorrelation, power spectrum and shapelets. The weakest point of this approach is its high computational burden which prevents its use in massive data environments. Shapelet Transform is one of the multiple algorithms that compose this ensemble. It has been shown to achieve a good performance over many reference datasets. Nevertheless, its computational complexity is also too high to be used in massive data environments. On the other hand, Big Data has emerged as an approach to manage massive datasets, which also applies to time series. We propose an algorithm for time series classification in a Big Data environment, DFST. It is based on a combination of the FastShapelet and Shapelet Transform ideas and it is the first completely scalable algorithm for time series classification. We have shown that our proposal scales linearly with the number of time series in dataset. In addition, the classification accuracy is equal to or higher than that of comparable sequential algorithms.},
  langid = {english},
  keywords = {Big Data,Classification,Shapelet,Time series},
  file = {/Users/hugo/Zotero/storage/356M6D9I/S0020025518308454.html}
}

@article{baldiNeuralNetworksPrincipal1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  shorttitle = {Neural Networks and Principal Component Analysis},
  author = {Baldi, Pierre and Hornik, Kurt},
  date = {1989-01},
  journaltitle = {Neural Networks},
  volume = {2},
  number = {1},
  pages = {53--58},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90014-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900142},
  urldate = {2020-03-05},
  abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/baldiNeuralNetworksPrincipal1989.pdf}
}

@inproceedings{banerjeeMETEORAutomaticMetric2005,
  title = {{{METEOR}}: {{An Automatic Metric}} for {{MT Evaluation}} with {{Improved Correlation}} with {{Human Judgments}}},
  shorttitle = {{{METEOR}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Intrinsic}} and {{Extrinsic Evaluation Measures}} for {{Machine Translation}} and/or {{Summarization}}@{{ACL}} 2005, {{Ann Arbor}}, {{Michigan}}, {{USA}}, {{June}} 29, 2005},
  author = {Banerjee, Satanjeev and Lavie, Alon},
  editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare R.},
  date = {2005},
  pages = {65--72},
  publisher = {{Association for Computational Linguistics}},
  url = {https://aclanthology.org/W05-0909/},
  urldate = {2022-08-04}
}

@inproceedings{banzhafSelforganizationSystemBinary1994,
  title = {Self-Organization in a System of Binary Strings},
  booktitle = {Proceedings of {{Artificial Life IV}}},
  author = {Banzhaf, Wolfgang},
  date = {1994},
  pages = {109--118},
  publisher = {{MIT Press Cambridge, MA}}
}

@inreference{bar-yamGeneralFeaturesComplex2002,
  title = {General {{Features}} of {{Complex Systems}}},
  booktitle = {Encyclopedia of {{Life Support Systems}} ({{EOLSS}}), {{UNESCO}}},
  author = {Bar-Yam, Yaneer},
  date = {2002},
  volume = {1},
  pages = {10},
  publisher = {{EOLSS Publishers}},
  location = {{Oxford, UK}},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/9W3VTUMB/Bar-Yam - General Features of Complex Systems.pdf}
}

@article{barbieriTopicawareSocialInfluence2013,
  title = {Topic-Aware Social Influence Propagation Models},
  author = {Barbieri, Nicola and Bonchi, Francesco and Manco, Giuseppe},
  date = {2013-12-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {37},
  number = {3},
  pages = {555--584},
  issn = {0219-3116},
  doi = {10.1007/s10115-013-0646-6},
  url = {https://doi.org/10.1007/s10115-013-0646-6},
  urldate = {2019-01-03},
  abstract = {The study of influence-driven propagations in social networks and its exploitation for viral marketing purposes has recently received a large deal of attention. However, regardless of the fact that users authoritativeness, expertise, trust and influence are evidently topic-dependent, the research on social influence has surprisingly largely overlooked this aspect. In this article, we study social influence from a topic modeling perspective. We introduce novel topic-aware influence-driven propagation models that, as we show in our experiments, are more accurate in describing real-world cascades than the standard (i.e., topic-blind) propagation models studied in the literature. In particular, we first propose simple topic-aware extensions of the well-known Independent Cascade and Linear Threshold models. However, these propagation models have a very large number of parameters which could lead to overfitting. Therefore, we propose a different approach explicitly modeling authoritativeness, influence and relevance under a topic-aware perspective. Instead of considering user-to-user influence, the proposed model focuses on user authoritativeness and interests in a topic, leading to a drastic reduction in the number of parameters of the model. We devise methods to learn the parameters of the models from a data set of past propagations. Our experimentation confirms the high accuracy of the proposed models and learning schemes.},
  langid = {english},
  keywords = {Social influence,Topic modeling,Topic-aware propagation model,Viral marketing}
}

@inproceedings{barbieriTweetEvalUnifiedBenchmark2020,
  title = {{{TweetEval}}: {{Unified Benchmark}} and {{Comparative Evaluation}} for {{Tweet Classification}}},
  shorttitle = {{{TweetEval}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Barbieri, Francesco and Camacho-Collados, Jose and Espinosa Anke, Luis and Neves, Leonardo},
  date = {2020-11},
  pages = {1644--1650},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.148},
  url = {https://aclanthology.org/2020.findings-emnlp.148},
  urldate = {2022-09-08},
  abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.},
  eventtitle = {{{EMNLP-Findings}} 2020},
  file = {/Users/hugo/Papers/pdf/barbieriTweetEvalUnifiedBenchmark2020a.pdf}
}

@article{barronUniversalApproximationBounds1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A. R.},
  date = {1993-05},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {1557-9654},
  doi = {10.1109/18.256500},
  abstract = {Approximation properties of a class of artificial neural networks are established. It is shown that feedforward networks with one layer of sigmoidal nonlinearities achieve integrated squared error of order O(1/n), where n is the number of nodes. The approximated function is assumed to have a bound on the first moment of the magnitude distribution of the Fourier transform. The nonlinear parameters associated with the sigmoidal nodes, as well as the parameters of linear combination, are adjusted in the approximation. In contrast, it is shown that for series expansions with n terms, in which only the parameters of linear combination are adjusted, the integrated squared approximation error cannot be made smaller than order 1/n/sup 2/d/ uniformly for functions satisfying the same smoothness assumption, where d is the dimension of the input to the function. For the class of functions examined, the approximation rate and the parsimony of the parameterization of the networks are shown to be advantageous in high-dimensional settings.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {Approximation error,approximation theory,artificial neural networks,Artificial neural networks,error analysis,feedforward networks,feedforward neural nets,Feedforward neural networks,Feeds,Fourier transform,Fourier transforms,function approximation,high-dimensional settings,information theory,Information theory,integrated squared error,Linear approximation,linear combination,magnitude distribution,Neural networks,nonlinear parameters,parsimony,series expansions,sigmoidal function,sigmoidal nonlinearities,Statistical distributions,Statistics,superpositions,universal approximation bounds},
  file = {/Users/hugo/Papers/pdf/barronUniversalApproximationBounds1993.pdf;/Users/hugo/Zotero/storage/53HLDRG4/256500.html}
}

@inproceedings{bartlettLearningChangingConcepts1996,
  title = {Learning Changing Concepts by Exploiting the Structure of Change},
  booktitle = {Proceedings of the Ninth Annual Conference on {{Computational}} Learning Theory  - {{COLT}} '96},
  author = {Bartlett, Peter L. and Ben-David, Shai and Kulkarni, Sanjeev R.},
  date = {1996},
  pages = {131--139},
  publisher = {{ACM Press}},
  location = {{Desenzano del Garda, Italy}},
  doi = {10.1145/238061.238080},
  url = {http://portal.acm.org/citation.cfm?doid=238061.238080},
  urldate = {2022-08-24},
  eventtitle = {The Ninth Annual Conference},
  isbn = {978-0-89791-811-4},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bartlettLearningChangingConcepts1996.pdf}
}

@article{bartlettRademacherGaussianComplexities2002,
  title = {Rademacher and {{Gaussian Complexities}}: {{Risk Bounds}} and {{Structural Results}}},
  shorttitle = {Rademacher and {{Gaussian Complexities}}},
  author = {Bartlett, Peter L. and Mendelson, Shahar},
  date = {2002},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {3},
  pages = {463--482},
  url = {http://jmlr.org/papers/v3/bartlett02a.html},
  urldate = {2022-03-07}
}

@inproceedings{battagliaInteractionNetworksLearning2016,
  title = {Interaction {{Networks}} for {{Learning}} about {{Objects}}, {{Relations}} and {{Physics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Jimenez Rezende, Danilo and {kavukcuoglu}, koray},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {4502--4510},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6418-interaction-networks-for-learning-about-objects-relations-and-physics.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/battagliaInteractionNetworksLearning22.pdf;/Users/hugo/Zotero/storage/SKELMNX3/1612.html}
}

@unpublished{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-06-04},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2018-12-28},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one’s experiences—a hallmark of human intelligence from infancy—remains a formidable challenge for modern AI.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/H29ZHJ8Z/battagliaRelationalInductiveBiases2018.pdf}
}

@incollection{baxterTheoreticalModelsLearning1998,
  title = {Theoretical {{Models}} of {{Learning}} to {{Learn}}},
  booktitle = {Learning to {{Learn}}},
  author = {Baxter, Jonathan},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  date = {1998},
  pages = {71--94},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4615-5529-2_4},
  url = {https://doi.org/10.1007/978-1-4615-5529-2_4},
  urldate = {2022-08-24},
  abstract = {A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an environment of related tasks, then it can learn its own bias by learning sufficiently many tasks from the environment [Baxter, 1995b; Baxter, 1997]. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.},
  isbn = {978-1-4615-5529-2},
  langid = {english},
  keywords = {Empirical Process,Feature Weight,Hypothesis Space,Output Weight,Prior Distribution},
  file = {/Users/hugo/Papers/pdf/baxterTheoreticalModelsLearning1998.pdf}
}

@unpublished{beaulieuLearningContinuallyLearn2020,
  title = {Learning to {{Continually Learn}}},
  author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
  date = {2020-03-03},
  eprint = {2002.09571},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.09571},
  urldate = {2020-06-09},
  abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables contextdependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/beaulieuLearningContinuallyLearn2020.pdf}
}

@online{beautifulInformationBeautiful,
  title = {Information Is {{Beautiful}}},
  author = {is Beautiful, Information},
  url = {https://informationisbeautiful.net/},
  urldate = {2018-06-18},
  abstract = {Distilling the world's data, information \& knowledge into beautiful infographics \& visualizations},
  langid = {english},
  organization = {{Information is Beautiful}},
  file = {/Users/hugo/Zotero/storage/3CS89HDL/informationisbeautiful.net.html}
}

@unpublished{beckhamAdversarialMixupResynthesis2019,
  title = {On {{Adversarial Mixup Resynthesis}}},
  author = {Beckham, Christopher and Honari, Sina and Verma, Vikas and Lamb, Alex and Ghadiri, Farnoosh and Hjelm, R. Devon and Bengio, Yoshua and Pal, Christopher},
  date = {2019-10-23},
  eprint = {1903.02709},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.02709},
  urldate = {2019-12-11},
  abstract = {In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/LQ36UG5I/beckhamAdversarialMixupResynthesis2019.pdf}
}

@book{bedauEmergenceContemporaryReadings2008,
  title = {Emergence: Contemporary Readings in Philosophy and Science},
  shorttitle = {Emergence},
  editor = {Bedau, Mark and Humphreys, Paul},
  date = {2008},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-02621-5 978-0-262-52475-9},
  pagetotal = {464},
  keywords = {Emergence (Philosophy),Philosophy,Science},
  annotation = {OCLC: ocm79002063},
  file = {/Users/hugo/Papers/pdf/bedauEmergenceContemporaryReadings2008.pdf}
}

@article{bedauWeakEmergence1997,
  title = {Weak Emergence},
  author = {Bedau, Mark A.},
  date = {1997},
  journaltitle = {Philosophical perspectives},
  volume = {11},
  pages = {375--399},
  publisher = {{JSTOR}},
  file = {/Users/hugo/Papers/pdf/bedauWeakEmergence1997.pdf}
}

@article{beerCharacterizingAutopoiesisGame2015,
  title = {Characterizing {{Autopoiesis}} in the {{Game}} of {{Life}}},
  author = {Beer, Randall D.},
  date = {2015-02},
  journaltitle = {Artificial Life},
  volume = {21},
  number = {1},
  pages = {1--19},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/ARTL_a_00143},
  url = {http://www.mitpressjournals.org/doi/10.1162/ARTL_a_00143},
  urldate = {2020-07-20},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/beerCharacterizingAutopoiesisGame2015.pdf}
}

@article{beerCognitiveDomainGlider2014,
  title = {The Cognitive Domain of a Glider in the Game of Life},
  author = {Beer, Randall D.},
  date = {2014},
  journaltitle = {Artificial Life},
  shortjournal = {Artif. Life},
  volume = {20},
  number = {2},
  eprint = {24494612},
  eprinttype = {pmid},
  pages = {183--206},
  issn = {1064-5462},
  doi = {10.1162/ARTL_a_00125},
  abstract = {This article examines in some technical detail the application of Maturana and Varela's biology of cognition to a simple concrete model: a glider in the game of Life cellular automaton. By adopting an autopoietic perspective on a glider, the set of possible perturbations to it can be divided into destructive and nondestructive subsets. From a glider's reaction to each nondestructive perturbation, its cognitive domain is then mapped. In addition, the structure of a glider's possible knowledge of its immediate environment, and the way in which that knowledge is grounded in its constitution, are fully described. The notion of structural coupling is then explored by characterizing the paths of mutual perturbation that a glider and its environment can undergo. Finally, a simple example of a communicative interaction between two gliders is given. The article concludes with a discussion of the potential implications of this analysis for the enactive approach to cognition.},
  langid = {english},
  keywords = {Artificial Intelligence,Cognition,Game Theory,Humans,Life,Models; Biological,Models; Neurological,Models; Theoretical}
}

@article{beerIntegratedPerspectiveConstitutive2020,
  title = {An {{Integrated Perspective}} on the {{Constitutive}} and {{Interactive Dimensions}} of {{Autonomy}}},
  author = {Beer, Randall D.},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {202--209},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00245},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00245},
  urldate = {2020-07-27},
  abstract = {Enaction's claim of continuity between life and mind is a bold one. We investigate one aspect of this claim using a glider in the Game of Life as a toy model. Specifically, we study the relationship between theories of glider constitution and glider interaction, demonstrating how a glider's constitution completely determines its interaction graph, but not the particular life that it enacts, which also requires knowledge of the dynamics of its environment.},
  file = {/Users/hugo/Papers/pdf/beerIntegratedPerspectiveConstitutive2020.pdf;/Users/hugo/Zotero/storage/9PKUARSX/isal_a_00245.html}
}

@unpublished{belkinReconcilingModernMachine2019,
  title = {Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  date = {2019-09-10},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.11118},
  urldate = {2022-03-07},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/belkinReconcilingModernMachine2019.pdf}
}

@inproceedings{bellemareUnifyingCountbasedExploration2016,
  title = {Unifying Count-Based Exploration and Intrinsic Motivation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  date = {2016},
  pages = {1471--1479},
  file = {/Users/hugo/Papers/pdf/bellemareUnifyingCountbasedExploration2016.pdf;/Users/hugo/Zotero/storage/QGKDFXLG/d43ab110ab2489d6b9b2caa394bf920f-Abstract.html}
}

@inproceedings{ben-davidExploitingTaskRelatedness2003,
  title = {Exploiting {{Task Relatedness}} for {{Multiple Task Learning}}},
  booktitle = {Learning {{Theory}} and {{Kernel Machines}}},
  author = {Ben-David, Shai and Schuller, Reba},
  editor = {Schölkopf, Bernhard and Warmuth, Manfred K.},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {567--580},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45167-9_41},
  abstract = {The approach of learning of multiple “related” tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow “algorithmically related”, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task.},
  isbn = {978-3-540-45167-9},
  langid = {english}
}

@article{ben-davidNotionTaskRelatedness2008,
  title = {A Notion of Task Relatedness Yielding Provable Multiple-Task Learning Guarantees},
  author = {Ben-David, Shai and Borbely, Reba Schuller},
  date = {2008-12-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {73},
  number = {3},
  pages = {273--287},
  issn = {1573-0565},
  doi = {10.1007/s10994-007-5043-5},
  url = {https://doi.org/10.1007/s10994-007-5043-5},
  urldate = {2022-08-24},
  abstract = {The approach of learning multiple “related” tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow “algorithmically related”, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underlie these tasks.},
  langid = {english},
  keywords = {Classification prediction,Generalization bounds,Inductive transfer,Learning theory,Multi-task learning,Task relatedness,VC-dimension},
  file = {/Users/hugo/Papers/pdf/ben-davidNotionTaskRelatedness2008.pdf}
}

@inproceedings{benderClimbingNLUMeaning2020,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  shorttitle = {Climbing towards {{NLU}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bender, Emily M. and Koller, Alexander},
  date = {2020-07},
  pages = {5185--5198},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  url = {https://www.aclweb.org/anthology/2020.acl-main.463},
  urldate = {2020-07-16},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We've Been and Where We're Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/hugo/Papers/pdf/benderClimbingNLUMeaning2020.pdf}
}

@article{bengioLearningLongtermDependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  date = {1994-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  issn = {1941-0093},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,Intelligent networks,Neural networks,Neurofeedback,Production,Recurrent neural networks},
  file = {/Users/hugo/Papers/pdf/bengioLearningLongtermDependencies1994.pdf;/Users/hugo/Zotero/storage/GK3B286S/279181.html}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
  date = {2003},
  journaltitle = {Journal of Machine Learning Research},
  volume = {3},
  pages = {1137--1155},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v3/bengio03a.html},
  urldate = {2019-07-03},
  issue = {Feb},
  file = {/Users/hugo/Zotero/storage/YFQQJ368/bengioNeuralProbabilisticLanguage2003.pdf;/Users/hugo/Zotero/storage/K5VVNDFA/bengio03a.html}
}

@unpublished{bengtssonImportanceBeingUnistochastic2004,
  title = {The {{Importance}} of {{Being Unistochastic}}},
  author = {Bengtsson, Ingemar},
  date = {2004-03-11},
  eprint = {quant-ph/0403088},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/quant-ph/0403088},
  urldate = {2021-08-30},
  abstract = {A bistochastic matrix is a square matrix with positive entries such that rows and columns sum to unity. A unistochastic matrix is a bistochastic matrix whose matrix elements are the absolute values squared of a unitary matrix. We can now ask questions such as when a given bistochastic matrix is unistochastic. I review these questions: Why they are asked, why they are difficult to answer, and what is known about them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantum Physics},
  file = {/Users/hugo/Zotero/storage/MMQ5APJT/Bengtsson - 2004 - The Importance of Being Unistochastic.pdf}
}

@incollection{bennettLogicalDepthPhysical1995,
  title = {Logical {{Depth}} and {{Physical Complexity}}},
  booktitle = {The {{Universal Turing Machine A Half-Century Survey}}},
  author = {Bennett, Charles H.},
  editor = {Herken, Rolf},
  date = {1995},
  volume = {2},
  pages = {207--235},
  publisher = {{Springer Vienna}},
  location = {{Vienna}},
  doi = {10.1007/978-3-7091-6597-3_8},
  url = {http://www.springerlink.com/index/10.1007/978-3-7091-6597-3_8},
  urldate = {2019-09-05},
  abstract = {Some mathematical and natural objects (a random sequence, a sequence of zeros, a perfect crystal, a gas) are intuitively trivial, while others (e.g. the human body, the digits of π) contain internal evidence of a nontrivial causal history.},
  editorb = {Herken, Rolf},
  editorbtype = {redactor},
  isbn = {978-3-211-82637-9 978-3-7091-6597-3},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/EJRQMHI8/bennettLogicalDepthPhysical1995.pdf}
}

@book{berlekampWinningWaysYour1982,
  title = {Winning Ways, for Your Mathematical Plays},
  author = {Berlekamp, Elwyn R. and Conway, John H. and Guy, Richard K.},
  date = {1982},
  publisher = {{Academic Press}},
  location = {{London ; New York}},
  isbn = {978-0-12-091150-9},
  pagetotal = {1},
  keywords = {Mathematical recreations}
}

@book{berlekampWinningWaysYour2001,
  title = {Winning Ways for Your Mathematical Plays},
  author = {Berlekamp, Elwyn R. and Conway, John Horton and Guy, Richard K.},
  date = {2001},
  edition = {2nd ed},
  publisher = {{A.K. Peters}},
  location = {{Natick, Mass}},
  isbn = {978-1-56881-130-7 978-1-56881-142-0 978-1-56881-143-7 978-1-56881-144-4},
  pagetotal = {4},
  keywords = {Mathematical recreations}
}

@book{berners-leeWeavingWebOriginal1999,
  title = {Weaving the {{Web}}: {{The Original Design}} and {{Ultimate Destiny}} of the {{World Wide Web}} by {{Its Inventor}}},
  shorttitle = {Weaving the {{Web}}},
  author = {Berners-Lee, Tim},
  date = {1999},
  publisher = {{Harper San Francisco}},
  abstract = {From the Publisher:Tim Berners-Lee, the inventor of the World Wide Web, has been hailed by Time magazine as one of the 100 greatest minds of this century. His creation has already changed the way people do business, entertain themselves, exchange ideas, and socialize with one another.. "Berners-Lee offers insights to help readers understand the true nature of the Web, enabling them to use it to their fullest advantage. He shares his views on such critical issues as censorship, privacy, the increasing power of software companies in the online world, and the need to find the ideal balance between the commercial and social forces on the Web. His criticism of the Web's current state makes clear that there is still much work to be done. Finally, Berners-Lee presents his own plan for the Web's future, one that calls for the active support and participation of programmers, computer manufacturers, and social organizations to make it happen.},
  isbn = {978-1-4028-4293-1}
}

@unpublished{beyerKnowledgeDistillationGood2021,
  title = {Knowledge Distillation: {{A}} Good Teacher Is Patient and Consistent},
  shorttitle = {Knowledge Distillation},
  author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
  date = {2021-06-09},
  eprint = {2106.05237},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.05237},
  urldate = {2021-06-14},
  abstract = {There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8\% top-1 accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/AF3AGI73/Beyer et al. - 2021 - Knowledge distillation A good teacher is patient .pdf}
}

@unpublished{bhattacharjeeSurveyCellularAutomata2016,
  title = {A {{Survey}} of {{Cellular Automata}}: {{Types}}, {{Dynamics}}, {{Non-uniformity}} and {{Applications}}},
  shorttitle = {A {{Survey}} of {{Cellular Automata}}},
  author = {Bhattacharjee, Kamalika and Naskar, Nazma and Roy, Souvik and Das, Sukanta},
  date = {2016-07-08},
  eprint = {1607.02291},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1607.02291},
  urldate = {2019-06-05},
  abstract = {Cellular automata (CAs) are dynamical systems which exhibit complex global behavior from simple local interaction and computation. Since the inception of cellular automaton (CA) by von Neumann in 1950s, it has attracted the attention of several researchers over various backgrounds and fields for modelling different physical, natural as well as reallife phenomena. Classically, CAs are uniform. However, non-uniformity has also been introduced in update pattern, lattice structure, neighborhood dependency and local rule. In this survey, we tour to the various types of CAs introduced till date, the different characterization tools, the global behaviors of CAs, like universality, reversibility, dynamics etc. Special attention is given to non-uniformity in CAs and especially to non-uniform elementary CAs, which have been very useful in solving several real-life problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Formal Languages and Automata Theory,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Zotero/storage/TSJV75WX/bhattacharjeeSurveyCellularAutomata2016.pdf}
}

@article{bhattacharyaCollectiveEntityResolution2007,
  title = {Collective Entity Resolution in Relational Data},
  author = {Bhattacharya, Indrajit and Getoor, Lise},
  date = {2007-03-01},
  journaltitle = {ACM Transactions on Knowledge Discovery from Data},
  volume = {1},
  number = {1},
  pages = {5-es},
  issn = {15564681},
  doi = {10.1145/1217299.1217304},
  url = {http://portal.acm.org/citation.cfm?doid=1217299.1217304},
  urldate = {2018-05-07},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bhattacharyaCollectiveEntityResolution22.pdf}
}

@inproceedings{bidloEvolutionCellularAutomata2013,
  title = {Evolution of Cellular Automata with Conditionally Matching Rules},
  booktitle = {2013 {{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {Bidlo, Michal and Vasicek, Zdenek},
  date = {2013-06},
  pages = {1178--1185},
  publisher = {{IEEE}},
  location = {{Cancun, Mexico}},
  doi = {10.1109/CEC.2013.6557699},
  url = {http://ieeexplore.ieee.org/document/6557699/},
  urldate = {2019-05-09},
  eventtitle = {2013 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  isbn = {978-1-4799-0454-9 978-1-4799-0453-2 978-1-4799-0451-8 978-1-4799-0452-5},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/V8VCWJHJ/bidloEvolutionCellularAutomata2013.pdf}
}

@article{bidloRoutineEvolutionComplex2016,
  title = {On {{Routine Evolution}} of {{Complex Cellular Automata}}},
  author = {Bidlo, Michal},
  date = {2016-10},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  shortjournal = {IEEE Trans. Evol. Computat.},
  volume = {20},
  number = {5},
  pages = {742--754},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2016.2516242},
  url = {http://ieeexplore.ieee.org/document/7377086/},
  urldate = {2020-06-09},
  file = {/Users/hugo/Papers/pdf/bidloRoutineEvolutionComplex2016.pdf}
}

@article{bilottaARTIFICIALMICROWORLDSPART2011,
  title = {{{ARTIFICIAL MICRO-WORLDS PART II}}: {{CELLULAR AUTOMATA GROWTH DYNAMICS}}},
  shorttitle = {{{ARTIFICIAL MICRO-WORLDS PART II}}},
  author = {Bilotta, Eleonora and Pantano, Pietro},
  date = {2011-03},
  journaltitle = {International Journal of Bifurcation and Chaos},
  volume = {21},
  number = {03},
  pages = {619--645},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S0218127411028672},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218127411028672},
  urldate = {2019-09-02},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/MXB3UVBD/bilottaARTIFICIALMICROWORLDSPART2011.pdf}
}

@article{bilottaARTIFICIALMICROWORLDSPART2011a,
  title = {{{ARTIFICIAL MICRO-WORLDS PART I}}: {{A NEW APPROACH FOR STUDYING LIFE-LIKE PHENOMENA}}},
  shorttitle = {{{ARTIFICIAL MICRO-WORLDS PART I}}},
  author = {Bilotta, Eleonora and Pantano, Pietro and Vena, Stefano},
  date = {2011-02},
  journaltitle = {International Journal of Bifurcation and Chaos},
  volume = {21},
  number = {02},
  pages = {373--398},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S0218127411028659},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218127411028659},
  urldate = {2019-09-02},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bilottaARTIFICIALMICROWORLDSPART2011a2.pdf}
}

@article{bishopMixtureDensityNetworks1994,
  title = {Mixture Density Networks},
  author = {Bishop, Christopher M.},
  date = {1994},
  publisher = {{Aston University}},
  file = {/Users/hugo/Papers/pdf/bishopMixtureDensityNetworks1994.pdf;/Users/hugo/Zotero/storage/25XMV8X5/373.html}
}

@article{bizerLinkedDataStory,
  title = {Linked {{Data}} - {{The Story So Far}}},
  author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
  pages = {26},
  abstract = {The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions - the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bizerLinkedDataStory2.pdf}
}

@inproceedings{blitzerDomainAdaptationStructural2006,
  title = {Domain {{Adaptation}} with {{Structural Correspondence Learning}}},
  booktitle = {Proceedings of the 2006 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Blitzer, John and McDonald, Ryan and Pereira, Fernando},
  date = {2006-07},
  pages = {120--128},
  publisher = {{Association for Computational Linguistics}},
  location = {{Sydney, Australia}},
  url = {https://aclanthology.org/W06-1615},
  urldate = {2022-02-23},
  file = {/Users/hugo/Papers/pdf/blitzerDomainAdaptationStructural2006.pdf}
}

@unpublished{blondelFastDifferentiableSorting2020,
  title = {Fast {{Differentiable Sorting}} and {{Ranking}}},
  author = {Blondel, Mathieu and Teboul, Olivier and Berthet, Quentin and Djolonga, Josip},
  date = {2020-06-29},
  eprint = {2002.08871},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.08871},
  urldate = {2020-07-02},
  abstract = {The sorting operation is one of the most commonly used building blocks in computer programming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the O(n log n) time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with O(n log n) time and O(n) space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman’s rank correlation coefficient and least trimmed squares.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/blondelFastDifferentiableSorting2020.pdf}
}

@unpublished{blondelSmoothSparseOptimal2017,
  title = {Smooth and {{Sparse Optimal Transport}}},
  author = {Blondel, Mathieu and Seguy, Vivien and Rolet, Antoine},
  date = {2017-10-17},
  eprint = {1710.06276},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.06276},
  urldate = {2018-11-22},
  abstract = {Entropic regularization is quickly emerging as a new standard in optimal transport (OT). It enables to cast the OT computation as a differentiable and unconstrained convex optimization problem, which can be efficiently solved using the Sinkhorn algorithm. However, entropy keeps the transportation plan strictly positive and therefore completely dense, unlike unregularized OT. This lack of sparsity can be problematic in applications where the transportation plan itself is of interest. In this paper, we explore regularizing the primal and dual OT formulations with a strongly convex term, which corresponds to relaxing the dual and primal constraints with smooth approximations. We show how to incorporate squared 2-norm and group lasso regularizations within that framework, leading to sparse and group-sparse transportation plans. On the theoretical side, we bound the approximation error introduced by regularizing the primal and dual formulations. Our results suggest that, for the regularized primal, the approximation error can often be smaller with squared 2-norm than with entropic regularization. We showcase our proposed framework on the task of color transfer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/blondelSmoothSparseOptimal22.pdf}
}

@online{BLOOM,
  title = {{{BLOOM}}},
  url = {https://bigscience.huggingface.co/blog/bloom},
  urldate = {2022-07-22},
  abstract = {Our 176B parameter language model is here.},
  file = {/Users/hugo/Zotero/storage/TWQ8Y7EL/bloom.html}
}

@article{blumTraining3nodeNeural1992,
  title = {Training a 3-Node Neural Network Is {{NP-complete}}},
  author = {Blum, Avrim L. and Rivest, Ronald L.},
  date = {1992-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {5},
  number = {1},
  pages = {117--127},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(05)80010-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608005800103},
  urldate = {2022-03-07},
  abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids.},
  langid = {english},
  keywords = {Computational complexity,Intractability,Learning,Multilayer perceptron,Neural networks,NP-completeness,Representation,Training},
  file = {/Users/hugo/Papers/pdf/blumTraining3nodeNeural1992.pdf;/Users/hugo/Zotero/storage/7CMIR5NK/S0893608005800103.html}
}

@incollection{boasCellularPottsModel2018,
  title = {Cellular {{Potts Model}}: {{Applications}} to {{Vasculogenesis}} and {{Angiogenesis}}},
  shorttitle = {Cellular {{Potts Model}}},
  booktitle = {Probabilistic {{Cellular Automata}}: {{Theory}}, {{Applications}} and {{Future Perspectives}}},
  author = {Boas, Sonja E. M. and Jiang, Yi and Merks, Roeland M. H. and Prokopiou, Sotiris A. and Rens, Elisabeth G.},
  editor = {Louis, Pierre-Yves and Nardi, Francesca R.},
  date = {2018},
  series = {Emergence, {{Complexity}} and {{Computation}}},
  pages = {279--310},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-65558-1_18},
  url = {https://doi.org/10.1007/978-3-319-65558-1_18},
  urldate = {2022-11-03},
  abstract = {The cellular Potts model (CPM, a.k.a. Glazier–Graner–Hogeweg or GGH model) is a somewhat liberal extension of probabilistic cellular automata. The model is derived from the Ising and Potts models and represents biological cells as domains of CA-sites of the same state. A Hamiltonian energy is used to describe the balance of forces that the biological cells apply onto one another and their local environment. A Metropolis algorithm iteratively copies the state from one site into one of the adjacent sites, thus shifting the domain interfaces and moving the biological cells along the lattice. The approach is commonly used in applications of developmental biology, where the CPM often interacts with systems of ordinary-differential equations that model the intracellular chemical kinetics and partial-differential equations that model the extracellular chemical signal dynamics to constitute a hybrid and multiscale description of the biological system. In this chapter we will introduce the cellular Potts model and discuss its use in developmental biology, focusing on the development of blood vessels, a process called vascular morphogenesis. We will start by introducing a range of models focusing on uncovering the basic mechanisms of vascular morphogenesis: network formation and sprouting and then show how these models are extended with models of intracellular regulation and with interactions with the extracellular micro-environment. We then briefly review the integration of models of vascular morphogenesis in several examples of organ development in health and disease, including development, cancer, and age-related macular degeneration. We end by discussing the computational efficiency of the CPM and the available strategies for the validation of CPM-based simulation models.},
  isbn = {978-3-319-65558-1},
  langid = {english},
  keywords = {Angiogenesis,Cellular Potts model,Delta-Notch,Extracellular matrix,Mechanical signaling,Multiscale modeling,Vasculogenesis,VEGF},
  file = {/Users/hugo/Papers/pdf/boasCellularPottsModel2018.pdf}
}

@book{boccaraModelingComplexSystems2010,
  title = {Modeling {{Complex Systems}}},
  author = {Boccara, Nino},
  date = {2010-09-09},
  eprint = {boUorPmcbKMC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The essential points of this ?rst chapter are • The de?nition of a complex system • The notion of emergence • The de?nition of a model • The notion of dynamical system This book is about the dynamics of complex systems. Roughly speaking, a system is a collection of interacting elements making up a whole such as, for instance, a mechanical clock. While many systems may be quite complicated, they are not necessarily considered to be complex. Today, most authors agree on the essential properties a system has to possess to be called complex. The ?rst section is devoted to the description of these properties. To interpret the time evolution of a system, scientists build up models, which are simpli?ed mathematical representations of the system. The exact purpose of a model and what its essential features should be is explained in the second section. The mathematical models that will be discussed in this book are dynami- 1 calsystems. A dynamical system is essentially a set of equations whose- lutiondescribesthe evolution,asafunction oftime, ofthe state ofthe system. There exist di?erent types of dynamical systems. Some of them are de?ned in the third section. 1 There is an extensive literature on mathematical modeling. The reader may, for example, consult [11,88,163,233].},
  isbn = {978-1-4419-6562-2},
  langid = {english},
  pagetotal = {500},
  keywords = {Computers / Computer Science,Computers / Computer Simulation,Computers / Programming / Algorithms,Computers / Software Development & Engineering / General,Mathematics / Applied,Mathematics / Counting & Numeration,Mathematics / Numerical Analysis,Science / Physics / General,Science / Physics / Mathematical & Computational}
}

@article{boccaraParticlelikeStructuresTheir1991,
  title = {Particlelike Structures and Their Interactions in Spatiotemporal Patterns Generated by One-Dimensional Deterministic Cellular-Automaton Rules},
  author = {Boccara, N. and Nasser, J. and Roger, M.},
  date = {1991-07-01},
  journaltitle = {Physical Review A},
  shortjournal = {Phys. Rev. A},
  volume = {44},
  number = {2},
  pages = {866--875},
  issn = {1050-2947, 1094-1622},
  doi = {10.1103/PhysRevA.44.866},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.44.866},
  urldate = {2020-03-30},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/boccaraParticlelikeStructuresTheir1991.pdf}
}

@article{bojanowskiEnrichingWordVectors,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  pages = {12},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bojanowskiEnrichingWordVectors2.pdf}
}

@article{bollackerFreebaseCollaborativelyCreated2008,
  title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
  author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  date = {2008},
  journaltitle = {SIGMOD 08 Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  eprint = {3105260},
  eprinttype = {pmid},
  pages = {1247--1250},
  issn = {07308078},
  doi = {10.1145/1376616.1376746},
  url = {http://ids.snu.ac.kr/w/images/9/98/SC17.pdf http://doi.acm.org/10.1145/1376616.1376746},
  abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
  keywords = {E1 Data Structures,Graphs and networks General Terms Design,Human Factors,Information Networks,Languages},
  file = {/Users/hugo/Papers/pdf/bollackerFreebaseCollaborativelyCreated22.pdf}
}

@article{bolltExplainingSurprisingSuccess2021,
  title = {On Explaining the Surprising Success of Reservoir Computing Forecaster of Chaos? {{The}} Universal Machine Learning Dynamical System with Contrast to {{VAR}} and {{DMD}}},
  shorttitle = {On Explaining the Surprising Success of Reservoir Computing Forecaster of Chaos?},
  author = {Bollt, Erik},
  date = {2021-01},
  journaltitle = {Chaos (Woodbury, N.Y.)},
  shortjournal = {Chaos},
  volume = {31},
  number = {1},
  eprint = {33754755},
  eprinttype = {pmid},
  pages = {013108},
  issn = {1089-7682},
  doi = {10.1063/5.0024890},
  abstract = {Machine learning has become a widely popular and successful paradigm, especially in data-driven science and engineering. A major application problem is data-driven forecasting of future states from a complex dynamical system. Artificial neural networks have evolved as a clear leader among many machine learning approaches, and recurrent neural networks are considered to be particularly well suited for forecasting dynamical systems. In this setting, the echo-state networks or reservoir computers (RCs) have emerged for their simplicity and computational complexity advantages. Instead of a fully trained network, an RC trains only readout weights by a simple, efficient least squares method. What is perhaps quite surprising is that nonetheless, an RC succeeds in making high quality forecasts, competitively with more intensively trained methods, even if not the leader. There remains an unanswered question as to why and how an RC works at all despite randomly selected weights. To this end, this work analyzes a further simplified RC, where the internal activation function is an identity function. Our simplification is not presented for the sake of tuning or improving an RC, but rather for the sake of analysis of what we take to be the surprise being not that it does not work better, but that such random methods work at all. We explicitly connect the RC with linear activation and linear readout to well developed time-series literature on vector autoregressive (VAR) averages that includes theorems on representability through the Wold theorem, which already performs reasonably for short-term forecasts. In the case of a linear activation and now popular quadratic readout RC, we explicitly connect to a nonlinear VAR, which performs quite well. Furthermore, we associate this paradigm to the now widely popular dynamic mode decomposition; thus, these three are in a sense different faces of the same thing. We illustrate our observations in terms of popular benchmark examples including Mackey-Glass differential delay equations and the Lorenz63 system.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bolltExplainingSurprisingSuccess2021.pdf}
}

@article{bookerClassifierSystemsGenetic1989,
  title = {Classifier Systems and Genetic Algorithms},
  author = {Booker, Lashon B. and Goldberg, David E. and Holland, John H.},
  date = {1989},
  journaltitle = {Artificial intelligence},
  volume = {40},
  number = {1-3},
  pages = {235--282},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Papers/pdf/bookerClassifierSystemsGenetic1989.pdf;/Users/hugo/Zotero/storage/MPAS52IX/0004370289900507.html}
}

@book{bookerPerspectivesAdaptationNatural2004,
  title = {Perspectives on {{Adaptation}} in {{Natural}} and {{Artificial Systems}} ({{Proceedings Volume}} in the {{Santa Fe Institute Studies}} in the {{Sciences}} of {{comPlexity}}.)},
  author = {Booker, Lashon},
  date = {2004},
  publisher = {{Oxford University Press, Inc.}},
  location = {{New York, NY, USA}},
  isbn = {978-0-19-516293-6}
}

@article{bordesTranslatingEmbeddingsModeling2013,
  title = {Translating {{Embeddings}} for {{Modeling Multi-Relational Data}}},
  author = {Bordes, Antoine and Usunier, Nicolas and Weston, Jason and Yakhnenko, Oksana},
  date = {2013},
  journaltitle = {Advances in NIPS},
  volume = {26},
  eprint = {2328551},
  eprinttype = {pmid},
  pages = {2787--2795},
  issn = {10495258},
  doi = {10.1007/s13398-014-0173-7.2},
  abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose, TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.}
}

@article{bottouOptimizationMethodsLargeScale2017,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, Leon and Curtis, Frank E and Nocedal, Jorge},
  date = {2017-06},
  journaltitle = {Stat},
  volume = {1050},
  pages = {1--45},
  url = {http://arxiv.org/abs/1606.04838 http://arxiv.org/abs/1606.04838%0Ahttp://arxiv.org/abs/1606.04838%0Apapers3://publication/uuid/8F5A6B58-D611-4591-AA7C-2A483A319FC8},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  file = {/Users/hugo/Papers/pdf/bottouOptimizationMethodsLargeScale22.pdf}
}

@article{bottouOptimizationMethodsLargescale2018,
  title = {Optimization Methods for Large-Scale Machine Learning},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018},
  journaltitle = {Siam Review},
  volume = {60},
  number = {2},
  pages = {223--311},
  publisher = {{SIAM}},
  file = {/Users/hugo/Papers/pdf/bottouOptimizationMethodsLargescale2018.pdf;/Users/hugo/Zotero/storage/5A4XZPJM/16M1080173.html}
}

@unpublished{bouritsasPartitionCodeLearning2021,
  title = {Partition and {{Code}}: Learning How to Compress Graphs},
  shorttitle = {Partition and {{Code}}},
  author = {Bouritsas, Giorgos and Loukas, Andreas and Karalias, Nikolaos and Bronstein, Michael M.},
  date = {2021-07-05},
  eprint = {2107.01952},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2107.01952},
  urldate = {2021-09-30},
  abstract = {Can we use machine learning to compress graph data? The absence of ordering in graphs poses a significant challenge to conventional compression algorithms, limiting their attainable gains as well as their ability to discover relevant patterns. On the other hand, most graph compression approaches rely on domain-dependent handcrafted representations and cannot adapt to different underlying graph distributions. This work aims to establish the necessary principles a lossless graph compression method should follow to approach the entropy storage lower bound. Instead of making rigid assumptions about the graph distribution, we formulate the compressor as a probabilistic model that can be learned from data and generalise to unseen instances. Our “Partition and Code” framework entails three steps: first, a partitioning algorithm decomposes the graph into elementary structures, then these are mapped to the elements of a small dictionary on which we learn a probability distribution, and finally, an entropy encoder translates the representation into bits. All three steps are parametric and can be trained with gradient descent. We theoretically compare the compression quality of several graph encodings and prove, under mild conditions, a total ordering of their expected description lengths. Moreover, we show that, under the same conditions, PnC achieves compression gains w.r.t. the baselines that grow either linearly or quadratically with the number of vertices. Our algorithms are quantitatively evaluated on diverse real-world networks obtaining significant performance improvements with respect to different families of non-parametric and parametric graph compressors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/bouritsasPartitionCodeLearning2021.pdf}
}

@article{bourlardAutoassociationMultilayerPerceptrons1988,
  title = {Auto-Association by Multilayer Perceptrons and Singular Value Decomposition},
  author = {Bourlard, H. and Kamp, Y.},
  date = {1988-09},
  journaltitle = {Biological Cybernetics},
  volume = {59},
  number = {4-5},
  pages = {291--294},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00332918},
  url = {http://link.springer.com/10.1007/BF00332918},
  urldate = {2020-02-25},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bourlardAutoassociationMultilayerPerceptrons1988.pdf}
}

@article{bourneSemidiscreteUnbalancedOptimal2018,
  title = {Semi-Discrete Unbalanced Optimal Transport and Quantization},
  author = {Bourne, David P. and Schmitzer, Bernhard and Wirth, Benedikt},
  date = {2018-08-06},
  url = {https://arxiv.org/abs/1808.01962v1},
  urldate = {2018-11-22},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/bourneSemidiscreteUnbalancedOptimal22.pdf;/Users/hugo/Zotero/storage/YC3FYU28/1808.html}
}

@inproceedings{brantDiversityPreservationMinimal2020,
  title = {Diversity Preservation in Minimal Criterion Coevolution through Resource Limitation},
  booktitle = {Proceedings of the 2020 {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Brant, Jonathan C. and Stanley, Kenneth O.},
  date = {2020-06-25},
  pages = {58--66},
  publisher = {{ACM}},
  location = {{Cancún Mexico}},
  doi = {10.1145/3377930.3389809},
  url = {https://dl.acm.org/doi/10.1145/3377930.3389809},
  urldate = {2020-07-30},
  abstract = {Minimal Criterion Coevolution (MCC) is a recently-introduced algorithm that demonstrates how interactions between two populations, each subject to a simple reproductive constraint, can produce an open-ended search process. Unlike conventional quality diversity (QD) algorithms, which also promote divergence, MCC does not require an explicit characterization of behavior or a comparison of performance, thereby addressing bottlenecks introduced by an intrinsically-finite behavior descriptor and by an assessment of comparative quality. Genetic speciation, a common method of diversity preservation, maintains population diversity in MCC; however, it requires an unnatural explicit comparison of genetic similarity. In nature, organisms are implicitly segregated into niches that each have a carrying capacity dictated by the amount of available resources. To show that MCC can be simpler and more natural while still working effectively, this paper introduces a method of diversity preservation through resource limitation, thereby alleviating the need to formalize and compare genetic distance. Experimental results in a maze navigation domain demonstrate that resource limitation not only maintains higher population diversity in both the maze and agent populations, but also accelerates evolution by forcing individuals to explore new niches, thereby suggesting that resource limitation is an effective, simpler, and more natural alternative for diversity preservation in MCC.},
  eventtitle = {{{GECCO}} '20: {{Genetic}} and {{Evolutionary Computation Conference}}},
  isbn = {978-1-4503-7128-5},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/brantDiversityPreservationMinimal2020.pdf}
}

@incollection{braunEvolvingNeuralFeedforward1993,
  title = {Evolving {{Neural Feedforward Networks}}},
  booktitle = {Artificial {{Neural Nets}} and {{Genetic Algorithms}}},
  author = {Braun, Heinrich and Weisbrod, Joachim},
  editor = {Albrecht, Rudolf F. and Reeves, Colin R. and Steele, Nigel C.},
  date = {1993},
  pages = {25--32},
  publisher = {{Springer Vienna}},
  location = {{Vienna}},
  doi = {10.1007/978-3-7091-7533-0_5},
  url = {http://link.springer.com/10.1007/978-3-7091-7533-0_5},
  urldate = {2020-02-08},
  isbn = {978-3-211-82459-7 978-3-7091-7533-0},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/CVW9DHGK/braunEvolvingNeuralFeedforward1993.pdf}
}

@inbook{breimanReflectionsRefereeingPapers2018,
  title = {Reflections {{After Refereeing Papers}} for {{NIPS}}},
  booktitle = {The {{Mathematics}} of {{Generalization}}},
  author = {Breiman, Leo},
  date = {2018-03-05},
  edition = {1},
  pages = {11--15},
  publisher = {{CRC Press}},
  doi = {10.1201/9780429492525-2},
  url = {https://www.taylorfrancis.com/books/9780429961076/chapters/10.1201/9780429492525-2},
  urldate = {2022-03-07},
  bookauthor = {Wolpert, David. H},
  isbn = {978-0-429-49252-5},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/breimanReflectionsRefereeingPapers2018.pdf}
}

@article{brooksIntelligenceReason1991,
  title = {Intelligence {{Without Reason}}},
  author = {Brooks, Rodney A},
  date = {1991},
  journaltitle = {Artificial Intelligence},
  volume = {47},
  pages = {27},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/brooksIntelligenceReason1991.pdf}
}

@inproceedings{brooksIntelligenceRepresentation1991,
  title = {Intelligence without Representation},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Brooks, Rodney A},
  date = {1991},
  volume = {1},
  pages = {12},
  abstract = {Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments.},
  eventtitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/brooksIntelligenceRepresentation1991.pdf}
}

@article{brownClassbasedNgramModels1992,
  title = {Class-Based {\emph{n}}-Gram Models of Natural Language},
  author = {Brown, Peter F. and {deSouza}, Peter V. and Mercer, Robert L. and Pietra, Vincent J. Della and Lai, Jenifer C.},
  date = {1992-12-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Comput. Linguist.},
  volume = {18},
  number = {4},
  pages = {467--479},
  issn = {0891-2017},
  abstract = {We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.}
}

@unpublished{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-06-04},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2020-06-24},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/brownLanguageModelsAre2020.pdf;/Users/hugo/Zotero/storage/PT8UXAWS/2005.html}
}

@article{buckmanSampleefficientReinforcementLearning2018,
  title = {Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion},
  author = {Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
  date = {2018},
  journaltitle = {Advances in neural information processing systems},
  volume = {31},
  file = {/Users/hugo/Papers/pdf/buckmanSampleefficientReinforcementLearning2018.pdf;/Users/hugo/Zotero/storage/IDBNPPRQ/f02208a057804ee16ac72ff4d3cec53b-Abstract.html}
}

@unpublished{buligaArtificialChemistryExperiments2020,
  title = {Artificial Chemistry Experiments with Chemlambda, Lambda Calculus, Interaction Combinators},
  author = {Buliga, Marius},
  date = {2020-03-31},
  eprint = {2003.14332},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/2003.14332},
  urldate = {2020-07-21},
  abstract = {Given a graph rewrite system, a graph G is a quine graph if it has a non-void maximal collection of non-conflicting matches of left patterns of graphs rewrites, such that after the parallel application of the rewrites we obtain a graph isomorphic with G. Such graphs exhibit a metabolism, they can multiply or they can die, when reduced by a random rewriting algorithm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Mathematics - Logic},
  file = {/Users/hugo/Papers/pdf/buligaArtificialChemistryExperiments2020.pdf}
}

@unpublished{buligaArtificialLifeProperties2020,
  title = {Artificial Life Properties of Directed Interaction Combinators vs. Chemlambda},
  author = {Buliga, M.},
  date = {2020-05-12},
  eprint = {2005.06060},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2005.06060},
  urldate = {2020-07-21},
  abstract = {We provide a framework for experimentation at this link with two artificial chemistries: directed interaction combinators (dirIC, defined in section 2) and chemlambda [5]. We are interested if these chemistries allow for artificial life behaviour: replication, metabolism and death.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Emerging Technologies,Quantitative Biology - Molecular Networks},
  file = {/Users/hugo/Papers/pdf/buligaArtificialLifeProperties2020.pdf}
}

@unpublished{burdaExplorationRandomNetwork2018,
  title = {Exploration by Random Network Distillation},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  date = {2018},
  eprint = {1810.12894},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/burdaExplorationRandomNetwork2018.pdf;/Users/hugo/Zotero/storage/8M8P65LM/1810.html}
}

@unpublished{burdaLargeScaleStudyCuriosityDriven2018,
  ids = {burdaLargescaleStudyCuriositydriven2018},
  title = {Large-{{Scale Study}} of {{Curiosity-Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  date = {2018-08-13},
  eprint = {1808.04355},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1808.04355},
  urldate = {2020-11-26},
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/burdaLargescaleStudyCuriositydriven2018.pdf;/Users/hugo/Zotero/storage/J444MWIS/Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf;/Users/hugo/Zotero/storage/UZE888BY/1808.html}
}

@thesis{buteraProgrammingPaintableComputer2002,
  type = {phdthesis},
  title = {Programming a Paintable Computer},
  author = {Butera, William Joseph},
  date = {2002},
  institution = {{Massachusetts Institute of Technology}},
  location = {{USA}},
  abstract = {A paintable computer is defined as an agglomerate of numerous, finely dispersed, ultra-miniaturized computing particles; each positioned randomly, running asynchronously and communicating locally. Individual particles are tightly resource bound, and processing is necessarily distributed. Yet computing elements are vanishingly cheap and are regarded as freely expendable. In this regime, a limiting problem is the distribution of processing over a particle ensemble whose topology can vary unexpectedly. The principles of material self-assembly are employed to guide the positioning of “process fragments”—autonomous, mobile pieces of a larger process. These fragments spatially position themselves and re-aggregate into a running process. We present the results of simulations to show that “process self-assembly” is viable, robust and supports a variety of useful applications on a paintable computer. We describe a hardware reference platform as an initial guide to the application domain. We describe a programming model which normatively defines the term process fragment and which provides environmental support for the fragment's mobility, scheduling and data exchange. The programming model is embodied in a simulator that supports development, test and visualization on a 2D particle ensemble. Experiments on simple combinations of fragments demonstrate robustness and explore the limits of scale invariance. Process fragments are shown interacting to approximate conservative fields, and using these fields to implement scaffolded and thermodynamic self-assembly. Four applications demonstrate practical relevance, delineate the application domain and collectively illustrate the paintable's capacity for storage, communication and signal processing. These four applications are Audio Streaming, Holistic Data Storage, Surface Bus and Image Segmentation. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)},
  annotation = {AAI0804036}
}

@article{byeInvestigationElementaryCellular2016,
  title = {Investigation of {{Elementary Cellular Automata}} for {{Reservoir Computing}}},
  author = {Bye, Emil Taylor},
  date = {2016},
  journaltitle = {51},
  publisher = {{NTNU}},
  url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2415318},
  urldate = {2020-10-06},
  abstract = {Reservoir computing is an approach to machine learning. Typical reservoir computing approaches use large, untrained artificial neural networks to transform an input signal. To produce the desired output, a readout layer is trained using linear regression on the neural network. Recently, several attempts have been made using other kinds of dynamic systems instead of artificial neural networks. Cellular automata are an example of a dynamic sys- tem that has been proposed as a replacement.- This thesis attempts to discover whether cellular automata are a viable candidate for use in reservoir computing. Four different tasks solved by other reservoir computing sys- tems are attempted with elementary cellular automata, a limited subset of all possible cellular automata. The effect of changing different properties of the cellular automata are investigated, and the results are compared with the results when performing the same experiments with typical reservoir computing systems. Reservoir computing seems like a potentially very interesting utilization of cellular automata. However, it is evident that more research into this field is necessary to reach performance comparable to existing reservoir computing systems.},
  langid = {english},
  annotation = {Accepted: 2016-10-14T14:00:44Z},
  file = {/Users/hugo/Papers/pdf/byeInvestigationElementaryCellular2016.pdf;/Users/hugo/Zotero/storage/UAFUQSJN/2415318.html}
}

@article{cafarellaStructuredDataWeb2011,
  title = {Structured Data on the Web},
  author = {Cafarella, Michael J. and Halevy, Alon and Madhavan, Jayant},
  date = {2011-02},
  journaltitle = {Communications of the ACM},
  volume = {54},
  number = {2},
  pages = {72},
  issn = {00010782},
  doi = {10.1145/1897816.1897839},
  url = {http://portal.acm.org/citation.cfm?doid=1897816.1897839},
  abstract = {Google's Web Tables and Deep Web Crawler identify and deliver this otherwise inaccessible resource directly to end users.}
}

@unpublished{caiProxylessNASDirectNeural2019,
  title = {{{ProxylessNAS}}: {{Direct Neural Architecture Search}} on {{Target Task}} and {{Hardware}}},
  shorttitle = {{{ProxylessNAS}}},
  author = {Cai, Han and Zhu, Ligeng and Han, Song},
  date = {2019-02-22},
  eprint = {1812.00332},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.00332},
  urldate = {2019-12-13},
  abstract = {Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. \$10\^4\$ GPU hours) makes it difficult to \textbackslash emph\{directly\} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize\textasciitilde\textbackslash emph\{proxy\} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \textbackslash emph\{ProxylessNAS\} that can \textbackslash emph\{directly\} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\textbackslash\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\$\textbackslash times\$ fewer parameters. On ImageNet, our model achieves 3.1\textbackslash\% better top-1 accuracy than MobileNetV2, while being 1.2\$\textbackslash times\$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/JSJJ3B2Q/caiProxylessNASDirectNeural2019.pdf}
}

@article{cammaertsAbnormalisationSocialJustice2022,
  title = {The Abnormalisation of Social Justice: {{The}} ‘Anti-Woke Culture War’ Discourse in the {{UK}}},
  shorttitle = {The Abnormalisation of Social Justice},
  author = {Cammaerts, Bart},
  date = {2022-05-12},
  journaltitle = {Discourse \& Society},
  shortjournal = {Discourse \& Society},
  pages = {09579265221095407},
  publisher = {{SAGE Publications Ltd}},
  issn = {0957-9265},
  doi = {10.1177/09579265221095407},
  url = {https://doi.org/10.1177/09579265221095407},
  urldate = {2022-06-02},
  abstract = {In this article, the so-called ‘anti-woke’ culture war is deconstructed through the notions of metapolitics in fascist discourses – linked to the Gramscian ‘hegemonisation’ and ‘the war of position’ – as well as the Schmittian friend/enemy distinction coupled with theories of deviance and moral panics. The appropriation of the neo-fascist culture war discourse by the mainstream right in the UK is analysed discursively, combining political discourse analysis, the discourse-historical approach and discourse-conceptual analysis. The anti-woke culture war by the British conservative party as well as rightwing media will serve to analyse how social justice struggles like anti-racism, anti-sexism and pro-LGBTQ rights are being abnormalised and positioned as extreme deviant political positions. Linked to this, so-called ‘cancel culture’ is strategically deployed by dominant groups to neutralise contestations against racist, sexist and anti-LGBTQ views. Finally, freedom of speech and the right to offend is weaponised to protect racist and discriminatory language and to position these idea’s as valid opinions worthy of democratic debate.},
  langid = {english},
  keywords = {Anti-woke discourse,cancel culture,culture war,deviance,moral panics,neo-fascism,war of position},
  file = {/Users/hugo/Papers/pdf/cammaertsAbnormalisationSocialJustice2022.pdf}
}

@article{candesRobustUncertaintyPrinciples2006,
  title = {Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information},
  shorttitle = {Robust Uncertainty Principles},
  author = {Candes, E.J. and Romberg, J. and Tao, T.},
  date = {2006-02},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {52},
  number = {2},
  pages = {489--509},
  issn = {1557-9654},
  doi = {10.1109/TIT.2005.862083},
  abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f/spl isin/C/sup N/ and a randomly chosen set of frequencies /spl Omega/. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set /spl Omega/? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=/spl sigma//sub /spl tau//spl isin/T/f(/spl tau/)/spl delta/(t-/spl tau/) obeying |T|/spl les/C/sub M//spl middot/(log N)/sup -1/ /spl middot/ |/spl Omega/| for some constant C/sub M/{$>$}0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N/sup -M/), f can be reconstructed exactly as the solution to the /spl lscr//sub 1/ minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C/sub M/ which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|/spl middot/logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N/sup -M/) would in general require a number of frequency samples at least proportional to |T|/spl middot/logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {Biomedical imaging,Convex optimization,duality in optimization,free probability,Frequency,image reconstruction,Image reconstruction,linear programming,Linear programming,Mathematics,random matrices,Robustness,Sampling methods,Signal processing,Signal reconstruction,sparsity,total-variation minimization,trigonometric expansions,Uncertainty,uncertainty principle},
  file = {/Users/hugo/Papers/pdf/candesRobustUncertaintyPrinciples2006.pdf}
}

@incollection{cardoneHistoryLambdacalculusCombinatory2006,
  title = {History of {{Lambda-calculus}} and {{Combinatory Logic}}},
  booktitle = {Handbook of the {{History}} of {{Logic}}},
  author = {Cardone, Felice and Hindley, J Roger},
  date = {2006-01},
  volume = {5},
  pages = {95},
  publisher = {{Dov M. Gabbay and John Woods, Elsevier Co.}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/cardoneHistoryLambdacalculusCombinatory.pdf}
}

@incollection{cardoneLambdaCalculusCombinators20th2009,
  title = {Lambda-{{Calculus}} and {{Combinators}} in the 20th {{Century}}},
  booktitle = {Logic from {{Russell}} to {{Church}}},
  author = {Cardone, Felice and Hindley, J. Roger},
  editor = {Gabbay, Dov M. and Woods, John},
  date = {2009},
  series = {Handbook of the {{History}} of {{Logic}}},
  volume = {5},
  pages = {723--817},
  publisher = {{Elsevier}},
  doi = {10.1016/S1874-5857(09)70018-4}
}

@online{carliniDigitalLogicGates2020,
  type = {Personal web page},
  title = {Digital {{Logic Gates}} on {{Conway}}'s {{Game}} of {{Life}} - {{Part}} 1},
  author = {Carlini, Nicholas},
  date = {2020-04-01},
  url = {https://nicholas.carlini.com/writing/2020/digital-logic-game-of-life.html},
  urldate = {2022-11-15},
  langid = {english}
}

@article{carlsonArchitectureNeverEndingLanguage2010,
  title = {Toward an {{Architecture}} for {{Never-Ending Language Learning}}.},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan},
  date = {2010},
  journaltitle = {In Proceedings of the Conference on Artificial Intelligence (AAAI) (2010)},
  eprint = {21259302},
  eprinttype = {pmid},
  pages = {1306--1313},
  issn = {1098-2345},
  doi = {10.1002/ajp.20927},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/download/1879/2201},
  abstract = {We consider here the problem of building a never-ending lan- guage learner; that is, an intelligent computer agent that runs forever and that each day must (1) extract, or read, informa- tion from the web to populate a growing structured knowl- edge base, and (2) learn to perform this task better than on the previous day. In particular, we propose an approach and a set of design principles for such an agent, describe a par- tial implementation of such a system that has already learned to extract a knowledge base containing over 242,000 beliefs with an estimated precision of 74\% after running for 67 days, and discuss lessons learned from this preliminary attempt to build a never-ending learning agent.},
  file = {/Users/hugo/Papers/pdf/carlsonArchitectureNeverEndingLanguage22.pdf}
}

@inproceedings{caronUnsupervisedLearningVisual2020,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  booktitle = {34th {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  date = {2020},
  eprint = {2006.09882},
  eprinttype = {arxiv},
  location = {{Vancouver, Canada}},
  url = {http://arxiv.org/abs/2006.09882},
  urldate = {2022-02-23},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archiveprefix = {arXiv},
  eventtitle = {{{NeurIPS}} 2020},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/caronUnsupervisedLearningVisual2021.pdf;/Users/hugo/Zotero/storage/FKQB8CIB/2006.html}
}

@article{cattaneoCellularAutomataFuzzy1997,
  title = {Cellular Automata in Fuzzy Backgrounds},
  author = {Cattaneo, G. and Flocchini, P. and Mauri, G. and Vogliotti, C. Quaranta and Santoro, N.},
  date = {1997-06-15},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {105},
  number = {1},
  pages = {105--120},
  issn = {0167-2789},
  doi = {10.1016/S0167-2789(96)00233-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0167278996002333},
  urldate = {2019-06-18},
  abstract = {The main purpose of this work is to understand some limitations introduced by the classical definitions of cellular automata (CA). To this end, we have defined a new model of CAs (fuzzy CAs) which allows the observation of interesting “chaotic” properties of elementary CAs. To date neither a formal nor a precise definition of “chaos” in CAs exists; we believe that the proposed model provides a “sharper” tool to detect which properties can be associated to a “chaotic” behavior. We also define a measure (rule entropy) which gives information about the CA's dynamics solely on the basis of the rule table and provides theoretical explanations to some of the empirical observations.},
  keywords = {Cellular automata,Classification,Fuzzification},
  file = {/Users/hugo/Papers/pdf/cattaneoCellularAutomataFuzzy12.pdf;/Users/hugo/Zotero/storage/B4944SGW/S0167278996002333.html}
}

@article{cerUniversalSentenceEncoder2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  date = {2018},
  journaltitle = {CoRR},
  volume = {abs/1803.11175},
  eprint = {1803.11175},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.11175},
  urldate = {2022-08-02},
  archiveprefix = {arXiv}
}

@article{chagnonExtractivismGlobalExtractivism2022,
  title = {From Extractivism to Global Extractivism: The Evolution of an Organizing Concept},
  shorttitle = {From Extractivism to Global Extractivism},
  author = {Chagnon, Christopher W. and Durante, Francesco and Gills, Barry K. and Hagolani-Albov, Sophia E. and Hokkanen, Saana and Kangasluoma, Sohvi M. J. and Konttinen, Heidi and Kröger, Markus and LaFleur, William and Ollinaho, Ossi and Vuola, Marketta P. S.},
  date = {2022-05-09},
  journaltitle = {The Journal of Peasant Studies},
  volume = {0},
  number = {0},
  pages = {1--33},
  publisher = {{Routledge}},
  issn = {0306-6150},
  doi = {10.1080/03066150.2022.2069015},
  url = {https://doi.org/10.1080/03066150.2022.2069015},
  urldate = {2022-06-01},
  abstract = {Research on extractivism has rapidly proliferated, expanding into new empirical and conceptual spaces. We examine the origins, evolution, and conceptual expansion of the concept. Extractivism is useful to analyze resource extraction practices around the world. ‘Global Extractivism’ is a new conceptual tool for assessing global phenomena. We situate extractivism within an ensemble of concepts, and explore its relation to development, the state, and value. Extractivism as an organizing concept addresses many fields of research. Extractivism forms a complex of self-reinforcing practices, mentalities, and power differentials underwriting and rationalizing socio-ecologically destructive modes of organizing life-through subjugation, depletion, and non-reciprocity.},
  keywords = {Extractivism,global crises,global extractivism,political ontology,resource frontiers,resource politics},
  annotation = {\_eprint: https://doi.org/10.1080/03066150.2022.2069015},
  file = {/Users/hugo/Papers/pdf/chagnonExtractivismGlobalExtractivism2022.pdf;/Users/hugo/Zotero/storage/SBJP65EL/03066150.2022.html}
}

@article{chaitinAlgorithmicInformationTheory1977,
  title = {Algorithmic Information Theory},
  author = {Chaitin, Gregory J},
  date = {1977},
  journaltitle = {IBM journal of research and development},
  shortjournal = {IBM journal of research and development},
  volume = {21},
  number = {4},
  pages = {350--359},
  issn = {0018-8646}
}

@article{chaitinLengthProgramsComputing1969,
  title = {On the {{Length}} of {{Programs}} for {{Computing Finite Binary Sequences}}: {{Statistical Considerations}}},
  shorttitle = {On the {{Length}} of {{Programs}} for {{Computing Finite Binary Sequences}}},
  author = {Chaitin, Gregory J.},
  date = {1969-01},
  journaltitle = {J. ACM},
  volume = {16},
  number = {1},
  pages = {145--159},
  issn = {0004-5411},
  doi = {10.1145/321495.321506},
  url = {http://doi.acm.org/10.1145/321495.321506},
  urldate = {2019-05-28},
  abstract = {An attempt is made to carry out a program (outlined in a previous paper) for defining the concept of a random or patternless, finite binary sequence, and for subsequently defining a random or patternless, infinite binary sequence to be a sequence whose initial segments are all random or patternless finite binary sequences. A definition based on the bounded-transfer Turing machine is given detailed study, but insufficient understanding of this computing machine precludes a complete treatment. A computing machine is introduced which avoids these difficulties.},
  file = {/Users/hugo/Zotero/storage/NADRKEJR/chaitinLengthProgramsComputing1969.pdf}
}

@article{chaitinTheoryProgramSize1975,
  title = {A {{Theory}} of {{Program Size Formally Identical}} to {{Information Theory}}},
  author = {Chaitin, Gregory J},
  date = {1975-07},
  journaltitle = {Journal of the Association for Computing Machinery},
  shortjournal = {JACM},
  volume = {22},
  number = {3},
  pages = {12},
  abstract = {A new definition of program-size complexity is made. H(A,B/C,D) is defined to be the size in bits of the shortest self-delimiting program for calculating strings A and B if one is given a minimal-size self-delimiting program for calculating strings C and D. This differs from previous definitions: (1) programs are required to be self-delimiting, i.e. no program is a prefix of another, and (2) instead of being given C and D directly, one is given a program for calculating them that is minimal in size. Unlike previous definitions, this one has precisely the formal properties of the entropy concept of information theory. For example, H(A,B) = H(A) + H ( B / A ) -\textasciitilde{} 0(1). Also, if a program of length k is assigned measure 2-k, then H(A) = -log2 (the probability that the standard universal computer will calculate A) -- 0(1).},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/YRK4AXD6/chaitinTheoryProgramSize1975.pdf}
}

@article{chambersEventSchemaInduction2013,
  title = {Event {{Schema Induction}} with a {{Probabilistic Entity-Driven Model}}},
  author = {Chambers, Nathanael},
  date = {2013},
  pages = {11},
  abstract = {Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text. Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction. Recent research suggests event schemas can be learned from raw text. Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning. Our generative model is conceptually simpler than the pipelined approach and requires far less training data. It also provides an interesting contrast with a recent HMM-based model. We evaluate on a common dataset for template schema extraction. Our generative model matches the pipeline’s performance, and outperforms the HMM by 7 F1 points (20\%).},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/chambersEventSchemaInduction22.pdf}
}

@article{chambersTemplateBasedInformationExtraction2011,
  title = {Template-{{Based Information Extraction}} without the {{Templates}}},
  author = {Chambers, Nathanael},
  date = {2011},
  pages = {11},
  abstract = {Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/chambersTemplateBasedInformationExtraction22.pdf}
}

@unpublished{changNeuralNetworkQuine2018,
  title = {Neural {{Network Quine}}},
  author = {Chang, Oscar and Lipson, Hod},
  date = {2018-05-24},
  eprint = {1803.05859},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.05859},
  urldate = {2020-11-12},
  abstract = {Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems. Here we describe how to build and train self-replicating neural networks. The network replicates itself by learning to output its own weights. The network is designed using a loss function that can be optimized with either gradient-based or non-gradient-based methods. We also describe a method we call regeneration to train the network without explicit optimization, by injecting the network with predictions of its own parameters. The best solution for a self-replicating network was found by alternating between regeneration and optimization steps. Finally, we describe a design for a self-replicating neural network that can solve an auxiliary task such as MNIST image classification. We observe that there is a trade-off between the network's ability to classify images and its ability to replicate, but training is biased towards increasing its specialization at image classification at the expense of replication. This is analogous to the trade-off between reproduction and other tasks observed in nature. We suggest that a self-replication mechanism for artificial intelligence is useful because it introduces the possibility of continual improvement through natural selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/changNeuralNetworkQuine2018.pdf;/Users/hugo/Zotero/storage/NDHE4LCF/1803.html}
}

@unpublished{chanLeniaBiologyArtificial2019a,
  title = {Lenia - {{Biology}} of {{Artificial Life}}},
  author = {Chan, Bert Wang-Chak},
  date = {2019-05-04},
  eprint = {1812.05433},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1812.05433},
  urldate = {2020-07-20},
  abstract = {We report a new system of artificial life called Lenia (from Latin lenis “smooth”), a two-dimensional cellular automaton with continuous spacetime-state and generalized local rule. Computer simulations show that Lenia supports a great diversity of complex autonomous patterns or “lifeforms” bearing resemblance to real-world microscopic organisms. More than 400 species in 18 families have been identified, many discovered via interactive evolutionary computation. They differ from other cellular automata patterns in being geometric, metameric, fuzzy, resilient, adaptive, and rule-generic.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Cellular Automata and Lattice Gases,Nonlinear Sciences - Pattern Formation and Solitons},
  file = {/Users/hugo/Papers/pdf/chanLeniaBiologyArtificial2019a.pdf}
}

@unpublished{chanLeniaExpandedUniverse2020,
  title = {Lenia and {{Expanded Universe}}},
  author = {Chan, Bert Wang-Chak},
  date = {2020-05-07},
  eprint = {2005.03742},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/2005.03742},
  urldate = {2020-07-20},
  abstract = {We report experimental extensions of Lenia, a continuous cellular automata family capable of producing lifelike selforganizing autonomous patterns. The rule of Lenia was generalized into higher dimensions, multiple kernels, and multiple channels. The final architecture approaches what can be seen as a recurrent convolutional neural network. Using semiautomatic search e.g. genetic algorithm, we discovered new phenomena like polyhedral symmetries, individuality, selfreplication, emission, growth by ingestion, and saw the emergence of “virtual eukaryotes” that possess internal division of labor and type differentiation. We discuss the results in the contexts of biology, artificial life, and artificial intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Nonlinear Sciences - Cellular Automata and Lattice Gases,Nonlinear Sciences - Pattern Formation and Solitons},
  file = {/Users/hugo/Papers/pdf/chanLeniaExpandedUniverse2020.pdf}
}

@article{channonEvolutionaryEmergenceIncreasingly2000,
  title = {Towards the Evolutionary Emergence of Increasingly Complex Advantageous Behaviours},
  author = {Channon, Alastair Daniel and Damper, Robert I.},
  date = {2000},
  journaltitle = {International Journal of Systems Science},
  volume = {31},
  number = {7},
  pages = {843--860},
  file = {/Users/hugo/Zotero/storage/QHVHQ4VC/channonEvolutionaryEmergenceIncreasingly2000.pdf;/Users/hugo/Zotero/storage/434ST444/002077200406570.html}
}

@article{channonImprovingStillPassing2003,
  title = {Improving and Still Passing the {{ALife}} Test: {{Component-normalised}} Activity Statistics Classify Evolution in {{Geb}} as Unbounded},
  shorttitle = {Improving and Still Passing the {{ALife}} Test},
  author = {Channon, Alastair},
  date = {2003},
  journaltitle = {Proceedings of Artificial Life VIII, Sydney, RK Standish, MA Bedau, and HA Abbass,(eds.), MIT Press: Cambridge, MA},
  pages = {173--181},
  file = {/Users/hugo/Zotero/storage/M8U4ULRK/channonImprovingStillPassing2003.pdf;/Users/hugo/Zotero/storage/A83BKATH/books.html}
}

@inproceedings{channonPassingALifeTest2001,
  title = {Passing the {{ALife}} Test: {{Activity}} Statistics Classify Evolution in {{Geb}} as Unbounded},
  shorttitle = {Passing the {{ALife}} Test},
  booktitle = {European {{Conference}} on {{Artificial Life}}},
  author = {Channon, Alastair},
  date = {2001},
  pages = {417--426},
  publisher = {{Springer}},
  file = {/Users/hugo/Zotero/storage/HYEQIGHJ/channonPassingALifeTest2001.pdf;/Users/hugo/Zotero/storage/L2WFSTG6/3-540-44811-X_45.html}
}

@article{channonUnboundedEvolutionaryDynamics2006,
  title = {Unbounded Evolutionary Dynamics in a System of Agents That Actively Process and Transform Their Environment},
  author = {Channon, Alastair},
  date = {2006},
  journaltitle = {Genetic Programming and Evolvable Machines},
  volume = {7},
  number = {3},
  pages = {253--281},
  file = {/Users/hugo/Zotero/storage/NH7KKE73/s10710-006-9009-3.html}
}

@incollection{chatzilygeroudisQualityDiversityOptimizationNovel2021,
  title = {Quality-{{Diversity Optimization}}: {{A Novel Branch}} of {{Stochastic Optimization}}},
  shorttitle = {Quality-{{Diversity Optimization}}},
  booktitle = {Black {{Box Optimization}}, {{Machine Learning}}, and {{No-Free Lunch Theorems}}},
  author = {Chatzilygeroudis, Konstantinos and Cully, Antoine and Vassiliades, Vassilis and Mouret, Jean-Baptiste},
  editor = {Pardalos, Panos M. and Rasskazova, Varvara and Vrahatis, Michael N.},
  date = {2021},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  pages = {109--135},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-66515-9_4},
  url = {https://doi.org/10.1007/978-3-030-66515-9_4},
  urldate = {2022-11-15},
  abstract = {Traditional optimization algorithms search for a single global optimum that maximizes (or minimizes) the objective function. Multimodal optimization algorithms search for the highest peaks in the search space that can be more than one. Quality-Diversity algorithms are a recent addition to the evolutionary computation toolbox that do not only search for a single set of local optima, but instead try to illuminate the search space. In effect, they provide a holistic view of how high-performing solutions are distributed throughout a search space. The main differences with multimodal optimization algorithms are that (1) Quality-Diversity typically works in the behavioral space (or feature space), and not in the genotypic (or parameter) space, and (2) Quality-Diversity attempts to fill the whole behavior space, even if the niche is not a peak in the fitness landscape. In this chapter, we provide a gentle introduction to Quality-Diversity optimization, discuss the main representative algorithms, and the main current topics under consideration in the community. Throughout the chapter, we also discuss several successful applications of Quality-Diversity algorithms, including deep learning, robotics, and reinforcement learning.},
  isbn = {978-3-030-66515-9},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/chatzilygeroudisQualityDiversityOptimizationNovel2021.pdf}
}

@inproceedings{chenCartoonGANGenerativeAdversarial2018,
  title = {{{CartoonGAN}}: Generative Adversarial Networks for Photo Cartoonization},
  shorttitle = {{{CartoonGAN}}},
  author = {Chen, Yang and Lai, Yukun and Liu, Yong-Jin},
  date = {2018},
  location = {{Lake Salt City, USA}},
  url = {http://orca.cf.ac.uk/110341/},
  urldate = {2019-01-19},
  abstract = {In this paper, we propose a solution to transforming photos of real-world scenes into cartoon style images, which is valuable and challenging in computer vision and computer graphics. Our solution belongs to learning based methods, which have recently become popular to stylize images in artistic forms such as painting. However, existing methods do not produce satisfactory results for cartoonization, due to the fact that (1) cartoon styles have unique characteristics with high level simplification and abstraction, and (2) cartoon images tend to have clear edges, smooth color shading and relatively simple textures, which exhibit significant challenges for texture-descriptor-based loss functions used in existing methods. In this paper, we propose CartoonGAN, a generative adversarial network (GAN) framework for cartoon stylization. Our method takes unpaired photos and cartoon images for training, which is easy to use. Two novel losses suitable for cartoonization are proposed: (1) a semantic content loss, which is formulated as a sparse regularization in the high-level feature maps of the VGG network to cope with substantial style variation between photos and cartoons, and (2) an edge-promoting adversarial loss for preserving clear edges.  We further introduce an initialization phase, to improve the convergence of the network to the target manifold. Our method is also much more efficient to train than existing methods. Experimental results show that our method is able to generate high-quality cartoon images from real-world photos (i.e., following specific artists’ styles and with clear edges and smooth shading) and outperforms state-of-the-art methods.},
  eventtitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/chenCartoonGANGenerativeAdversarial22.pdf;/Users/hugo/Papers/pdf/chenCartoonGANGenerativeAdversarial3.pdf;/Users/hugo/Zotero/storage/FM5ZSMDI/110341.html}
}

@misc{chenDecisionTransformerReinforcement2021,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  date = {2021-06-24},
  number = {arXiv:2106.01345},
  eprint = {2106.01345},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.01345},
  url = {http://arxiv.org/abs/2106.01345},
  urldate = {2022-07-22},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/chenDecisionTransformerReinforcement2021.pdf;/Users/hugo/Zotero/storage/YQS9D5XG/2106.html}
}

@unpublished{chenEmpiricalStudySmoothing1996,
  title = {An {{Empirical Study}} of {{Smoothing Techniques}} for {{Language Modeling}}},
  author = {Chen, Stanley F. and Goodman, Joshua T.},
  date = {1996-06-10},
  eprint = {cmp-lg/9606011},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/cmp-lg/9606011},
  urldate = {2022-02-23},
  abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/chenEmpiricalStudySmoothing1996.pdf}
}

@inproceedings{chenMultilingualKnowledgeGraph2017,
  title = {Multilingual Knowledge Graph Embeddings for Cross-Lingual Knowledge Alignment},
  booktitle = {{{IJCAI International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Chen, Muhao and Tian, Yingtao and Yang, Mohan and Zaniolo, Carlo},
  date = {2017},
  pages = {1511--1517},
  doi = {10.24963/ijcai.2017/209},
  abstract = {Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable crosslingual alignment by human labor is very costly and errorprone. Thus, we propose MTransE, a translation-based model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE.},
  isbn = {978-0-9992411-0-3},
  file = {/Users/hugo/Papers/pdf/chenMultilingualKnowledgeGraph22.pdf}
}

@unpublished{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  date = {2019-12-13},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.07366},
  urldate = {2021-09-30},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/chenNeuralOrdinaryDifferential2019.pdf;/Users/hugo/Zotero/storage/7AXKIM2R/1806.html}
}

@article{chenReprogrammableMechanicalMetamaterial2021,
  title = {A Reprogrammable Mechanical Metamaterial with Stable Memory},
  author = {Chen, Tian and Pauly, Mark and Reis, Pedro M.},
  date = {2021-01},
  journaltitle = {Nature},
  volume = {589},
  number = {7842},
  pages = {386--390},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03123-5},
  url = {https://www.nature.com/articles/s41586-020-03123-5},
  urldate = {2022-03-15},
  abstract = {Metamaterials are designed to realize exotic physical properties through the geometric arrangement of their underlying structural layout1,2. Traditional mechanical metamaterials achieve functionalities such as a target Poisson’s ratio3 or shape transformation4–6 through unit-cell optimization7–9, often with spatial heterogeneity10–12. These functionalities are programmed into the layout of the metamaterial in a way that cannot be altered. Although recent efforts have produced means of tuning such properties post-fabrication13–19, they have not demonstrated mechanical reprogrammability analogous to that of digital devices, such as hard disk drives, in which each unit can be written to or read from in real time as required. Here we overcome this challenge by using a design framework for a tileable mechanical metamaterial with stable memory at the unit-cell level. Our design comprises an array of physical binary elements (m-bits), analogous to digital bits, with clearly delineated writing and reading phases. Each m-bit can be independently and reversibly switched between two stable states (acting as memory) using magnetic actuation to move between the equilibria of a bistable shell20–25. Under deformation, each state is associated with a distinctly different mechanical response that is fully elastic and can be reversibly cycled until the system is reprogrammed. Encoding a set of binary instructions onto the tiled array yields markedly different mechanical properties; specifically, the stiffness and strength can be made to range over an order of magnitude. We expect that the stable memory and on-demand reprogrammability of mechanical properties in this design paradigm will facilitate the development of advanced forms of mechanical metamaterials.},
  issue = {7842},
  langid = {english},
  keywords = {Engineering,Materials for devices,Mechanical properties,Polymers},
  file = {/Users/hugo/Papers/pdf/chenReprogrammableMechanicalMetamaterial2021.pdf;/Users/hugo/Zotero/storage/WV9K6V3K/s41586-020-03123-5.html}
}

@inproceedings{chenScalableInfluenceMaximization2010,
  title = {Scalable Influence Maximization for Prevalent Viral Marketing in Large-Scale Social Networks},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '10},
  author = {Chen, Wei and Wang, Chi and Wang, Yajun},
  date = {2010},
  pages = {1029},
  publisher = {{ACM Press}},
  location = {{Washington, DC, USA}},
  doi = {10.1145/1835804.1835934},
  url = {http://dl.acm.org/citation.cfm?doid=1835804.1835934},
  urldate = {2018-11-28},
  abstract = {Influence maximization, defined by Kempe, Kleinberg, and Tardos (2003), is the problem of finding a small set of seed nodes in a social network that maximizes the spread of influence under certain influence cascade models. The scalability of influence maximization is a key factor for enabling prevalent viral marketing in large-scale online social networks. Prior solutions, such as the greedy algorithm of Kempe et al. (2003) and its improvements are slow and not scalable, while other heuristic algorithms do not provide consistently good performance on influence spreads. In this paper, we design a new heuristic algorithm that is easily scalable to millions of nodes and edges in our experiments. Our algorithm has a simple tunable parameter for users to control the balance between the running time and the influence spread of the algorithm. Our results from extensive simulations on several real-world and synthetic networks demonstrate that our algorithm is currently the best scalable solution to the influence maximization problem: (a) our algorithm scales beyond million-sized graphs where the greedy algorithm becomes infeasible, and (b) in all size ranges, our algorithm performs consistently well in influence spread —it is always among the best algorithms, and in most cases it significantly outperforms all other scalable heuristics to as much as 100\%–260\% increase in influence spread.},
  eventtitle = {The 16th {{ACM SIGKDD}} International Conference},
  isbn = {978-1-4503-0055-1},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/chenScalableInfluenceMaximization22.pdf}
}

@inproceedings{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-06-30},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  location = {{Vienna, Austria}},
  url = {http://arxiv.org/abs/2002.05709},
  urldate = {2022-02-23},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  eventtitle = {{{ICML}} 2020},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/chenSimpleFrameworkContrastive2020.pdf;/Users/hugo/Zotero/storage/HZQM6BVB/2002.html}
}

@article{chenVisualHideSeek2020,
  title = {Visual {{Hide}} and {{Seek}}},
  author = {Chen, Boyuan and Song, Shuran and Lipson, Hod and Vondrick, Carl},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {645--655},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00269},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00269},
  urldate = {2020-07-27},
  abstract = {We train embodied agents to play Visual Hide and Seek to study the relationship between agent behaviors and environmental complexity. In Visual Hide and Seek, a prey must navigate in a simulated environment in order to avoid capture from a predator, only relying on first-person visual observations. By probing different environmental factors, agents exhibit diverse hiding strategies and even the knowledge of its own visibility to other agents in the scene. Furthermore, we quantitatively analyze how agent weaknesses, such as slower speed, affect the learned policy. Our results suggest that, although agent weakness makes the learning problem more challenging, they also cause more useful features to be learned. Our project website is available at http://www.cs.columbia.edu/bchen/visualhideseek/.},
  file = {/Users/hugo/Papers/pdf/chenVisualHideSeek2020.pdf;/Users/hugo/Zotero/storage/AH26LRRX/isal_a_00269.html}
}

@article{cheungProbabilisticFrameInduction2013,
  title = {Probabilistic {{Frame Induction}}},
  author = {Cheung, Jackie Chi Kit and Poon, Hoifung and Vanderwende, Lucy},
  date = {2013},
  pages = {10},
  abstract = {In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, and participants as latent topics and learns those frame and event transitions that best explain the text. The number of frame components is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produces state-of-the-art results while substantially reducing engineering effort.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/cheungProbabilisticFrameInduction22.pdf}
}

@unpublished{chevalier-boisvertBabyaiPlatformStudy2018,
  title = {Babyai: {{A}} Platform to Study the Sample Efficiency of Grounded Language Learning},
  shorttitle = {Babyai},
  author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
  date = {2018},
  eprint = {1810.08272},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/chevalier-boisvertBabyaiPlatformStudy2018.pdf;/Users/hugo/Zotero/storage/QDXNSPSG/1810.html}
}

@article{chizatGlobalConvergenceGradient,
  title = {On the {{Global Convergence}} of {{Gradient Descent}} for {{Over-parameterized Models}} Using {{Optimal Transport}}},
  author = {Chizat, Lénaïc and Bach, Francis},
  pages = {11},
  abstract = {Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/chizatGlobalConvergenceGradient2.pdf}
}

@unpublished{chizatInterpolatingDistanceOptimal2015,
  title = {An {{Interpolating Distance}} between {{Optimal Transport}} and {{Fisher-Rao}}},
  author = {Chizat, Lenaic and Schmitzer, Bernhard and Peyré, Gabriel and Vialard, François-Xavier},
  date = {2015-06-21},
  eprint = {1506.06430},
  eprinttype = {arxiv},
  primaryclass = {math},
  url = {http://arxiv.org/abs/1506.06430},
  urldate = {2018-11-22},
  abstract = {This paper defines a new transport metric over the space of non-negative measures. This metric interpolates between the quadratic Wasserstein and the Fisher-Rao metrics and generalizes optimal transport to measures with different masses. It is defined as a generalization of the dynamical formulation of optimal transport of Benamou and Brenier, by introducing a source term in the continuity equation. The influence of this source term is measured using the Fisher-Rao metric, and is averaged with the transportation term. This gives rise to a convex variational problem defining our metric. Our first contribution is a proof of the existence of geodesics (i.e. solutions to this variational problem). We then show that (generalized) optimal transport and Fisher-Rao metrics are obtained as limiting cases of our metric. Our last theoretical contribution is a proof that geodesics between mixtures of sufficiently close Diracs are made of translating mixtures of Diracs. Lastly, we propose a numerical scheme making use of first order proximal splitting methods and we show an application of this new distance to image interpolation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Analysis of PDEs},
  file = {/Users/hugo/Papers/pdf/chizatInterpolatingDistanceOptimal22.pdf}
}

@unpublished{chngGARFGaussianActivated2022,
  title = {{{GARF}}: {{Gaussian Activated Radiance Fields}} for {{High Fidelity Reconstruction}} and {{Pose Estimation}}},
  shorttitle = {{{GARF}}},
  author = {Chng, Shin-Fang and Ramasinghe, Sameera and Sherrah, Jamie and Lucey, Simon},
  date = {2022-04-12},
  eprint = {2204.05735},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.05735},
  urldate = {2022-04-14},
  abstract = {Despite Neural Radiance Fields (NeRF) showing compelling results in photorealistic novel views synthesis of real-world scenes, most existing approaches require accurate prior camera poses. Although approaches for jointly recovering the radiance field and camera pose exist (BARF), they rely on a cumbersome coarse-to-fine auxiliary positional embedding to ensure good performance. We present Gaussian Activated neural Radiance Fields (GARF), a new positional embedding-free neural radiance field architecture - employing Gaussian activations - that outperforms the current state-of-the-art in terms of high fidelity reconstruction and pose estimation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/chngGARFGaussianActivated2022.pdf;/Users/hugo/Zotero/storage/PZRYNC6R/2204.html}
}

@inproceedings{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}–{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of {{SSST-8}}, {{Eighth Workshop}} on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, Kyunghyun and van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  options = {useprefix=true},
  date = {2014},
  pages = {103--111},
  file = {/Users/hugo/Papers/pdf/choPropertiesNeuralMachine2014.pdf}
}

@misc{chowdheryPaLMScalingLanguage2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  date = {2022-04-19},
  number = {arXiv:2204.02311},
  eprint = {2204.02311},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2204.02311},
  urldate = {2022-07-26},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/chowdheryPaLMScalingLanguage2022.pdf;/Users/hugo/Zotero/storage/4K6JZ6GW/2204.html}
}

@unpublished{christenCyberneticalConceptsCellular2019,
  title = {Cybernetical {{Concepts}} for {{Cellular Automaton}} and {{Artificial Neural Network Modelling}} and {{Implementation}}},
  author = {Christen, Patrik and Del Fabbro, Olivier},
  date = {2019-11-24},
  eprint = {2001.02037},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2001.02037},
  urldate = {2020-01-10},
  abstract = {As a discipline cybernetics has a long and rich history. In its first generation it not only had a worldwide span, in the area of computer modelling, for example, its proponents such as John von Neumann, Stanislaw Ulam, Warren McCulloch and Walter Pitts, also came up with models and methods such as cellular automata and artificial neural networks, which are still the foundation of most modern modelling approaches. At the same time, cybernetics also got the attention of philosophers, such as the Frenchman Gilbert Simondon, who made use of cybernetical concepts in order to establish a metaphysics and a natural philosophy of individuation, giving cybernetics thereby a philosophical interpretation, which he baptised allagmatic. In this paper, we emphasise this allagmatic theory by showing how Simondon’s philosophical concepts can be used to formulate a generic computer model or metamodel for complex systems modelling and its implementation in program code, according to generic programming. We also present how the developed allagmatic metamodel is capable of building simple cellular automata and artificial neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Other Computer Science},
  file = {/Users/hugo/Zotero/storage/IYIPLR9J/christenCyberneticalConceptsCellular2019.pdf}
}

@article{chuaCellularNeuralNetworks1988,
  title = {Cellular Neural Networks: Applications},
  shorttitle = {Cellular Neural Networks},
  author = {Chua, L.O. and Yang, L.},
  date = {1988-10},
  journaltitle = {IEEE Transactions on Circuits and Systems},
  shortjournal = {IEEE Trans. Circuits Syst.},
  volume = {35},
  number = {10},
  pages = {1273--1290},
  issn = {00984094},
  doi = {10.1109/31.7601},
  url = {http://ieeexplore.ieee.org/document/7601/},
  urldate = {2020-07-01},
  file = {/Users/hugo/Papers/pdf/chuaCellularNeuralNetworks1988.pdf}
}

@article{chuaCellularNeuralNetworks1988a,
  title = {Cellular Neural Networks: Theory},
  shorttitle = {Cellular Neural Networks},
  author = {Chua, L.O. and Yang, L.},
  date = {1988-10},
  journaltitle = {IEEE Transactions on Circuits and Systems},
  volume = {35},
  number = {10},
  pages = {1257--1272},
  issn = {1558-1276},
  doi = {10.1109/31.7600},
  abstract = {A novel class of information-processing systems called cellular neural networks is proposed. Like neural networks, they are large-scale nonlinear analog circuits that process signals in real time. Like cellular automata, they consist of a massive aggregate of regularly spaced circuit clones, called cells, which communicate with each other directly only through their nearest neighbors. Each cell is made of a linear capacitor, a nonlinear voltage-controlled current source, and a few resistive linear circuit elements. Cellular neural networks share the best features of both worlds: their continuous-time feature allows real-time signal processing, and their local interconnection feature makes them particularly adapted for VLSI implementation. Cellular neural networks are uniquely suited for high-speed parallel signal processing.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}}},
  keywords = {active networks,Aggregates,Analog circuits,analogue computer circuits,Capacitors,cellular arrays,cellular neural networks,Cellular neural networks,Cloning,computerised signal processing,continuous-time feature,high-speed parallel signal processing,information-processing systems,large-scale nonlinear analog circuits,Large-scale systems,linear capacitor,local interconnection feature,Nearest neighbor searches,neural nets,Neural networks,nonlinear voltage-controlled current source,parallel architectures,real-time signal processing,real-time systems,resistive linear circuit elements,Signal processing,VLSI implementation,Voltage},
  file = {/Users/hugo/Papers/pdf/chuaCellularNeuralNetworks1988a.pdf;/Users/hugo/Zotero/storage/TT6A9T4G/7600.html}
}

@book{chungFlexibilityParadoxWhy2022,
  title = {The Flexibility Paradox: Why Flexible Working Leads to (Self-)Exploitation},
  shorttitle = {The Flexibility Paradox},
  author = {Chung, Heejung},
  date = {2022},
  publisher = {{Polity Press}},
  location = {{Bristol}},
  isbn = {978-1-4473-5477-2},
  langid = {english},
  pagetotal = {259}
}

@article{cikaResilientLifeExploration2020,
  title = {Resilient {{Life}}: {{An Exploration}} of {{Perturbed Autopoietic Patterns}} in {{Conway}}'s {{Game}} of {{Life}}},
  shorttitle = {Resilient {{Life}}},
  author = {Cika, Arta and Cohen, Elissa and Kruszewski, Germán and Seet, Luther and Steinmann, Patrick and Yin, Wenqian},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {656--664},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00305},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00305},
  urldate = {2020-07-27},
  abstract = {Complex systems can exhibit autopoiesis–a remarkable capability to reproduce or restore themselves to maintain existence and functionality. We explore the resilience of autopoietic patterns–their ability to recover from shocks or perturbations–in a simplified form in Conway's Game of Life. We subject a large number of autopoietic patterns in the Game of Life to various perturbations, and record their responses using multiple resilience metrics. Our results show that while resilience is rare, we are able to identify structural features improving patterns' resilience. We also draw several parallels between the resilience of patterns in the Game of Life to real-world complex systems. Our work may be useful both for improved searching for resilient patterns in the Game of Life, and for exploring resilience in complex systems.},
  file = {/Users/hugo/Papers/pdf/cikaResilientLifeExploration2020.pdf;/Users/hugo/Zotero/storage/MXIYNYBW/isal_a_00305.html}
}

@article{cilibrasiClusteringCompression2005,
  title = {Clustering by {{Compression}}},
  author = {Cilibrasi, Rudi and Vitanyi, Paul M B},
  date = {2005},
  journaltitle = {IEEE TRANSACTIONS ON INFORMATION THEORY},
  volume = {51},
  number = {4},
  pages = {21},
  abstract = {We present a new method for clustering based on compression. The method doesn’t use subject-specific features or background knowledge, and works as follows: First, we determine a parameter-free, universal, similarity distance, the normalized compression distance or NCD , computed from the lengths of compressed data files (singly and in pairwise concatenation). Second, we apply a hierarchical clustering method. The NCD is not restricted to a specific application area, and works across application area boundaries. A theoretical precursor, the normalized information distance, co-developed by one of the authors, is provably optimal. However, the optimality comes at the price of using the non-computable notion of Kolmogorov complexity. We propose axioms to capture the real-world setting, and show that the NCD approximates optimality. To extract a hierarchy of clusters from the distance matrix, we determine a dendrogram (binary tree) by a new quartet method and a fast heuristic to implement it. The method is implemented and available as public software, and is robust under choice of different compressors. To substantiate our claims of universality and robustness, we report evidence of successful application in areas as diverse as genomics, virology, languages, literature, music, handwritten digits, astronomy, and combinations of objects from completely different domains, using statistical, dictionary, and block sorting compressors. In genomics we presented new evidence for major questions in Mammalian evolution, based on whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta hypothesis against the Theria hypothesis.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/cilibrasiClusteringCompression22.pdf}
}

@inproceedings{cisnerosBenchmarkingLearningEfficiency2022,
  title = {Benchmarking {{Learning Efficiency}} in {{Deep Reservoir Computing}}},
  booktitle = {Conference on {{Lifelong Learning Agents}}},
  author = {Cisneros, Hugo and Mikolov, Tomas and Sivic, Josef},
  date = {2022-08-18},
  url = {https://virtual.lifelong-ml.cc/poster_34.html},
  urldate = {2022-09-22},
  abstract = {It is common to evaluate the performance of a machine learning model by measuring its predictive power on a test dataset. This approach favors complicated models that can smoothly fit complex functions and generalize well from training data points. Although essential components of intelligence, speed and data efficiency of this learning process are rarely reported or compared between different candidate models. In this paper, we introduce a benchmark of increasingly difficult tasks together with a data efficiency metric to measure how quickly machine learning models learn from training data. We compare the learning speed of some established sequential supervised models, such as RNNs, LSTMs, or Transformers, with relatively less known alternative models based on reservoir computing. The proposed tasks require a wide range of computational primitives, such as memory or the ability to compute Boolean functions, to be effectively solved. Surprisingly, we observe that reservoir computing systems that rely on dynamically evolving feature maps learn faster than fully supervised methods trained with stochastic gradient optimization while achieving comparable accuracy scores. The code, benchmark, trained models, and results to reproduce our experiments are available at https://github.com/hugcis/benchmark\_learning\_efficiency.},
  eventtitle = {Conference {{On Lifelong Learning Agents}}},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/4GBIWB75/poster_34.html}
}

@inproceedings{cisnerosEvolvingStructuresComplex2019,
  title = {Evolving {{Structures}} in {{Complex Systems}}},
  booktitle = {2019 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Cisneros, Hugo and Sivic, Josef and Mikolov, Tomas},
  date = {2019-12},
  pages = {230--237},
  publisher = {{IEEE}},
  location = {{Xiamen, China}},
  doi = {10.1109/SSCI44817.2019.9002840},
  url = {https://ieeexplore.ieee.org/document/9002840/},
  urldate = {2020-03-09},
  eventtitle = {2019 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  isbn = {978-1-72812-485-8},
  file = {/Users/hugo/Papers/pdf/cisnerosEvolvingStructuresComplex2019.pdf}
}

@online{cisnerosOpenendedCreationHybrid2021,
  title = {Open-Ended Creation of Hybrid Creatures with {{Neural Cellular Automata}}},
  author = {Cisneros, Hugo},
  date = {2021-08-03},
  url = {https://hugocisneros.com/blog/open-ended-creation-of-hybrid-creatures-with-neural-cellular-automata/},
  langid = {english},
  organization = {{Hugo Cisneros' blog}}
}

@inproceedings{cisnerosVisualizingComputationLargescale2020,
  title = {Visualizing Computation in Large-Scale Cellular Automata},
  booktitle = {Artificial {{Life Conference Proceedings}}},
  author = {Cisneros, Hugo and Sivic, Josef and Mikolov, Tomas},
  date = {2020-07-01},
  volume = {32},
  pages = {239--247},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00277},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00277},
  urldate = {2020-07-14},
  abstract = {Emergent processes in complex systems such as cellular automata can perform computations of increasing complexity, and could possibly lead to artificial evolution. Such a feat would require scaling up current simulation sizes to allow for enough computational capacity. Understanding complex computations happening in cellular automata and other systems capable of emergence poses many challenges, especially in large-scale systems. We propose methods for coarse-graining cellular automata based on frequency analysis of cell states, clustering and autoencoders. These innovative techniques facilitate the discovery of large-scale structure formation and complexity analysis in those systems. They emphasize interesting behaviors in elementary cellular automata while filtering out background patterns. Moreover, our methods reduce large 2D automata to smaller sizes and enable identifying systems that behave interestingly at multiple scales.},
  eventtitle = {{{ALife}} 2020},
  file = {/Users/hugo/Papers/pdf/cisnerosVisualizingComputationLargescale2020.pdf;/Users/hugo/Zotero/storage/I4WSGJAQ/isal_a_00277.html}
}

@inproceedings{clarkELECTRAPretrainingText2020,
  title = {{{ELECTRA}}: {{Pre-training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  booktitle = {8th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2020, {{Addis Ababa}}, {{Ethiopia}}, {{April}} 26-30, 2020},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  date = {2020},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=r1xMH1BtvB},
  urldate = {2022-07-22}
}

@unpublished{cluneAIGAsAIgeneratingAlgorithms2019,
  title = {{{AI-GAs}}: {{AI-generating}} Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence},
  shorttitle = {{{AI-GAs}}},
  author = {Clune, Jeff},
  date = {2019-05-27},
  eprint = {1905.10985},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.10985},
  urldate = {2020-01-27},
  abstract = {Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces that might be required for intelligence, with the implicit assumption that at some point in the future some group will complete the Herculean task of figuring out how to combine all of those pieces into an extremely complex machine. I call this the “manual AI approach.” This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend from the history of machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which itself automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. While work has begun on the first two pillars, little has been done on the third. Here I argue that either the manual or AI-GA approach could be the first to lead to general AI, and that both are worthwhile scientific endeavors irrespective of which is the fastest path. Because both approaches are roughly equally promising, and because the machine learning community is mostly committed to the engineered AI approach currently, I argue that our community should shift a substantial amount of its research investment to the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss the safety and ethical considerations unique to the AI-GA approach. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/hugo/Zotero/storage/7WRV2L3S/cluneAIGAsAIgeneratingAlgorithms2019.pdf}
}

@article{cluneEvolutionaryOriginsModularity2013,
  title = {The Evolutionary Origins of Modularity},
  author = {Clune, Jeff and Mouret, Jean-Baptiste and Lipson, Hod},
  date = {2013-03-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proc. R. Soc. B.},
  volume = {280},
  number = {1755},
  pages = {20122863},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2012.2863},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2012.2863},
  urldate = {2021-08-10},
  abstract = {A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks—their organization as functional, sparsely connected subunits—but there is no consensus regarding why modularity itself evolved. Although most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Computational evolution experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyse research in numerous disciplines, such as neuroscience and genetics, and enhance our ability to harness evolution for engineering purposes.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/KKH5NCVF/Clune et al. - 2013 - The evolutionary origins of modularity.pdf}
}

@book{coelloEvolutionaryAlgorithmsSolving2007,
  title = {Evolutionary Algorithms for Solving Multi-Objective Problems},
  author = {Coello, Carlos A. Coello and Lamont, Gary B. and Van Veldhuizen, David A.},
  date = {2007},
  volume = {5},
  publisher = {{Springer}},
  file = {/Users/hugo/Papers/pdf/coelloEvolutionaryAlgorithmsSolving2007.pdf}
}

@unpublished{colasGeppgDecouplingExploration2018,
  title = {Gep-Pg: {{Decoupling}} Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
  shorttitle = {Gep-Pg},
  author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  date = {2018},
  eprint = {1802.05054},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/colasGeppgDecouplingExploration2018.pdf;/Users/hugo/Zotero/storage/D9B537GF/1802.html}
}

@misc{conneauUnsupervisedCrosslingualRepresentation2020,
  title = {Unsupervised {{Cross-lingual Representation Learning}} at {{Scale}}},
  author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2020-04-07},
  number = {arXiv:1911.02116},
  eprint = {1911.02116},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.02116},
  url = {http://arxiv.org/abs/1911.02116},
  urldate = {2022-07-27},
  abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/conneauUnsupervisedCrosslingualRepresentation2020.pdf;/Users/hugo/Zotero/storage/HRW9K8NQ/1911.html}
}

@inproceedings{contiImprovingExplorationEvolution2018,
  title = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  date = {2018},
  pages = {5027--5038},
  file = {/Users/hugo/Papers/pdf/contiImprovingExplorationEvolution2018.pdf;/Users/hugo/Zotero/storage/D2NMJVG3/b1301141feffabac455e1f90a7de2054-Abstract.html}
}

@article{cookUniversalityElementaryCellular2004,
  title = {Universality in {{Elementary Cellular Automata}}},
  author = {Cook, Matthew},
  date = {2004},
  journaltitle = {Complex Systems},
  pages = {40},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/9ACRWMAX/cookUniversalityElementaryCellular2004.pdf}
}

@thesis{cooreBotanicalComputingDevelopmental1999,
  type = {phdthesis},
  title = {Botanical Computing: A Developmental Approach to Generating Interconnect Topologies on an Amorphous Computer},
  shorttitle = {Botanical Computing},
  author = {Coore, Daniel},
  date = {1999},
  institution = {{Massachusetts Institute of Technology}},
  file = {/Users/hugo/Papers/pdf/cooreBotanicalComputingDevelopmental1999.pdf}
}

@article{cordenBoomingSectorDutch1984,
  title = {Booming {{Sector}} and {{Dutch Disease Economics}}: {{Survey}} and {{Consolidation}}},
  shorttitle = {Booming {{Sector}} and {{Dutch Disease Economics}}},
  author = {Corden, W. M.},
  date = {1984},
  journaltitle = {Oxford Economic Papers},
  volume = {36},
  number = {3},
  eprint = {2662669},
  eprinttype = {jstor},
  pages = {359--380},
  publisher = {{Oxford University Press}},
  issn = {0030-7653}
}

@article{coseSagaScottsboroBoys2020,
  title = {The Saga of {{The Scottsboro Boys}}},
  author = {Cose, Ellis},
  date = {2020-07-27},
  journaltitle = {ACLU},
  url = {https://www.aclu.org/issues/racial-justice/saga-scottsboro-boys},
  urldate = {2022-02-25},
  abstract = {The case of the Scottsboro Boys provided an unforgettable window into the South’s brutal system of justice — and how it failed Black Americans.},
  entrysubtype = {magazine}
}

@article{costanzaDevelopmentTimeLeave2014,
  title = {Development: {{Time}} to Leave {{GDP}} Behind},
  shorttitle = {Development},
  author = {Costanza, Robert and Kubiszewski, Ida and Giovannini, Enrico and Lovins, Hunter and McGlade, Jacqueline and Pickett, Kate E. and Ragnarsdóttir, Kristín Vala and Roberts, Debra and De Vogli, Roberto and Wilkinson, Richard},
  date = {2014-01},
  journaltitle = {Nature},
  volume = {505},
  number = {7483},
  pages = {283--285},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/505283a},
  url = {https://www.nature.com/articles/505283a},
  urldate = {2022-11-01},
  abstract = {Gross domestic product is a misleading measure of national success. Countries should act now to embrace new metrics, urge Robert Costanza and colleagues.},
  issue = {7483},
  langid = {english},
  keywords = {Economics,Environmental sciences,Policy,Sociology},
  file = {/Users/hugo/Papers/pdf/costanzaDevelopmentTimeLeave2014.pdf;/Users/hugo/Zotero/storage/QGNJSN6C/505283a.html}
}

@incollection{crutchfieldAnythingEverNew2008,
  title = {Is {{Anything Ever New}}? {{Considering Emergence}}},
  shorttitle = {Is {{Anything Ever New}}?},
  booktitle = {Emergence},
  author = {Crutchfield, James P.},
  editor = {Bedau, Mark A. and Humphreys, Paul},
  date = {2008-03-28},
  pages = {269--286},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9780262026215.003.0018},
  url = {https://academic.oup.com/mit-press-scholarship-online/book/13765/chapter/167404216},
  urldate = {2022-11-15},
  isbn = {978-0-262-02621-5}
}

@article{crutchfieldCalculiEmergenceComputation1994,
  title = {The Calculi of Emergence: Computation, Dynamics and Induction},
  shorttitle = {The Calculi of Emergence},
  author = {Crutchfield, James P.},
  date = {1994-08-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {75},
  number = {1},
  pages = {11--54},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(94)90273-9},
  url = {http://www.sciencedirect.com/science/article/pii/0167278994902739},
  urldate = {2020-06-19},
  abstract = {Defining structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientific activities. Despite the difficulties, these problems can be analyzed in terms of how model-building observers infer from measurements the computational capabilities embedded in nonlinear processes. An observer's notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtlely though on how those resources are organized. The descriptive power of the observer's chosen (or implicit) computational model class, for example, can be an overwhelming determinant in finding regularity in data. This paper presents an overview of an inductive framework-hierarchical ϵ-machine reconstruction—in which the emergence of complexity is associated with the innovation of new computational model classes. Complexity metrics for detecting structure and quantifying emergence, along with an analysis of the constraints on the dynamics of innovation, are outlined. Illustrative examples are drawn from the onset of unpredictability in nonlinear systems, finitary nondeterministic processes, and cellular automata pattern recognition. They demonstrate how finite inference resources drive the innovation of new structures and so lead to the emergence of complexity.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/QQ5LV6ZX/0167278994902739.html}
}

@inproceedings{crutchfieldComputationOnsetChaos1988,
  title = {Computation at the Onset of Chaos},
  booktitle = {The {{Santa Fe Institute}}, {{Westview}}},
  author = {Crutchfield, James P. and Young, Karl},
  date = {1988},
  pages = {223--269},
  publisher = {{Press}},
  abstract = {Computation at levels beyond storage and transmission of information appears in physical systems at phase transitions. We investigate this phenomenon using minimal computational models of dynamical systems that undergo a transition to chaos as a function of a nonlinearity parameter. For period-doubling and band-merging cascades, we derive expressions for the entropy, the interdependence of-machine complexity and entropy, and the latent complexity of the transition to chaos. At the transition deterministic finite automaton models diverge in size. Although there is no regular or context-free Chomsky grammar in this case, we give finite descriptions at the higher computational level of context-free Lindenmayer systems. We construct a restricted indexed context-free grammar and its associated one-way nondeterministic nested stack automaton for the cascade limit language. This analysis of a family of dynamical systems suggests a complexity theoretic description of phase transitions based on the informational diversity and computational complexity of observed data that is independent of particular system control parameters. The approach gives a much more refined picture of the architecture of critical states than is available via},
  file = {/Users/hugo/Papers/pdf/crutchfieldComputationOnsetChaos1988.pdf;/Users/hugo/Zotero/storage/BK78H5C9/summary.html}
}

@article{crutchfieldEvolutionEmergentComputation1995,
  title = {The Evolution of Emergent Computation.},
  author = {Crutchfield, J. P. and Mitchell, M.},
  date = {1995-11-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {92},
  number = {23},
  pages = {10742--10746},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.92.23.10742},
  url = {http://www.pnas.org/cgi/doi/10.1073/pnas.92.23.10742},
  urldate = {2020-05-28},
  abstract = {A simple evolutionary process can discover sophisticated methods for emergent information processing in decentralized spatially extended systems. The mechanisms underlying the resulting emergent computation are explicated by a technique for analyzing particle-based logic embedded in pattern-forming systems. Understanding how globally coordinated computation can emerge in evolution is relevant both for the scientific understanding of natural information processing and for engineering new forms of parallel computing systems.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/crutchfieldEvolutionEmergentComputation1995.pdf}
}

@article{crutchfieldInferringStatisticalComplexity1989,
  title = {Inferring Statistical Complexity},
  author = {Crutchfield, James P. and Young, Karl},
  date = {1989-07-10},
  journaltitle = {Physical Review Letters},
  volume = {63},
  number = {2},
  pages = {105--108},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.63.105},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.63.105},
  urldate = {2020-03-05},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/crutchfieldInferringStatisticalComplexity1989.pdf}
}

@article{crutchfieldTurbulentPatternBases1993,
  title = {Turbulent Pattern Bases for Cellular Automata},
  author = {Crutchfield, James P. and Hanson, James E.},
  date = {1993-12},
  journaltitle = {Physica D: Nonlinear Phenomena},
  volume = {69},
  number = {3-4},
  pages = {279--301},
  issn = {01672789},
  doi = {10.1016/0167-2789(93)90092-F},
  url = {https://linkinghub.elsevier.com/retrieve/pii/016727899390092F},
  urldate = {2020-03-05},
  abstract = {Unpredictable patterns generated by cellular automata (CA) can be decomposed with respect to a turbulent, positive entropy rate pattern basis. The resulting filtered patterns uncover significant structural organization in a CA’s dynamics and information processing capabilities. We illustrate the decomposition technique by analyzing a binary, range-2 cellular automaton having two invariant chaotic domains of different complexities and entropies. Once identified, the domains are seen to organize the CA’s state space and to dominate its evolution. Starting from the domains’ structures, we show how to construct a finite-state transducer that performs nonlinear spatial filtering such that the resulting space-time patterns reveal the domains and the intervening walls and dislocations. To show the statistical consequences of domain detection, we compare the entropy and complexity densities of each domain with the globally averaged quantities. A more graphical comparison uses difference patterns and difference plumes which trace the space-time influence of a single-site perturbation. We also investigate the diversity of walls and particles emanating from the interface between two adjacent domains.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/crutchfieldTurbulentPatternBases1993.pdf}
}

@article{cullyQualityDiversityOptimization2017,
  title = {Quality and Diversity Optimization: {{A}} Unifying Modular Framework},
  shorttitle = {Quality and Diversity Optimization},
  author = {Cully, Antoine and Demiris, Yiannis},
  date = {2017},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {22},
  number = {2},
  pages = {245--259},
  publisher = {{IEEE}},
  file = {/Users/hugo/Zotero/storage/QP6IFTBN/7959075.html}
}

@article{cullyRobotsThatCan2015,
  title = {Robots That Can Adapt like Animals},
  author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {503--507},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14422},
  url = {https://www.nature.com/articles/nature14422},
  urldate = {2020-07-27},
  abstract = {An intelligent trial-and-error learning algorithm is presented that allows robots to adapt in minutes to compensate for a wide variety of types of damage.},
  issue = {7553},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/cullyRobotsThatCan2015.pdf;/Users/hugo/Zotero/storage/JCP428V7/nature14422.html}
}

@article{cuturiSemidualRegularizedOptimal2018,
  title = {Semi-Dual {{Regularized Optimal Transport}}},
  author = {Cuturi, Marco and Peyré, Gabriel},
  date = {2018-01},
  journaltitle = {SIAM Review},
  volume = {60},
  number = {4},
  eprint = {1811.05527},
  eprinttype = {arxiv},
  pages = {941--965},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/18M1208654},
  url = {http://arxiv.org/abs/1811.05527},
  urldate = {2018-11-22},
  abstract = {Variational problems that involve Wasserstein distances and more generally optimal transport (OT) theory are playing an increasingly important role in data sciences. Such problems can be used to form an examplar measure out of various probability measures, as in the Wasserstein barycenter problem, or to carry out parametric inference and density fitting, where the loss is measured in terms of an optimal transport cost to the measure of observations. Despite being conceptually simple, such problems are computationally challenging because they involve minimizing over quantities (Wasserstein distances) that are themselves hard to compute. Entropic regularization has recently emerged as an efficient tool to approximate the solution of such variational Wasserstein problems. In this paper, we give a thorough duality tour of these regularization techniques. In particular, we show how important concepts from classical OT such as c-transforms and semi-discrete approaches translate into similar ideas in a regularized setting. These dual formulations lead to smooth variational problems, which can be solved using smooth, differentiable and convex optimization problems that are simpler to implement and numerically more stable that their un-regularized counterparts. We illustrate the versatility of this approach by applying it to the computation of Wasserstein barycenters and gradient flows of spatial regularization functionals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/cuturiSemidualRegularizedOptimal22.pdf;/Users/hugo/Zotero/storage/SG7IETYB/1811.html}
}

@inproceedings{cuturiSinkhornDistancesLightspeed2013,
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transport}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Cuturi, Marco},
  date = {2013},
  pages = {9},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf},
  abstract = {Optimal transport distances are a fundamental family of distances for probability measures and histograms of features. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost can quickly become prohibitive whenever the size of the support of these measures or the histograms’ dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport problems from a maximumentropy perspective. We smooth the classic optimal transport problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn’s matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon classic optimal transport distances on the MNIST classification problem.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/cuturiSinkhornDistancesLightspeed22.pdf}
}

@unpublished{cuturiSmoothedDualApproach2015,
  title = {A {{Smoothed Dual Approach}} for {{Variational Wasserstein Problems}}},
  author = {Cuturi, Marco and Peyré, Gabriel},
  date = {2015-03-09},
  eprint = {1503.02533},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1503.02533},
  urldate = {2018-11-22},
  abstract = {Variational problems that involve Wasserstein distances have been recently proposed to summarize and learn from probability measures. Despite being conceptually simple, such problems are computationally challenging because they involve minimizing over quantities (Wasserstein distances) that are themselves hard to compute. We show that the dual formulation of Wasserstein variational problems introduced recently by Carlier et al. (2014) can be regularized using an entropic smoothing, which leads to smooth, differentiable, convex optimization problems that are simpler to implement and numerically more stable. We illustrate the versatility of this approach by applying it to the computation of Wasserstein barycenters and gradient flows of spacial regularization functionals.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/cuturiSmoothedDualApproach22.pdf;/Users/hugo/Zotero/storage/TPTF7WII/1503.html}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12},
  journaltitle = {Mathematics of Control, Signals, and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  url = {http://link.springer.com/10.1007/BF02551274},
  urldate = {2020-10-07},
  langid = {english}
}

@misc{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  date = {2019-06-02},
  number = {arXiv:1901.02860},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.02860},
  url = {http://arxiv.org/abs/1901.02860},
  urldate = {2022-07-27},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/daiTransformerXLAttentiveLanguage2019.pdf;/Users/hugo/Zotero/storage/JM4JQTBZ/1901.html}
}

@inproceedings{dasGeneticAlgorithmDiscovers1994,
  title = {A Genetic Algorithm Discovers Particle-Based Computation in Cellular Automata},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} — {{PPSN III}}},
  author = {Das, Rajarshi and Mitchell, Melanie and Crutchfield, James P.},
  editor = {Davidor, Yuval and Schwefel, Hans-Paul and Männer, Reinhard},
  date = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {344--353},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-58484-6_278},
  abstract = {How does evolution produce sophisticated emergent computation in systems composed of simple components limited to local interactions? To model such a process, we used a genetic algorithm (GA) to evolve cellular automata to perform a computational task requiring globally-coordinated information processing. On most runs a class of relatively unsophisticated strategies was evolved, but on a subset of runs a number of quite sophisticated strategies was discovered. We analyze the emergent logic underlying these strategies in terms of information processing performed by “particles” in space-time, and we describe in detail the generational progression of the GA evolution of these strategies. Our analysis is a preliminary step in understanding the general mechanisms by which sophisticated emergent computational capabilities can be automatically produced in decentralized multiprocessor systems.},
  isbn = {978-3-540-49001-2},
  langid = {english},
  keywords = {Cellular Automaton,Deterministic Finite Automaton,Genetic Algorithm,Initial Configuration,Regular Domain}
}

@book{dasguptaEvolutionaryAlgorithmsEngineering2013,
  title = {Evolutionary Algorithms in Engineering Applications},
  author = {Dasgupta, Dipankar and Michalewicz, Zbigniew},
  date = {2013},
  publisher = {{Springer Science \& Business Media}}
}

@article{davisWhatKnowledgeRepresentation1993,
  title = {What Is a {{Knowledge Representation}}?},
  author = {Davis, Randall and Shrobe, Howard and Szolovits, Peter},
  date = {1993},
  journaltitle = {AI Magazine},
  volume = {14},
  number = {1},
  pages = {17--33},
  abstract = {Although knowledge representation is one of the central and in some ways most familiar concepts in AI, the most fundamental question about it - What is it? - has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various prop erties a representation should have while still others have fo cused on prop erties that are imp ortant to the notion of representation in general In this paper we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and at times conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field.},
  file = {/Users/hugo/Papers/pdf/davisWhatKnowledgeRepresentation12.pdf}
}

@report{dechezlepretreFightingClimateChange2022,
  title = {Fighting Climate Change: {{International}} Attitudes toward Climate Policies},
  shorttitle = {Fighting Climate Change},
  author = {Dechezleprêtre, Antoine and Fabre, Adrien and Kruse, Tobias and Planterose, Bluebery and Chico, Ana Sanchez and Stantcheva, Stefanie},
  date = {2022},
  institution = {{National Bureau of Economic Research}},
  file = {/Users/hugo/Papers/pdf/dechezlepretreFightingClimateChange2022.pdf;/Users/hugo/Zotero/storage/8FGVYRN4/w30265.html}
}

@unpublished{defazioSAGAFastIncremental2014,
  title = {{{SAGA}}: {{A Fast Incremental Gradient Method With Support}} for {{Non-Strongly Convex Composite Objectives}}},
  shorttitle = {{{SAGA}}},
  author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  date = {2014-07-01},
  eprint = {1407.0202},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1407.0202},
  urldate = {2018-11-22},
  abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/defazioSAGAFastIncremental22.pdf;/Users/hugo/Zotero/storage/D6DGH4BI/1407.html}
}

@article{dehaanHowEmergenceArises2006,
  title = {How Emergence Arises},
  author = {de Haan, J.},
  options = {useprefix=true},
  date = {2006-12-01},
  journaltitle = {Ecological Complexity},
  shortjournal = {Ecological Complexity},
  series = {Complexity and {{Ecological Economics}}},
  volume = {3},
  number = {4},
  pages = {293--301},
  issn = {1476-945X},
  doi = {10.1016/j.ecocom.2007.02.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1476945X07000049},
  urldate = {2022-11-15},
  abstract = {The concept of emergence is used in a variety of scientific fields and disciplines to denote an even wider variety of phenomena. This has resulted in a plethora of perspectives and treatments. The diversity of phenomena going under the moniker of emergence raises the question of what these phenomena have in common and whether different types of emergence can be distinguished on general grounds. Since emergence is recognised in so many different fields as a relevant concept, it would be useful to have a general conceptual framework that allows a treatment of emergence without explicit reference to the specific underlying mechanism. This article aims to provide a general framework in which emergence can be addressed. For this a new concept, the conjugate, is introduced and the place of the observer relative to the system is given a renewed appreciation. Since the framework abstracts from the underlying interactions, emergence in different types of systems can be compared and discussed on equal terms. Reframing emergence in this way leads to a typology of emergences that is both illuminating and integrative In the elaboration the concepts and different types of emergence are discussed and compared with other treatments and typologies of emergence in the literature. Examples from various fields are given to illustrate the theoretical discourse. Furthermore applications of the concepts and typology to social systems, evolution, and co-evolution in ecosystems will be given It will be concluded that many apparently different views and treatments of emergence can be restated and compared within the framework presented in this article.},
  langid = {english},
  keywords = {Complexity,Conceptual framework,Emergence,Evolution,Observer,Typology},
  file = {/Users/hugo/Zotero/storage/5IRNAPN8/S1476945X07000049.html}
}

@incollection{delacourtRealTimeLanguage2007,
  title = {Real {{Time Language Recognition}} on {{2D Cellular Automata}}: {{Dealing}} with {{Non-convex Neighborhoods}}},
  shorttitle = {Real {{Time Language Recognition}} on {{2D Cellular Automata}}},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2007},
  author = {Delacourt, Martin and Poupet, Victor},
  editor = {Kučera, Luděk and Kučera, Antonín},
  date = {2007},
  volume = {4708},
  pages = {298--309},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74456-6_28},
  url = {http://link.springer.com/10.1007/978-3-540-74456-6_28},
  urldate = {2019-05-09},
  abstract = {In this paper we study language recognition by two-dimensional cellular automata on different possible neighborhoods. Since it is known that all complete neighborhoods are linearly equivalent we focus on a natural sub-linear complexity class: the real time.},
  isbn = {978-3-540-74455-9 978-3-540-74456-6},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/delacourtRealTimeLanguage22.pdf}
}

@unpublished{delangeContinualLearningSurvey2020,
  title = {A Continual Learning Survey: {{Defying}} Forgetting in Classification Tasks},
  shorttitle = {A Continual Learning Survey},
  author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
  date = {2020-05-26},
  eprint = {1909.08383},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.08383},
  urldate = {2020-11-27},
  abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/delangeContinualLearningSurvey2020.pdf;/Users/hugo/Zotero/storage/TDP6I7LT/1909.html}
}

@inproceedings{delgiornoDiscriminativeFrameworkAnomaly2016,
  title = {A {{Discriminative Framework}} for {{Anomaly Detection}} in {{Large Videos}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Del Giorno, Allison and Bagnell, J. Andrew and Hebert, Martial},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {334--349},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46454-1_21},
  abstract = {We address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of temporal ordering. Current algorithms in anomaly detection are based on the classical density estimation approach of learning high-dimensional models and finding low-probability events. These algorithms are sensitive to the order in which anomalies appear and require either training data or early context assumptions that do not hold for longer, more complex videos. By defining anomalies as examples that can be distinguished from other examples in the same video, our definition inspires a shift in approaches from classical density estimation to simple discriminative learning. Our contributions include a novel framework for anomaly detection that is (1) independent of temporal ordering of anomalies, and (2) unsupervised, requiring no separate training sequences. We show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing training sequences from standard datasets.},
  isbn = {978-3-319-46454-1},
  langid = {english},
  keywords = {Anomaly detection,Context,Discriminative,Surveillance,Temporal invariance,Unsupervised},
  file = {/Users/hugo/Zotero/storage/YYLH84SC/delgiornoDiscriminativeFrameworkAnomaly2016.pdf}
}

@inproceedings{denkowskiMeteorUniversalLanguage2014,
  title = {Meteor {{Universal}}: {{Language Specific Translation Evaluation}} for {{Any Target Language}}},
  shorttitle = {Meteor {{Universal}}},
  booktitle = {Proceedings of the {{Ninth Workshop}} on {{Statistical Machine Translation}}, {{WMT}}@{{ACL}} 2014, {{June}} 26-27, 2014, {{Baltimore}}, {{Maryland}}, {{USA}}},
  author = {Denkowski, Michael J. and Lavie, Alon},
  date = {2014},
  pages = {376--380},
  publisher = {{The Association for Computer Linguistics}},
  doi = {10.3115/v1/w14-3348},
  file = {/Users/hugo/Papers/pdf/denkowskiMeteorUniversalLanguage2014.pdf}
}

@book{dennettDarwinDangerousIdea1996,
  title = {Darwin's Dangerous Idea: Evolution and the Meanings of Life},
  shorttitle = {Darwin's Dangerous Idea},
  author = {Dennett, D. C. and Dennett, D. C.},
  date = {1996},
  series = {A {{Touchstone}} Book},
  edition = {1. Touchstone ed},
  publisher = {{Simon \& Schuster}},
  location = {{New York}},
  isbn = {978-0-684-82471-0 978-0-684-80290-9},
  langid = {english},
  pagetotal = {586}
}

@article{departmentofmechanicalengineeringben-gurionuniversityComplexitySteeringCellular2018,
  title = {Complexity {{Steering}} in {{Cellular Automata}}},
  author = {{Department of Mechanical Engineering Ben-Gurion University} and Peled, Bar Y. and Carmi, Avishy Y. and {Department of Mechanical Engineering Ben-Gurion University}},
  date = {2018-06-15},
  journaltitle = {Complex Systems},
  pages = {159--175},
  issn = {08912513},
  doi = {10.25088/ComplexSystems.27.2.159},
  url = {http://www.complex-systems.com/abstracts/v27_i02_a04/},
  urldate = {2019-05-07},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/departmentofmechanicalengineeringben-gurionuniversityComplexitySteeringCellular22.pdf}
}

@article{desaIncrementalKnowledgeBase2017,
  title = {Incremental Knowledge Base Construction Using {{DeepDive}}},
  author = {De Sa, Christopher and Ratner, Alex and Ré, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce},
  date = {2017},
  journaltitle = {VLDB Journal},
  volume = {26},
  number = {1},
  eprint = {24655651},
  eprinttype = {pmid},
  pages = {81--105},
  issn = {0949877X},
  doi = {10.1007/s00778-016-0437-2},
  abstract = {Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.},
  keywords = {Incremental,Knowledge base construction,Performance}
}

@misc{dettmersLLMInt88bit2022,
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  date = {2022-08-15},
  number = {arXiv:2208.07339},
  eprint = {2208.07339},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.07339},
  url = {http://arxiv.org/abs/2208.07339},
  urldate = {2022-08-18},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/dettmersLLMInt88bit2022.pdf;/Users/hugo/Zotero/storage/VBL6I8FU/2208.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-07-22},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/devlinBERTPretrainingDeep2019a.pdf;/Users/hugo/Zotero/storage/MSINDEKA/1810.html}
}

@inproceedings{dewolfEmergenceSelfOrganisationDifferent2005,
  title = {Emergence {{Versus Self-Organisation}}: {{Different Concepts}} but {{Promising When Combined}}},
  shorttitle = {Emergence {{Versus Self-Organisation}}},
  booktitle = {Engineering {{Self-Organising Systems}}},
  author = {De Wolf, Tom and Holvoet, Tom},
  editor = {Brueckner, Sven A. and Di Marzo Serugendo, Giovanna and Karageorgos, Anthony and Nagpal, Radhika},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--15},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11494676_1},
  abstract = {A clear terminology is essential in every research discipline. In the context of ESOA, a lot of confusion exists about the meaning of the terms emergence and self-organisation. One of the sources of the confusion comes from the fact that a combination of both phenomena often occurs in dynamical systems. In this paper a historic overview of the use of each concept as well as a working definition, that is compatible with the historic and current meaning of the concepts, is given. Each definition is explained by supporting it with important characteristics found in the literature. We show that emergence and self-organisation each emphasise different properties of a system. Both phenomena can exist in isolation. The paper also outlines some examples of such systems and considers the combination of emergence and self-organisation as a promising approach in complex multi-agent systems.},
  isbn = {978-3-540-31901-6},
  langid = {english},
  keywords = {Chaotic Attractor,Complex Adaptive System,Emergent Property,Global Behaviour,Historic Overview},
  file = {/Users/hugo/Papers/pdf/dewolfEmergenceSelfOrganisationDifferent2005.pdf}
}

@article{dicaprio3DCellularAutomata2016,
  title = {{{3D}} Cellular Automata Simulations of Intra and Intergranular Corrosion},
  author = {Di Caprio, Dung and Stafiej, Janusz and Luciano, Giorgio and Arurault, Laurent},
  date = {2016},
  journaltitle = {Corrosion Science},
  volume = {112},
  pages = {438--450},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Papers/pdf/dicaprio3DCellularAutomata2016.pdf;/Users/hugo/Zotero/storage/BE4Z63PH/S0010938X1630395X.html}
}

@article{dittrichArtificialChemistriesReview2001,
  title = {Artificial {{Chemistries}}—{{A Review}}},
  author = {Dittrich, Peter and Ziegler, Jens and Banzhaf, Wolfgang},
  date = {2001-07},
  journaltitle = {Artificial Life},
  volume = {7},
  number = {3},
  pages = {225--275},
  issn = {1064-5462},
  doi = {10.1162/106454601753238636},
  abstract = {This article reviews the growing body of scientific work in artificial chemistry. First, common motivations and fundamental concepts are introduced. Second, current research activities are discussed along three application dimensions: modeling, information processing, and optimization. Finally, common phenomena among the different systems are summarized. It is argued here that artificial chemistries are “the right stuff” for the study of prebiotic and biochemical evolution, and they provide a productive framework for questions regarding the origin and evolution of organizations in general. Furthermore, artificial chemistries have a broad application range of practical problems, as shown in this review.},
  eventtitle = {Artificial {{Life}}},
  keywords = {chemical computing,complex systems,emergence,evolution,molecular simulation,origin of life,self-organization},
  file = {/Users/hugo/Zotero/storage/KRTMSMHW/6790776.html}
}

@online{dobbeAIClimateChange2019,
  title = {{{AI}} and {{Climate Change}}: {{How}} They’re Connected, and What We Can Do about It},
  shorttitle = {{{AI}} and {{Climate Change}}},
  author = {Dobbe, R. and Whittaker, M.},
  date = {2019-10-17T17:35:55},
  url = {https://medium.com/@AINowInstitute/ai-and-climate-change-how-theyre-connected-and-what-we-can-do-about-it-6aa8d0f5b32c},
  urldate = {2022-05-02},
  abstract = {On September 20th, workers from 12 tech companies joined the global climate strike, highlighting tech’s role in climate change and…},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/hugo/Zotero/storage/PS77XMAC/ai-and-climate-change-how-theyre-connected-and-what-we-can-do-about-it-6aa8d0f5b32c.html}
}

@book{domingosMarkovLogicInterface2009,
  title = {Markov {{Logic}}: {{An Interface Layer}} for {{Artificial Intelligence}}},
  shorttitle = {Markov {{Logic}}},
  author = {Domingos, Pedro and Lowd, Daniel},
  date = {2009},
  edition = {1st},
  publisher = {{Morgan and Claypool Publishers}},
  abstract = {Most subfields of computer science have an interface layer via which applications communicate with the infrastructure, and this is key to their success (e.g., the Internet in networking, the relational model in databases, etc.). So far this interface layer has been missing in AI. First-order logic and probabilistic graphical models each have some of the necessary features, but a viable interface layer requires combining both. Markov logic is a powerful new language that accomplishes this by attaching weights to first-order formulas and treating them as templates for features of Markov random fields. Most statistical models in wide use are special cases of Markov logic, and first-order logic is its infinite-weight limit. Inference algorithms for Markov logic combine ideas from satisfiability, Markov chain Monte Carlo, belief propagation, and resolution. Learning algorithms make use of conditional likelihood, convex optimization, and inductive logic programming. Markov logic has been successfully applied to problems in information extraction and integration, natural language processing, robot mapping, social networks, computational biology, and others, and is the basis of the open-source Alchemy system.},
  isbn = {978-1-59829-692-1}
}

@inproceedings{doncieuxNoveltySearchTheoretical2019,
  title = {Novelty Search: A Theoretical Perspective},
  shorttitle = {Novelty Search},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Doncieux, Stephane and Laflaquière, Alban and Coninx, Alexandre},
  date = {2019-07-13},
  pages = {99--106},
  publisher = {{ACM}},
  location = {{Prague Czech Republic}},
  doi = {10.1145/3321707.3321752},
  url = {https://dl.acm.org/doi/10.1145/3321707.3321752},
  urldate = {2022-11-10},
  abstract = {Novelty Search is an exploration algorithm driven by the novelty of a behavior. The same individual evaluated at different generations has different fitness values. The corresponding fitness landscape is thus constantly changing and if, at the scale of a single generation, the metaphor of a fitness landscape with peaks and valleys still holds, this is not the case anymore at the scale of the whole evolutionary process. How does this kind of algorithms behave? Is it possible to define a model that would help understand how it works? This understanding is critical to analyse existing Novelty Search variants and design new and potentially more efficient ones. We assert that Novelty Search asymptotically behaves like a uniform random search process in the behavior space. This is an interesting feature, as it is not possible to directly sample in this space: the algorithm has a direct access to the genotype space only, whose relationship to the behavior space is complex. We describe the model and check its consistency on a classical Novelty Search experiment. We also show that it sheds a new light on results of the literature and suggests future research work.},
  eventtitle = {{{GECCO}} '19: {{Genetic}} and {{Evolutionary Computation Conference}}},
  isbn = {978-1-4503-6111-8},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/doncieuxNoveltySearchTheoretical2019.pdf}
}

@article{dongDataFusionKnowledge2015,
  title = {From {{Data Fusion}} to {{Knowledge Fusion}}},
  author = {Dong, Xin Luna and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Murphy, Kevin and Sun, Shaohua and Zhang, Wei},
  date = {2015},
  issn = {21508097},
  doi = {10.14778/2732951.2732962},
  abstract = {The task of data fusion is to identify the true values of data items (eg, the true date of birth for Tom Cruise) among multiple observed values drawn from different sources (eg, Web sites) of varying (and unknown) reliability. A recent survey LDL+12 has provided a detailed comparison of various fusion methods on Deep Web data. In this paper, we study the applicability and limitations of different fusion techniques on a more challenging problem: knowledge fusion. Knowledge fusion identifies true subject-predicate-object triples extracted by multiple information extractors from multiple information sources. These extractors perform the tasks of entity linkage and schema alignment, thus introducing an additional source of noise that is quite different from that traditionally considered in the data fusion literature, which only focuses on factual errors in the original sources. We adapt state-of-the-art data fusion techniques and apply them to a knowledge base with 1.6B unique knowledge triples extracted by 12 extractors from over 1B Web pages, which is three orders of magnitude larger than the data sets used in previous data fusion papers. We show great promise of the data fusion approaches in solving the knowledge fusion problem, and suggest interesting research directions through a detailed error analysis of the methods.},
  file = {/Users/hugo/Papers/pdf/dongDataFusionKnowledge22.pdf}
}

@article{dongIntegratingConflictingData2009,
  title = {Integrating Conflicting Data: The Role of Source Dependence},
  author = {Dong, Xin Luna and Berti-Equille, Laure and Srivastava, Divesh},
  date = {2009},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {2},
  number = {1},
  pages = {550--561},
  issn = {2150-8097},
  doi = {10.14778/1687627.1687690},
  url = {http://www.norc.org/PDFs/May 2011 Personal Validation and Entity Resolution Conference/Integrating Conflicting Data paper_PVERConf_May2011.pdf http://dl.acm.org/citation.cfm?id=1687627.1687690},
  abstract = {Many data management applications, such as setting up Web por- tals, managing enterprise data, managing community data, and shar- ing scientific data, require integrating data from multiple sources. Each of these sources provides a set of values and different sources can often provide conflicting values. To present quality data to users, it is critical that data integration systems can resolve con- flicts and discover true values. Typically, we expect a true value to be provided by more sources than any particular false one, so we can take the value provided by the majority of the sources as the truth. Unfortunately, a false value can be spread through copying and that makes truth discovery extremely tricky. In this paper, we consider how to find true values from conflicting information when there are a large number of sources, among which some may copy from others. We present a novel approach that considers dependence between data sources in truth discovery. Intuitively, if two data sources pro- vide a large number of common values and many of these values are rarely provided by other sources (e.g., particular false values), it is very likely that one copies from the other. We apply Bayesian analysis to decide dependence between sources and design an algo- rithm that iteratively detects dependence and discovers truth from conflicting information. We also extend our model by consider- ing accuracy of data sources and similarity between values. Our experiments on synthetic data as well as real-world data show that our algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources.},
  file = {/Users/hugo/Papers/pdf/dongIntegratingConflictingData22.pdf}
}

@article{dongKnowledgeVaultWebscale2014,
  title = {Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion},
  author = {Dong, Xin and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
  date = {2014},
  journaltitle = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
  eprint = {1975973},
  eprinttype = {pmid},
  pages = {601--610},
  issn = {0893-6080},
  doi = {10.1145/2623330.2623623},
  url = {http://dl.acm.org/citation.cfm?doid=2623330.2623623},
  abstract = {Recent years have witnessed a proliferation of large-scale knowledge bases, includingWikipedia, Freebase, YAGO,Mi- crosoft's Satori, and Google's Knowledge Graph. To in- crease the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous ap- proaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that com- bines extractions fromWeb content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived fromexisting knowledge repos- itories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilis- tic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.},
  keywords = {information extraction,knowledge bases,machine learning,probabilistic models}
}

@unpublished{dongReservoirComputingMeets2020,
  title = {Reservoir {{Computing}} Meets {{Recurrent Kernels}} and {{Structured Transforms}}},
  author = {Dong, Jonathan and Ohana, Ruben and Rafayelyan, Mushegh and Krzakala, Florent},
  date = {2020-06-12},
  eprint = {2006.07310},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2006.07310},
  urldate = {2020-09-28},
  abstract = {Reservoir Computing is a class of simple yet efficient Recurrent Neural Networks where internal weights are fixed at random and only a linear output layer is trained. In the large size limit, such random neural networks have a deep connection with kernel methods. Our contributions are threefold: a) We rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence. b) We test our models on chaotic time series prediction, a classic but challenging benchmark in Reservoir Computing, and show how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate. c) When the number of samples is too large, we leverage the success of structured Random Features for kernel approximation by introducing Structured Reservoir Computing. The two proposed methods, Recurrent Kernel and Structured Reservoir Computing, turn out to be much faster and more memory-efficient than conventional Reservoir Computing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/dongReservoirComputingMeets2020.pdf}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2022-07-27},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/dosovitskiyImageWorth16x162021.pdf;/Users/hugo/Zotero/storage/6GH7XGUB/2010.html}
}

@unpublished{duanRLFastReinforcement2016,
  title = {{{RL}}\$\^2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
  shorttitle = {{{RL}}\$\^2\$},
  author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
  date = {2016-11-09},
  eprint = {1611.02779},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.02779},
  urldate = {2020-01-28},
  abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/EGFXD8DI/duanRLFastReinforcement2016.pdf;/Users/hugo/Zotero/storage/MI2MYYLI/1611.html}
}

@misc{duGLaMEfficientScaling2021,
  title = {{{GLaM}}: {{Efficient Scaling}} of {{Language Models}} with {{Mixture-of-Experts}}},
  shorttitle = {{{GLaM}}},
  author = {Du, Nan and Huang, Yanping and Dai, Andrew M. and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and Zoph, Barret and Fedus, Liam and Bosma, Maarten and Zhou, Zongwei and Wang, Tao and Wang, Yu Emma and Webster, Kellie and Pellat, Marie and Robinson, Kevin and Meier-Hellstern, Kathy and Duke, Toju and Dixon, Lucas and Zhang, Kun and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng and Cui, Claire},
  date = {2021-12-13},
  number = {arXiv:2112.06905},
  eprint = {2112.06905},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.06905},
  urldate = {2022-07-26},
  abstract = {Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/duGLaMEfficientScaling2021.pdf;/Users/hugo/Zotero/storage/7EANTH64/2112.html}
}

@misc{duGLMGeneralLanguage2022,
  title = {{{GLM}}: {{General Language Model Pretraining}} with {{Autoregressive Blank Infilling}}},
  shorttitle = {{{GLM}}},
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  date = {2022-03-17},
  number = {arXiv:2103.10360},
  eprint = {2103.10360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.10360},
  url = {http://arxiv.org/abs/2103.10360},
  urldate = {2022-07-27},
  abstract = {There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/duGLMGeneralLanguage2022.pdf;/Users/hugo/Zotero/storage/P5C5Q4WF/2103.html}
}

@book{dysonOriginsLife1999,
  title = {Origins of Life},
  author = {Dyson, Freeman J.},
  date = {1999},
  edition = {Rev. ed},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge [England] ; New York}},
  isbn = {978-0-521-62668-2},
  pagetotal = {100},
  keywords = {Life,Origin}
}

@inproceedings{efrosTextureSynthesisNonparametric1999,
  title = {Texture Synthesis by Non-Parametric Sampling},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Efros, A.A. and Leung, T.K.},
  date = {1999},
  pages = {1033-1038 vol.2},
  publisher = {{IEEE}},
  location = {{Kerkyra, Greece}},
  doi = {10.1109/ICCV.1999.790383},
  url = {http://ieeexplore.ieee.org/document/790383/},
  urldate = {2019-06-19},
  abstract = {A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.},
  eventtitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  isbn = {978-0-7695-0164-2},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/efrosTextureSynthesisNonparametric12.pdf}
}

@article{ehrlingerDefinitionKnowledgeGraphs2016,
  title = {Towards a {{Deﬁnition}} of {{Knowledge Graphs}}},
  author = {Ehrlinger, Lisa and Wöß, Wolfram},
  date = {2016},
  pages = {4},
  abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly influenced by the introduction of Google’s Knowledge Graph in 2012, and since then the term has been widely used without a definition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google’s Knowledge Graph, although no official documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a definition that is free from ambiguity. We tackle this issue by discussing and defining the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a definition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/ehrlingerDefinitionKnowledgeGraphs22.pdf}
}

@book{eigenHypercycle1979,
  title = {The {{Hypercycle}}},
  author = {Eigen, Manfred and Schuster, Peter},
  date = {1979},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-67247-7},
  url = {http://link.springer.com/10.1007/978-3-642-67247-7},
  urldate = {2022-11-08},
  isbn = {978-3-540-09293-3 978-3-642-67247-7},
  langid = {english},
  keywords = {Charles Darwin,Darwin,dynamics,evolution,genes,Hypercycle,translation}
}

@article{elmanFindingStructureTime1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L.},
  date = {1990},
  journaltitle = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog1402_1},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
  urldate = {2022-02-20},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402\_1},
  file = {/Users/hugo/Papers/pdf/elmanFindingStructureTime1990.pdf;/Users/hugo/Zotero/storage/FRIQ94YD/s15516709cog1402_1.html}
}

@incollection{elmanLanguageDynamicalSystem1995,
  title = {Language as a Dynamical System},
  booktitle = {Mind as {{Motion}}: {{Explorations}} in the {{Dynamics}} of {{Cognition}}},
  author = {Elman, Jeffrey L},
  date = {1995},
  pages = {20},
  publisher = {{MIT Press}},
  isbn = {0-262-66110-1},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/elmanLanguageDynamicalSystem1995.pdf}
}

@article{elorantaKinkCellularAutomaton1992,
  title = {The Kink of Cellular Automaton Rule 18 Performs a Random Walk},
  author = {Eloranta, Kari and Nummelin, Esa},
  date = {1992-12},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {69},
  number = {5-6},
  pages = {1131--1136},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/BF01058766},
  url = {http://link.springer.com/10.1007/BF01058766},
  urldate = {2020-06-16},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/elorantaKinkCellularAutomaton1992.pdf}
}

@unpublished{elskenNeuralArchitectureSearch2019,
  title = {Neural {{Architecture Search}}: {{A Survey}}},
  shorttitle = {Neural {{Architecture Search}}},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  date = {2019-04-26},
  eprint = {1808.05377},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1808.05377},
  urldate = {2020-01-27},
  abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/JRSS7K7Z/elskenNeuralArchitectureSearch2019.pdf}
}

@article{etzioniOpenInformationExtraction2011,
  title = {Open {{Information Extraction}}: {{The Second Generation}}},
  author = {Etzioni, Oren and Fader, Anthony and Christensen, Janara and Soderland, Stephen},
  date = {2011},
  pages = {8},
  abstract = {How do we scale information extraction to the massive size and unprecedented heterogeneity of the Web corpus? Beginning in 2003, our KnowItAll project has sought to extract high-quality knowledge from the Web.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/etzioniOpenInformationExtraction22.pdf}
}

@inproceedings{faderIdentifyingRelationsOpen2011,
  title = {Identifying {{Relations}} for {{Open Information Extraction}}},
  booktitle = {Proceedings of the 2011 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Fader, Anthony and Soderland, Stephen and Etzioni, Oren},
  date = {2011-07},
  pages = {1535--1545},
  publisher = {{Association for Computational Linguistics}},
  location = {{Edinburgh, Scotland, UK.}},
  url = {http://www.aclweb.org/anthology/D11-1142},
  urldate = {2018-04-12},
  file = {/Users/hugo/Papers/pdf/faderIdentifyingRelationsOpen22.pdf}
}

@article{fangPatternRecognitionMaterials2016,
  title = {Pattern Recognition with “Materials That Compute”},
  author = {Fang, Yan and Yashin, Victor V. and Levitan, Steven P. and Balazs, Anna C.},
  date = {2016-09-02},
  journaltitle = {Science Advances},
  volume = {2},
  number = {9},
  pages = {e1601114},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.1601114},
  url = {https://www.science.org/doi/10.1126/sciadv.1601114},
  urldate = {2022-03-15},
  file = {/Users/hugo/Papers/pdf/fangPatternRecognitionMaterials.pdf}
}

@article{fangStochasticLargescaleMachine2019,
  title = {A {{Stochastic Large-scale Machine Learning Algorithm}} for {{Distributed Features}} and {{Observations}}},
  author = {Fang, Biyi and Klabjan, Diego},
  date = {2019},
  url = {https://arxiv.org/pdf/1803.11287.pdf},
  abstract = {As the size of modern datasets exceeds the disk and memory capacities of a single com-puter, machine learning practitioners have resorted to parallel and distributed comput-ing. Given that optimization is one of the pillars of machine learning and predictive modeling, distributed optimization methods have recently garnered ample attention, in particular when either observations or features are distributed, but not both. We propose a general stochastic algorithm where observations, features, and gradient com-ponents can be sampled in a double distributed setting, i.e., with both features and observations distributed. Very technical analyses establish convergence properties of the algorithm under different conditions on the learning rate (diminishing to zero or constant). Computational experiments in Spark demonstrate a superior performance of our algorithm versus a benchmark in early iterations of the algorithm, which is due to the stochastic components of the algorithm.},
  file = {/Users/hugo/Papers/pdf/fangStochasticLargescaleMachine2.pdf}
}

@unpublished{fanHierarchicalNeuralStory2018,
  title = {Hierarchical {{Neural Story Generation}}},
  author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  date = {2018-05-13},
  eprint = {1805.04833},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1805.04833},
  urldate = {2019-05-07},
  abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Zotero/storage/BQNLFN5I/fanHierarchicalNeuralStory2018.pdf}
}

@article{farahmandRandomProjectionFilter2017,
  title = {Random Projection Filter Bank for Time Series Data},
  author = {Farahmand, Amir-massoud and Pourazarm, Sepideh and Nikovski, Daniel},
  date = {2017},
  journaltitle = {Advances in neural information processing systems},
  volume = {30},
  file = {/Users/hugo/Papers/pdf/farahmandRandomProjectionFilter2017.pdf;/Users/hugo/Zotero/storage/6GF32NQN/ca3ec598002d2e7662e2ef4bdd58278b-Abstract.html}
}

@article{farberLinkedDataQuality2017,
  title = {Linked Data Quality of {{DBpedia}}, {{Freebase}}, {{OpenCyc}}, {{Wikidata}}, and {{YAGO}}},
  author = {Färber, Michael and Bartscherer, Frederic and Menne, Carsten and Rettinger, Achim},
  editor = {Zaveri, Amrapali and Kontokostas, Dimitris and Hellmann, Sebastian and Umbrich, Jürgen and Zaveri, Amrapali and Kontokostas, Dimitris and Hellmann, Sebastian and Umbrich, Jürgen},
  date = {2017-11-30},
  journaltitle = {Semantic Web},
  volume = {9},
  number = {1},
  pages = {77--129},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-170275},
  url = {http://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-170275},
  urldate = {2018-06-11},
  abstract = {In recent years, several noteworthy large, cross-domain and openly available knowledge graphs (KGs) have been created. These include DBpedia, Freebase, OpenCyc, Wikidata, and YAGO. Although extensively in use, these KGs have not been subject to an in-depth comparison so far. In this survey, we provide data quality criteria according to which KGs can be analyzed and analyze and compare the above mentioned KGs. Furthermore, we propose a framework for finding the most suitable KG for a given setting.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/QU2LBFNP/farberLinkedDataQuality2017.pdf}
}

@misc{fedusSwitchTransformersScaling2022,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  date = {2022-06-16},
  number = {arXiv:2101.03961},
  eprint = {2101.03961},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.03961},
  url = {http://arxiv.org/abs/2101.03961},
  urldate = {2022-07-27},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/fedusSwitchTransformersScaling2022.pdf;/Users/hugo/Zotero/storage/DGIQU8KM/2101.html}
}

@article{felleisenExpressivePowerProgramming1991,
  title = {On the Expressive Power of Programming Languages},
  author = {Felleisen, Matthias},
  date = {1991-12-01},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  volume = {17},
  number = {1},
  pages = {35--75},
  issn = {0167-6423},
  doi = {10.1016/0167-6423(91)90036-W},
  url = {http://www.sciencedirect.com/science/article/pii/016764239190036W},
  urldate = {2020-11-16},
  abstract = {The literature on programming languages contains an abundance of informal claims on the relative expressive power of programming languages, but there is no framework for formalizing such statements nor for deriving interesting consequences. As a first step in this direction, we develop a formal notion of expressiveness and investigate its properties. To validate the theory, we analyze some widely held beliefs about the expressive power of several extensions of functional languages. Based on these results, we believe that our system correctly captures many of the informal ideas on expressiveness, and that it constitutes a foundation for further research in this direction.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/felleisenExpressivePowerProgramming1991.pdf;/Users/hugo/Zotero/storage/GBTBJT2H/016764239190036W.html}
}

@inproceedings{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  date = {2017},
  volume = {70},
  pages = {10},
  location = {{Sydney, Australia}},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/NQ2BAQAE/Finn et al. - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf}
}

@article{flageatFastStableMAPElites2020,
  title = {Fast and Stable {{MAP-Elites}} in Noisy Domains Using Deep Grids},
  author = {Flageat, Manon and Cully, Antoine},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {273--282},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00316},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00316},
  urldate = {2020-07-27},
  abstract = {Quality-Diversity optimisation algorithms enable the evolution of collections of both high-performing and diverse solutions. These collections offer the possibility to quickly adapt and switch from one solution to another in case it is not working as expected. It therefore finds many applications in real-world domain problems such as robotic control. However, QD algorithms, like most optimisation algorithms, are very sensitive to uncertainty on the fitness function, but also on the behavioural descriptors. Yet, such uncertainties are frequent in real-world applications. Few works have explored this issue in the specific case of QD algorithms, and inspired by the literature in Evolutionary Computation, mainly focus on using sampling to approximate the ”true” value of the performances of a solution. However, sampling approaches require a high number of evaluations, which in many applications such as robotics, can quickly become impractical. In this work, we propose Deep-Grid MAP-Elites, a variant of the MAP-Elites algorithm that uses an archive of similar previously encountered solutions to approximate the performance of a solution. We compare our approach to previously explored ones on three noisy tasks: a standard optimisation task, the control of a redundant arm and a simulated Hexapod robot. The experimental results show that this simple approach is significantly more resilient to noise on the behavioural descriptors, while achieving competitive performances in terms of fitness optimisation, and being more sample-efficient than other existing approaches.},
  file = {/Users/hugo/Papers/pdf/flageatFastStableMAPElites2020.pdf;/Users/hugo/Zotero/storage/DF6DS9H4/isal_a_00316.html}
}

@article{flammEvolutionMetabolicNetworks2010,
  title = {Evolution of Metabolic Networks: A Computational Frame-Work},
  shorttitle = {Evolution of Metabolic Networks},
  author = {Flamm, Christoph and Ullrich, Alexander and Ekker, Heinz and Mann, Martin and Högerl, Daniel and Rohrschneider, Markus and Sauer, Sebastian and Scheuermann, Gerik and Klemm, Konstantin and Hofacker, Ivo L and Stadler, Peter F},
  date = {2010-12},
  journaltitle = {Journal of Systems Chemistry},
  shortjournal = {J Syst Chem},
  volume = {1},
  number = {1},
  pages = {4},
  issn = {1759-2208},
  doi = {10.1186/1759-2208-1-4},
  url = {https://jsystchem.springeropen.com/articles/10.1186/1759-2208-1-4},
  urldate = {2022-11-08},
  abstract = {Abstract                            Background               The metabolic architectures of extant organisms share many key pathways such as the citric acid cycle, glycolysis, or the biosynthesis of most amino acids. Several competing hypotheses for the evolutionary mechanisms that shape metabolic networks have been discussed in the literature, each of which finds support from comparative analysis of extant genomes. Alternatively, the principles of metabolic evolution can be studied by direct computer simulation. This requires, however, an explicit implementation of all pertinent components: a universe of chemical reactions upon which the metabolism is built, an explicit representation of the enzymes that implement the metabolism, a genetic system that encodes these enzymes, and a fitness function that can be selected for.                                         Results                                We describe here a simulation environment that implements all these components in a simplified way so that large-scale evolutionary studies are feasible. We employ an artificial chemistry that views chemical reactions as graph rewriting operations and utilizes a toy-version of quantum chemistry to derive thermodynamic parameters. Minimalist organisms with simple string-encoded genomes produce model ribozymes whose catalytic activity is determined by an                 ad hoc                 mapping between their secondary structure and the transition state graphs that they stabilize. Fitness is computed utilizing the ideas of metabolic flux analysis. We present an implementation of the complete system and first simulation results.                                                        Conclusions               The simulation system presented here allows coherent investigations into the evolutionary mechanisms of the first steps of metabolic evolution using a self-consistent toy universe.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/flammEvolutionMetabolicNetworks2010.pdf}
}

@unpublished{flennerhagBootstrappedMetaLearning2021,
  title = {Bootstrapped {{Meta-Learning}}},
  author = {Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and van Hasselt, Hado and Silver, David and Singh, Satinder},
  options = {useprefix=true},
  date = {2021-09-09},
  eprint = {2109.04504},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2109.04504},
  urldate = {2021-09-13},
  abstract = {Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem that often exhibits ill-conditioning, and myopic metaobjectives. We propose an algorithm that tackles these issues by letting the metalearner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the improvement is related to the target distance. Thus, by controlling curvature, the distance measure can be used to ease meta-optimization, for instance by reducing ill-conditioning. Further, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. The algorithm is versatile and easy to implement. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark, improve upon MAML in few-shot learning, and demonstrate how our approach opens up new possibilities by meta-learning efficient exploration in an ε-greedy Q-learning agent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/flennerhagBootstrappedMetaLearning2021.pdf}
}

@article{floreanoNeuroevolutionArchitecturesLearning2008,
  title = {Neuroevolution: From Architectures to Learning},
  shorttitle = {Neuroevolution},
  author = {Floreano, Dario and Dürr, Peter and Mattiussi, Claudio},
  date = {2008-03-01},
  journaltitle = {Evolutionary Intelligence},
  shortjournal = {Evol. Intel.},
  volume = {1},
  number = {1},
  pages = {47--62},
  issn = {1864-5917},
  doi = {10.1007/s12065-007-0002-4},
  url = {https://doi.org/10.1007/s12065-007-0002-4},
  urldate = {2019-12-12},
  abstract = {Artificial neural networks (ANNs) are applied to many real-world problems, ranging from pattern classification to robot control. In order to design a neural network for a particular task, the choice of an architecture (including the choice of a neuron model), and the choice of a learning algorithm have to be addressed. Evolutionary search methods can provide an automatic solution to these problems. New insights in both neuroscience and evolutionary biology have led to the development of increasingly powerful neuroevolution techniques over the last decade. This paper gives an overview of the most prominent methods for evolving ANNs with a special focus on recent advances in the synthesis of learning architectures.},
  langid = {english},
  keywords = {Evolution,Learning,Neural networks},
  file = {/Users/hugo/Zotero/storage/C4FKNCCR/floreanoNeuroevolutionArchitecturesLearning2008.pdf}
}

@article{floydAdaptiveAlgorithmSpatial1976,
  title = {An Adaptive Algorithm for  Spatial  Gray  Scale},
  author = {Floyd, Robert W. and Steinberg, L. S.},
  date = {1976},
  journaltitle = {Journal of the Society for Information Display},
  volume = {17},
  pages = {75--77}
}

@book{fogelArtificialIntelligenceSimulated1966,
  title = {Artificial Intelligence through Simulated Evolution},
  author = {Fogel, Lawrence J. and Owens, Alvin J. and Walsh, Michael J.},
  date = {1966},
  series = {Artificial Intelligence through Simulated Evolution},
  pages = {xii, 170},
  publisher = {{John Wiley \& Sons}},
  location = {{Oxford, England}},
  pagetotal = {xii, 170},
  file = {/Users/hugo/Zotero/storage/RZRJX25G/1966-12317-000.html}
}

@article{frankAlgorithmQuadraticProgramming1956,
  title = {An Algorithm for Quadratic Programming},
  author = {Frank, Marguerite and Wolfe, Philip},
  date = {1956},
  journaltitle = {Naval Research Logistics Quarterly},
  volume = {3},
  number = {1-2},
  pages = {95--110},
  issn = {1931-9193},
  doi = {10.1002/nav.3800030109},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109},
  urldate = {2022-08-30},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800030109},
  file = {/Users/hugo/Zotero/storage/75QUZ5VI/nav.html}
}

@unpublished{frankleLotteryTicketHypothesis2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2018-03-09},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.03635},
  urldate = {2019-05-07},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Zotero/storage/E42NUPKK/frankleLotteryTicketHypothesis2018.pdf;/Users/hugo/Zotero/storage/HH3LY4ZQ/1803.html}
}

@article{franklinScalingMultidimensionalMatrices1989,
  title = {On the Scaling of Multidimensional Matrices},
  author = {Franklin, Joel and Lorenz, Jens},
  date = {1989-03-01},
  journaltitle = {Linear Algebra and its Applications},
  volume = {114--115},
  pages = {717--735},
  issn = {0024-3795},
  doi = {10.1016/0024-3795(89)90490-4},
  url = {https://www.sciencedirect.com/science/article/pii/0024379589904904},
  urldate = {2018-12-23},
  abstract = {Elementary proofs are given for theorems of Bapat and Raghavan on the scaling of nonnegative multidimensional matrices. Theorems of Sinkhorn and of Br…},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/I4WYPUSU/0024379589904904.html}
}

@unpublished{fransPopulationBasedEvolutionOptimizes2021,
  title = {Population-{{Based Evolution Optimizes}} a {{Meta-Learning Objective}}},
  author = {Frans, Kevin and Witkowski, Olaf},
  date = {2021-03-10},
  eprint = {2103.06435},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.06435},
  urldate = {2021-04-20},
  abstract = {Meta-learning models, or models that learn to learn, have been a long-desired target for their ability to quickly solve new tasks. Traditional meta-learning methods can require expensive inner and outer loops, thus there is demand for algorithms that discover strong learners without explicitly searching for them. We draw parallels to the study of evolvable genomes in evolutionary systems -- genomes with a strong capacity to adapt -- and propose that meta-learning and adaptive evolvability optimize for the same objective: high performance after a set of learning iterations. We argue that population-based evolutionary systems with non-static fitness landscapes naturally bias towards high-evolvability genomes, and therefore optimize for populations with strong learning ability. We demonstrate this claim with a simple evolutionary algorithm, Population-Based Meta Learning (PBML), that consistently discovers genomes which display higher rates of improvement over generations, and can rapidly adapt to solve sparse fitness and robotic control tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Zotero/storage/CD939L89/Frans and Witkowski - 2021 - Population-Based Evolution Optimizes a Meta-Learni.pdf}
}

@article{frenchCatastrophicForgettingConnectionist1999,
  title = {Catastrophic Forgetting in Connectionist Networks},
  author = {French, Robert M.},
  date = {1999-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {4},
  pages = {128--135},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(99)01294-2},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661399012942},
  urldate = {2021-10-18},
  abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget ‘catastrophically’. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
  langid = {english},
  keywords = {Catastrophic forgetting,Connectionism,Connectionist networks,Interference,Learning,Memory},
  file = {/Users/hugo/Papers/pdf/frenchCatastrophicForgettingConnectionist1999.pdf;/Users/hugo/Zotero/storage/HA47FS8K/S1364661399012942.html}
}

@article{frognerLearningWassersteinLoss,
  title = {Learning with a {{Wasserstein Loss}}},
  author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya, Mauricio and Poggio, Tomaso A},
  pages = {9},
  abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn’t use the metric.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/frognerLearningWassersteinLoss2.pdf}
}

@inproceedings{fullmerUsingMarkerBasedGenetic1991,
  title = {Using {{Marker-Based Genetic Encoding Of Neural Networks To Evolve Finite-State Behaviour}}},
  booktitle = {In {{Proceedings}} of the First {{European Conference}} on {{Artificial Life}} ({{ECAL-91}})},
  author = {Fullmer, Brad and Miikkulainen, Risto},
  date = {1991},
  pages = {255--262},
  publisher = {{MIT Press}},
  abstract = {A new mechanism for genetic encoding of neural networks is proposed, which is loosely based on the marker structure of biological DNA. The mechanism allows all aspects of the network structure, including the number of nodes and their connectivity, to be evolved through genetic algorithms. The effectiveness of the encoding scheme is demonstrated in an object recognition task that requires artificial creatures (whose behaviour is driven by a neural network) to develop high-level finite-state exploration and discrimination strategies. The task requires solving the sensory-motor grounding problem, i.e. developing a functional understanding of the effects that a creature 's movement has on its sensory input.  1 Introduction  The behaviour of a particular biological organism is driven by its neural circuitry. In modeling artificial life forms it is therefore natural to represent the organism as an artificial neural network (ANN) with a set of sensory inputs and motor outputs. ANNs have been ...},
  file = {/Users/hugo/Zotero/storage/AEPNHFC7/fullmerUsingMarkerBasedGenetic1991.pdf;/Users/hugo/Zotero/storage/DJL8THZS/summary.html}
}

@article{furusawaOriginComplexityMulticellular2000,
  title = {Origin of Complexity in Multicellular Organisms},
  author = {Furusawa, C. and Kaneko, K.},
  date = {2000-06-26},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {84},
  eprint = {10991141},
  eprinttype = {pmid},
  pages = {6130--6133},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.84.6130},
  abstract = {Through extensive studies of dynamical system modeling cellular growth and reproduction, we find evidence that complexity arises in multicellular organisms naturally through evolution. Without any elaborate control mechanism, these systems can exhibit complex pattern formation with spontaneous cell differentiation. Such systems employ a "cooperative" use of resources and maintain a larger growth speed than simple cell systems, which exist in a homogeneous state and behave "selfishly." The relevance of the diversity of chemicals and reaction dynamics to the growth of a multicellular organism is demonstrated. Chaotic biochemical dynamics are found to provide the multipotency of stem cells.},
  issue = {26 Pt 1},
  langid = {english},
  keywords = {Cell Communication,Cell Differentiation,Cell Division,Cell Physiological Phenomena,Models; Biological,Nonlinear Dynamics,Stem Cells},
  file = {/Users/hugo/Papers/pdf/furusawaOriginComplexityMulticellular22.pdf}
}

@book{gabbayLogicRussellChurch2009,
  title = {Logic from {{Russell}} to {{Church}}},
  editor = {Gabbay, Dov M. and Woods, John},
  date = {2009},
  series = {Handbook of the {{History}} of {{Logic}}},
  volume = {5},
  publisher = {{Elsevier}},
  isbn = {978-0-444-51620-6}
}

@article{gacsReliableCellularAutomata2001,
  title = {Reliable {{Cellular Automata}} with {{Self-Organization}}},
  author = {Gács, Peter},
  date = {2001-04-01},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {Journal of Statistical Physics},
  volume = {103},
  number = {1},
  pages = {45--267},
  issn = {1572-9613},
  doi = {10.1023/A:1004823720305},
  url = {https://doi.org/10.1023/A:1004823720305},
  urldate = {2022-11-10},
  abstract = {In a probabilistic cellular automaton in which all local transitions have positive probability, the problem of keeping a bit of information indefinitely is nontrivial, even in an infinite automaton. Still, there is a solution in 2 dimensions, and this solution can be used to construct a simple 3-dimensional discrete-time universal fault-tolerant cellular automaton. This technique does not help much to solve the following problems: remembering a bit of information in 1 dimension; computing in dimensions lower than 3; computing in any dimension with non-synchronized transitions. Our more complex technique organizes the cells in blocks that perform a reliable simulation of a second (generalized) cellular automaton. The cells of the latter automaton are also organized in blocks, simulating even more reliably a third automaton, etc. Since all this (a possibly infinite hierarchy) is organized in “software,” it must be under repair all the time from damage caused by errors. A large part of the problem is essentially self-stabilization recovering from a mess of arbitrary size and content. The present paper constructs an asynchronous one-dimensional fault-tolerant cellular automaton, with the further feature of “self-organization.” The latter means that unless a large amount of input information must be given, the initial configuration can be chosen homogeneous.},
  langid = {english},
  keywords = {ergodicity,error-correction,fault-tolerance,hierarchy,interacting particle systems,probabilistic cellular automata,reliability,renormalization,self-organization,simulation},
  file = {/Users/hugo/Papers/pdf/gacsReliableCellularAutomata2001.pdf}
}

@article{gacsReliableComputationCellular1986,
  title = {Reliable Computation with Cellular Automata},
  author = {Gács, Peter},
  date = {1986},
  journaltitle = {Journal of Computer and System Sciences},
  volume = {32},
  number = {1},
  pages = {15--78},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Zotero/storage/8NHFFVRZ/0022000086900024.html}
}

@inproceedings{galarragaAMIEAssociationRule2013,
  title = {{{AMIE}}: Association Rule Mining under Incomplete Evidence in Ontological Knowledge Bases},
  shorttitle = {{{AMIE}}},
  author = {Galárraga, Luis Antonio and Teflioudi, Christina and Hose, Katja and Suchanek, Fabian},
  date = {2013},
  pages = {413--422},
  publisher = {{ACM Press}},
  doi = {10.1145/2488388.2488425},
  url = {http://dl.acm.org/citation.cfm?doid=2488388.2488425},
  urldate = {2018-06-12},
  abstract = {Recent advances in information extraction have led to huge knowledge bases (KBs), which capture knowledge in a machine-readable format. Inductive Logic Programming (ILP) can be used to mine logical rules from the KB. These rules can help deduce and add missing knowledge to the KB. While ILP is a mature field, mining logical rules from KBs is different in two aspects: First, current rule mining systems are easily overwhelmed by the amount of data (state-of-the art systems cannot even run on today’s KBs). Second, ILP usually requires counterexamples. KBs, however, implement the open world assumption (OWA), meaning that absent data cannot be used as counterexamples. In this paper, we develop a rule mining model that is explicitly tailored to support the OWA scenario. It is inspired by association rule mining and introduces a novel measure for confidence. Our extensive experiments show that our approach outperforms state-of-the-art approaches in terms of precision and coverage. Furthermore, our system, AMIE, mines rules orders of magnitude faster than state-of-the-art approaches.},
  isbn = {978-1-4503-2035-1},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/U45DPC8N/galarragaAMIEAssociationRule2013.pdf}
}

@book{galichonOptimalTransportMethods2018,
  title = {Optimal {{Transport Methods In Economics}}.},
  author = {Galichon, Alfred},
  date = {2018},
  publisher = {{Princeton University Press}},
  location = {{S.l.}},
  isbn = {978-0-691-18346-6},
  langid = {english},
  annotation = {OCLC: 1028167041}
}

@article{gallicchioFrontiersReservoirComputing2020,
  title = {Frontiers in {{Reservoir Computing}}},
  author = {Gallicchio, Claudio and Lukoševičius, Mantas and Scardapane, Simone},
  date = {2020},
  journaltitle = {Computational Intelligence},
  pages = {8},
  abstract = {Reservoir computing (RC) studies the properties of large recurrent networks of artificial neurons, with either fixed or random connectivity. Over the last years, reservoirs have become a key tool for pattern recognition and neuroscience problems, being able to develop a rich representation of the temporal information even if left untrained. The common paradigm has been instantiated into several models, among which the Echo State Network and the Liquid State Machine represent the most widely known ones. Nowadays, RC represents the de facto state-of-the-art approach for efficient learning in the temporal domain. Besides, theoretical studies in RC area can contribute to the broader field of Recurrent Neural Networks research by enabling a deeper understanding of the fundamental capabilities of dynamical recurrent models, even in the absence of training of the recurrent connections. RC paradigm also allows using different dynamical systems, including hardware, for computation.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/ESEJ7FNJ/Gallicchio et al. - 2020 - Frontiers in Reservoir Computing.pdf}
}

@article{gandin3DCellularAutomaton1997,
  title = {A {{3D}} Cellular Automaton Algorithm for the Prediction of Dendritic Grain Growth},
  author = {Gandin, Ch-A. and Rappaz, Michel},
  date = {1997},
  journaltitle = {Acta Materialia},
  volume = {45},
  number = {5},
  pages = {2187--2195},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Zotero/storage/SQHHBCMN/S1359645496003035.html}
}

@misc{gaoBlackboxGenerationAdversarial2018,
  title = {Black-Box {{Generation}} of {{Adversarial Text Sequences}} to {{Evade Deep Learning Classifiers}}},
  author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  date = {2018-05-23},
  number = {arXiv:1801.04354},
  eprint = {1801.04354},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1801.04354},
  url = {http://arxiv.org/abs/1801.04354},
  urldate = {2022-08-03},
  abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to black-box attacks, which are more realistic scenarios. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We employ novel scoring strategies to identify the critical tokens that, if modified, cause the classifier to make an incorrect prediction. Simple character-level transformations are applied to the highest-ranked tokens in order to minimize the edit distance of the perturbation, yet change the original classification. We evaluated DeepWordBug on eight real-world text datasets, including text classification, sentiment analysis, and spam detection. We compare the result of DeepWordBug with two baselines: Random (Black-box) and Gradient (White-box). Our experimental results indicate that DeepWordBug reduces the prediction accuracy of current state-of-the-art deep-learning models, including a decrease of 68\textbackslash\% on average for a Word-LSTM model and 48\textbackslash\% on average for a Char-CNN model.},
  archiveprefix = {arXiv},
  version = {5},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/gaoBlackboxGenerationAdversarial2018.pdf;/Users/hugo/Zotero/storage/SHSHAK5X/1801.html}
}

@incollection{garciaKernelHopfieldMemory2004,
  title = {The {{Kernel Hopfield Memory Network}}},
  booktitle = {Cellular {{Automata}}},
  author = {García, Cristina and Moreno, José Alí},
  editor = {Sloot, Peter M. A. and Chopard, Bastien and Hoekstra, Alfons G.},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3305},
  pages = {755--764},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30479-1_78},
  url = {http://link.springer.com/10.1007/978-3-540-30479-1_78},
  urldate = {2020-09-01},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-540-23596-5 978-3-540-30479-1}
}

@book{gardnerAhaAhaInsight1978,
  title = {Aha! {{Aha}}! Insight},
  author = {Gardner, Martin},
  date = {1978},
  publisher = {{Scientific American}},
  location = {{New York}},
  isbn = {978-0-89454-001-1},
  pagetotal = {179},
  keywords = {Mathematical recreations,Problem solving,Puzzles}
}

@article{gardnerMathematicalGames1970,
  title = {Mathematical {{Games}}},
  author = {Gardner, Martin},
  date = {1970-10},
  journaltitle = {Scientific American},
  volume = {223},
  number = {4},
  pages = {120--123},
  issn = {0036-8733},
  doi = {10.1038/scientificamerican1070-120},
  url = {http://www.nature.com/doifinder/10.1038/scientificamerican1070-120},
  urldate = {2019-12-02},
  file = {/Users/hugo/Papers/pdf/gardnerMathematicalGames1970.pdf}
}

@inproceedings{gargBAEBERTbasedAdversarial2020,
  title = {{{BAE}}: {{BERT-based Adversarial Examples}} for {{Text Classification}}},
  shorttitle = {{{BAE}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Garg, Siddhant and Ramakrishnan, Goutham},
  date = {2020},
  eprint = {2004.01970},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {6174--6181},
  doi = {10.18653/v1/2020.emnlp-main.498},
  url = {http://arxiv.org/abs/2004.01970},
  urldate = {2022-08-03},
  abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/gargBAEBERTbasedAdversarial2020.pdf;/Users/hugo/Zotero/storage/9LSXRBFV/2004.html}
}

@book{garzonModelsMassiveParallelism2012,
  title = {Models of {{Massive Parallelism}}: {{Analysis}} of {{Cellular Automata}} and {{Neural Networks}}},
  shorttitle = {Models of {{Massive Parallelism}}},
  author = {Garzon, Max},
  date = {2012-12-06},
  eprint = {e8OqCAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Locality is a fundamental restriction in nature. On the other hand, adaptive complex systems, life in particular, exhibit a sense of permanence and time lessness amidst relentless constant changes in surrounding environments that make the global properties of the physical world the most important problems in understanding their nature and structure. Thus, much of the differential and integral Calculus deals with the problem of passing from local information (as expressed, for example, by a differential equation, or the contour of a region) to global features of a system's behavior (an equation of growth, or an area). Fundamental laws in the exact sciences seek to express the observable global behavior of physical objects through equations about local interaction of their components, on the assumption that the continuum is the most accurate model of physical reality. Paradoxically, much of modern physics calls for a fundamen tal discrete component in our understanding of the physical world. Useful computational models must be eventually constructed in hardware, and as such can only be based on local interaction of simple processing elements.},
  isbn = {978-3-642-77905-3},
  langid = {english},
  pagetotal = {284},
  keywords = {Computers / Computer Science,Computers / Information Technology,Computers / Intelligence (AI) & Semantics,Computers / Machine Theory,Science / Physics / General,Science / Physics / Mathematical & Computational},
  file = {/Users/hugo/Papers/pdf/garzonModelsMassiveParallelism2012.pdf}
}

@incollection{garzonRealComputationCellular1993,
  title = {Real Computation with Cellular Automata},
  booktitle = {Cellular {{Automata}} and {{Cooperative Systems}}},
  author = {Garzon, Max and Botelho, Fernanda},
  date = {1993},
  pages = {191--202},
  publisher = {{Springer}},
  file = {/Users/hugo/Zotero/storage/2UG7EZEU/978-94-011-1691-6_17.html}
}

@inproceedings{gatysImageStyleTransfer2016,
  title = {Image {{Style Transfer Using Convolutional Neural Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2016-06},
  pages = {2414--2423},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.265},
  url = {http://ieeexplore.ieee.org/document/7780634/},
  urldate = {2018-12-02},
  abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/gatysImageStyleTransfer22.pdf}
}

@article{gauthierNextGenerationReservoir2021,
  title = {Next Generation Reservoir Computing},
  author = {Gauthier, Daniel J. and Bollt, Erik and Griffith, Aaron and Barbosa, Wendson A. S.},
  date = {2021-09-21},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {5564},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25801-2},
  url = {https://www.nature.com/articles/s41467-021-25801-2},
  urldate = {2021-09-22},
  abstract = {Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.},
  issue = {1},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational science;Electrical and electronic engineering Subject\_term\_id: computational-science;electrical-and-electronic-engineering},
  file = {/Users/hugo/Papers/pdf/gauthierNextGenerationReservoir2021.pdf;/Users/hugo/Zotero/storage/9P47T73K/s41467-021-25801-2.html}
}

@misc{geisaTheoryOutofdistributionLearning2022,
  title = {Towards a Theory of Out-of-Distribution Learning},
  author = {Geisa, Ali and Mehta, Ronak and Helm, Hayden S. and Dey, Jayanta and Eaton, Eric and Dick, Jeffery and Priebe, Carey E. and Vogelstein, Joshua T.},
  date = {2022-01-06},
  number = {arXiv:2109.14501},
  eprint = {2109.14501},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.14501},
  url = {http://arxiv.org/abs/2109.14501},
  urldate = {2022-08-24},
  abstract = {What is learning? 20\$\^\{st\}\$ century formalizations of learning theory -- which precipitated revolutions in artificial intelligence -- focus primarily on \$\textbackslash mathit\{in-distribution\}\$ learning, that is, learning under the assumption that the training data are sampled from the same distribution as the evaluation distribution. This assumption renders these theories inadequate for characterizing 21\$\^\{st\}\$ century real world data problems, which are typically characterized by evaluation distributions that differ from the training data distributions (referred to as out-of-distribution learning). We therefore make a small change to existing formal definitions of learnability by relaxing that assumption. We then introduce \$\textbackslash mathbf\{learning\textbackslash{} efficiency\}\$ (LE) to quantify the amount a learner is able to leverage data for a given problem, regardless of whether it is an in- or out-of-distribution problem. We then define and prove the relationship between generalized notions of learnability, and show how this framework is sufficiently general to characterize transfer, multitask, meta, continual, and lifelong learning. We hope this unification helps bridge the gap between empirical practice and theoretical guidance in real world problems. Finally, because biological learning continues to outperform machine learning algorithms on certain OOD challenges, we discuss the limitations of this framework vis-\textbackslash 'a-vis its ability to formalize biological learning, suggesting multiple avenues for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/geisaTheoryOutofdistributionLearning2022.pdf;/Users/hugo/Zotero/storage/EE8YVBDP/2109.html}
}

@book{gell-mannQuarkJaguarAdventures1995,
  title = {The {{Quark}} and the {{Jaguar}}: {{Adventures}} in the {{Simple}} and the {{Complex}}},
  shorttitle = {The {{Quark}} and the {{Jaguar}}},
  author = {Gell-Mann, Murray},
  date = {1995-09-15},
  eprint = {l6aCe4zqZ_sC},
  eprinttype = {googlebooks},
  publisher = {{St. Martin's Press}},
  abstract = {From one of the architects of the new science of simplicity and complexity comes a highly personal, unifying vision of the natural world. As a theoretical physicist, Murray Gell-Mann has explored nature at its most fundamental level. His achievements include the 1969 Nobel Prize for work leading up to his discovery of the quark - the basic building block of all atomic nuclei throughout the universe. But Gell-Mann is a man of many intellectual passions, with lifelong interests in fields that seek to understand existence at its most complex: natural history, biological evolution, the history of language, and the study of creative thinking. These seemingly disparate pursuits come together in Gell-Mann's current work at the Santa Fe Institute, where scientists are investigating the similarities and differences among complex adaptive systems - systems that learn or evolve by utilizing acquired information. They include a child learning his or her native language, a strain of bacteria becoming resistant to an antibiotic, the scientific community testing new theories, or an artist implementing a creative idea. The Quark and the Jaguar is Gell-Mann's own story of finding the connections between the basic laws of physics and the complexity and diversity of the natural world. The simple: a quark inside an atom. The complex: a jaguar prowling its jungle territory in the night. Exploring the relationship between them becomes a series of exciting intellectual adventures.},
  isbn = {978-0-8050-7253-2},
  langid = {english},
  pagetotal = {414},
  keywords = {Biography & Autobiography / General,Biography & Autobiography / Science & Technology,Science / Physics / General,Science / Physics / Nuclear,Science / Physics / Quantum Theory}
}

@article{gemanNeuralNetworksBias1992,
  title = {Neural {{Networks}} and the {{Bias}}/{{Variance Dilemma}}},
  author = {Geman, Stuart and Bienenstock, Elie and Doursat, René},
  date = {1992},
  journaltitle = {Neural Comput.},
  volume = {4},
  number = {1},
  pages = {1--58},
  doi = {10.1162/neco.1992.4.1.1}
}

@inproceedings{genevayStochasticOptimizationLargescale2016,
  title = {Stochastic {{Optimization}} for {{Large-scale Optimal Transport}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Genevay, Aude and Cuturi, Marco and Peyré, Gabriel and Bach, Francis},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {3440--3448},
  url = {http://papers.nips.cc/paper/6566-stochastic-optimization-for-large-scale-optimal-transport.pdf},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/Users/hugo/Papers/pdf/genevayStochasticOptimizationLargescale22.pdf;/Users/hugo/Zotero/storage/ZL3QYCTV/1605.html}
}

@article{gepperthBioInspiredIncrementalLearning2016,
  title = {A {{Bio-Inspired Incremental Learning Architecture}} for {{Applied Perceptual Problems}}},
  author = {Gepperth, Alexander and Karaoguz, Cem},
  date = {2016-10-01},
  journaltitle = {Cognitive Computation},
  shortjournal = {Cogn Comput},
  volume = {8},
  number = {5},
  pages = {924--934},
  issn = {1866-9964},
  doi = {10.1007/s12559-016-9389-5},
  url = {https://doi.org/10.1007/s12559-016-9389-5},
  urldate = {2022-02-28},
  abstract = {We present a biologically inspired architecture for incremental learning that remains resource-efficient even in the face of very high data dimensionalities ({$>$}1000) that are typically associated with perceptual problems. In particular, we investigate how a new perceptual (object) class can be added to a trained architecture without retraining, while avoiding the well-known catastrophic forgetting effects typically associated with such scenarios. At the heart of the presented architecture lies a generative description of the perceptual space by a self-organized approach which at the same time approximates the neighborhood relations in this space on a two-dimensional plane. This approximation, which closely imitates the topographic organization of the visual cortex, allows an efficient local update rule for incremental learning even in the face of very high dimensionalities, which we demonstrate by tests on the well-known MNIST benchmark. We complement the model by adding a biologically plausible short-term memory system, allowing it to retain excellent classification accuracy even under incremental learning in progress. The short-term memory is additionally used to reinforce new data statistics by replaying previously stored samples during dedicated “sleep” phases.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/gepperthBioInspiredIncrementalLearning2016.pdf}
}

@unpublished{gershensonEmergenceArtificialLife2021,
  title = {Emergence in Artificial Life},
  author = {Gershenson, Carlos},
  date = {2021-04-30},
  eprint = {2105.03216},
  eprinttype = {arxiv},
  primaryclass = {physics},
  url = {http://arxiv.org/abs/2105.03216},
  urldate = {2021-05-14},
  abstract = {Concepts similar to emergence have been used since antiquity, but we lack an agreed definition of emergence. Still, emergence has been identified as one of the features of complex systems. Most would agree on the statement “life is complex”. Thus, understanding emergence and complexity should benefit the study of living systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Physics - General Physics},
  file = {/Users/hugo/Zotero/storage/6MVV2CPN/Gershenson - 2021 - Emergence in artificial life.pdf}
}

@article{gershensonSelfOrganizationArtificialLife2020,
  title = {Self-{{Organization}} and {{Artificial Life}}},
  author = {Gershenson, Carlos and Trianni, Vito and Werfel, Justin and Sayama, Hiroki},
  date = {2020-09},
  journaltitle = {Artificial Life},
  shortjournal = {Artificial Life},
  volume = {26},
  number = {3},
  pages = {391--408},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/artl_a_00324},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/artl_a_00324},
  urldate = {2020-09-23},
  abstract = {Self-organization can be broadly defined as the ability of a system to display ordered spatiotemporal patterns solely as the result of the interactions among the system components. Processes of this kind characterize both living and artificial systems, making self-organization a concept that is at the basis of several disciplines, from physics to biology and engineering. Placed at the frontiers between disciplines, artificial life (ALife) has heavily borrowed concepts and tools from the study of self-organization, providing mechanistic interpretations of lifelike phenomena as well as useful constructivist approaches to artificial system design. Despite its broad usage within ALife, the concept of self-organization has been often excessively stretched or misinterpreted, calling for a clarification that could help with tracing the borders between what can and cannot be considered self-organization. In this review, we discuss the fundamental aspects of self-organization and list the main usages within three primary ALife domains, namely “soft” (mathematical/computational modeling), “hard” (physical robots), and “wet” (chemical/biological systems) ALife. We also provide a classification to locate this research. Finally, we discuss the usefulness of self-organization and related concepts within ALife studies, point to perspectives and challenges for future research, and list open questions. We hope that this work will motivate discussions related to self-organization in ALife and related fields.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/gershensonSelfOrganizationArtificialLife2020.pdf}
}

@unpublished{gilpinCellularAutomataConvolutional2018,
  title = {Cellular Automata as Convolutional Neural Networks},
  author = {Gilpin, William},
  date = {2018-09-09},
  eprint = {1809.02942},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:nlin, physics:physics},
  url = {http://arxiv.org/abs/1809.02942},
  urldate = {2019-06-05},
  abstract = {Deep learning techniques have recently demonstrated broad success in predicting complex dynamical systems ranging from turbulence to human speech, motivating broader questions about how neural networks encode and represent dynamical rules. We explore this problem in the context of cellular automata (CA), simple dynamical systems that are intrinsically discrete and thus difficult to analyze using standard tools from dynamical systems theory. We show that any CA may readily be represented using a convolutional neural network with a network-in-network architecture. This motivates our development of a general convolutional multilayer perceptron architecture, which we find can learn the dynamical rules for arbitrary CA when given videos of the CA as training data. In the limit of large network widths, we find that training dynamics are strongly stereotyped across replicates, and that common patterns emerge in the structure of networks trained on different CA rulesets. We train ensembles of networks on randomly-sampled CA, and we probe how the trained networks internally represent the CA rules using an information-theoretic technique based on distributions of layer activation patterns. We find that CA with simpler rule tables produce trained networks with hierarchical structure and layer specialization, while more complex CA tend to produce shallower representations---illustrating how the underlying complexity of the CA's rules influences the specificity of these internal representations. Our results suggest how the entropy of a physical process can affect its representation when learned by neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Cellular Automata and Lattice Gases,Physics - Computational Physics},
  file = {/Users/hugo/Papers/pdf/gilpinCellularAutomataConvolutional22.pdf}
}

@inproceedings{gloverDynamicalLandscapeReservoir2021,
  title = {The {{Dynamical Landscape}} of {{Reservoir Computing}} with {{Elementary Cellular Automata}}},
  booktitle = {{{ALIFE}} 2021: {{The}} 2021 {{Conference}} on {{Artificial Life}}},
  author = {Glover, Tom Eivind and Lind, Pedro and Yazidi, Anis and Osipov, Evgeny and Nichele, Stefano},
  date = {2021},
  publisher = {{MIT Press}}
}

@incollection{goldbergSimpleGeneticAlgorithms1987,
  title = {Simple Genetic Algorithms and the Minimal Deceptive Problem},
  booktitle = {Genetic {{Algorithms}} and {{Simulated Annealing}}},
  author = {Goldberg, David E.},
  date = {1987-01-01},
  edition = {1},
  pages = {74--88},
  publisher = {{Pitman}},
  location = {{Los Altos, CA}},
  isbn = {978-0-273-08771-7}
}

@article{goldenbergTalkNetworkComplex2001,
  title = {Talk of the {{Network}}: {{A Complex Systems Look}} at the {{Underlying Process}} of {{Word-of-Mouth}}},
  shorttitle = {Talk of the {{Network}}},
  author = {Goldenberg, Jacob and Libai, Barak and Muller, Eitan},
  date = {2001-08-01},
  journaltitle = {Marketing Letters},
  shortjournal = {Marketing Letters},
  volume = {12},
  number = {3},
  pages = {211--223},
  issn = {1573-059X},
  doi = {10.1023/A:1011122126881},
  url = {https://doi.org/10.1023/A:1011122126881},
  urldate = {2018-12-29},
  abstract = {Though word-of-mouth (w-o-m) communications is a pervasive and intriguing phenomenon, little is known on its underlying process of personal communications. Moreover as marketers are getting more interested in harnessing the power of w-o-m, for e-business and other net related activities, the effects of the different communications types on macro level marketing is becoming critical. In particular we are interested in the breakdown of the personal communication between closer and stronger communications that are within an individual's own personal group (strong ties) and weaker and less personal communications that an individual makes with a wide set of other acquaintances and colleagues (weak ties).We use a technique borrowed from Complex Systems Analysis called stochastic cellular automata in order to generate data and analyze the results so that answers to our main research issues could be ascertained. The following summarizes the impact of strong and weak ties on the speed of acceptance of a new product:••The influence of weak ties is at least as strong as the influence of strong ties. Despite the relative inferiority of the weak tie parameter in the model's assumptions, their effect approximates or exceeds that of strong ties, in all stages of the product life cycle.••External marketing efforts (e.g., advertising) are effective. However, beyond a relatively early stage of the growth cycle of the new product, their efficacy quickly diminishes and strong and weak ties become the main forces propelling growth. The results clearly indicate that information dissemination is dominated by both weak and strong w-o-m, rather than by advertising.••The effect of strong ties diminishes as personal network size decreases. Market attributes were also found to mediate the effects of weak and strong ties. When personal networks are small, weak ties were found to have a stronger impact on information dissemination than strong ties.},
  langid = {english},
  keywords = {cellular automata,complex systems,social networks,word-of-mouth},
  file = {/Users/hugo/Papers/pdf/goldenbergTalkNetworkComplex22.pdf}
}

@unpublished{goldfeldEstimatingInformationFlow2018,
  title = {Estimating {{Information Flow}} in {{Neural Networks}}},
  author = {Goldfeld, Ziv and van den Berg, Ewout and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
  date = {2018-10-12},
  eprint = {1810.05728},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.05728},
  urldate = {2019-04-22},
  abstract = {We study the flow of information and the evolution of internal representations during deep neural network (DNN) training, aiming to demystify the compression aspect of the information bottleneck theory. The theory suggests that DNN training comprises a rapid fitting phase followed by a slower compression phase, in which the mutual information I(X; T ) between the input X and internal representations T decreases. Several papers observe compression of estimated mutual information on different DNN models, but the true I(X; T ) over these networks is provably either constant (discrete X) or infinite (continuous X). This work explains the discrepancy between theory and experiments, and clarifies what was actually measured by these past works. To this end, we introduce an auxiliary (noisy) DNN framework for which I(X; T ) is a meaningful quantity that depends on the network’s parameters. This noisy framework is shown to be a good proxy for the original (deterministic) DNN both in terms of performance and the learned representations. We then develop a rigorous estimator for I(X; T ) in noisy DNNs and observe compression in various models. By relating I(X; T ) in the noisy DNN to an information-theoretic communication problem, we show that compression is driven by the progressive clustering of hidden representations of inputs from the same class. Several methods to directly monitor clustering of hidden representations, both in noisy and deterministic DNNs, are used to show that meaningful clusters form in the T space. Finally, we return to the estimator of I(X; T ) employed in past works, and demonstrate that while it fails to capture the true (vacuous) mutual information, it does serve as a measure for clustering. This clarifies the past observations of compression and isolates the geometric clustering of hidden representations as the true phenomenon of interest.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/goldfeldEstimatingInformationFlow22.pdf}
}

@article{goldsteinEmergenceComplexSystems2011,
  title = {Emergence in Complex Systems},
  author = {Goldstein, Jeffrey},
  date = {2011},
  journaltitle = {The sage handbook of complexity and management},
  pages = {65--78},
  publisher = {{Sage Thousand Oaks, CA}},
  file = {/Users/hugo/Papers/pdf/goldsteinEmergenceComplexSystems2011.pdf;/Users/hugo/Zotero/storage/RLWFB2QL/books.html}
}

@article{goldsteinEmergenceConstructHistory1999,
  title = {Emergence as a {{Construct}}: {{History}} and {{Issues}}},
  shorttitle = {Emergence as a {{Construct}}},
  author = {Goldstein, Jeffrey},
  date = {1999-03-01},
  journaltitle = {Emergence},
  volume = {1},
  number = {1},
  pages = {49--72},
  publisher = {{Routledge}},
  issn = {1521-3250},
  doi = {10.1207/s15327000em0101_4},
  url = {https://doi.org/10.1207/s15327000em0101_4},
  urldate = {2022-11-15},
  annotation = {\_eprint: https://doi.org/10.1207/s15327000em0101\_4}
}

@article{goldtStochasticThermodynamicsLearning2017,
  title = {Stochastic {{Thermodynamics}} of {{Learning}}},
  author = {Goldt, Sebastian and Seifert, Udo},
  date = {2017-01-06},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {118},
  number = {1},
  eprint = {1611.09428},
  eprinttype = {arxiv},
  pages = {010601},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.118.010601},
  url = {http://arxiv.org/abs/1611.09428},
  urldate = {2019-05-26},
  abstract = {Virtually every organism gathers information about its noisy environment and builds models from that data, mostly using neural networks. Here, we use stochastic thermodynamics to analyse the learning of a classification rule by a neural network. We show that the information acquired by the network is bounded by the thermodynamic cost of learning and introduce a learning efficiency \$\textbackslash eta\textbackslash le1\$. We discuss the conditions for optimal learning and analyse Hebbian learning in the thermodynamic limit.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Physics - Biological Physics},
  file = {/Users/hugo/Zotero/storage/XGLWDTQQ/goldtStochasticThermodynamicsLearning2017.pdf}
}

@inproceedings{gomesDevisingEffectiveNovelty2015,
  title = {Devising {{Effective Novelty Search Algorithms}}: {{A Comprehensive Empirical Study}}},
  shorttitle = {Devising {{Effective Novelty Search Algorithms}}},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Gomes, Jorge and Mariano, Pedro and Christensen, Anders Lyhne},
  date = {2015-07-11},
  series = {{{GECCO}} '15},
  pages = {943--950},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2739480.2754736},
  url = {https://doi.org/10.1145/2739480.2754736},
  urldate = {2022-11-15},
  abstract = {Novelty search is a state-of-the-art evolutionary approach that promotes behavioural novelty instead of pursuing a static objective. Along with a large number of successful applications, many different variants of novelty search have been proposed. It is still unclear, however, how some key parameters and algorithmic components influence the evolutionary dynamics and performance of novelty search. In this paper, we conduct a comprehensive empirical study focused on novelty search's algorithmic components. We study the "k" parameter -- the number of nearest neighbours used in the computation of novelty scores; the use and function of an archive; how to combine novelty search with fitness-based evolution; and how to configure the mutation rate of the underlying evolutionary algorithm. Our study is conducted in a simulated maze navigation task. Our results show that the configuration of novelty search can have a significant impact on performance and behaviour space exploration. We conclude with a number of guidelines for the implementation and configuration of novelty search, which should help future practitioners to apply novelty search more effectively.},
  isbn = {978-1-4503-3472-3},
  keywords = {empirical study,evolutionary robotics,neuroevolution,novelty search,premature convergence},
  file = {/Users/hugo/Papers/pdf/gomesDevisingEffectiveNovelty2015.pdf}
}

@article{gomesEvolutionSwarmRobotics2013,
  title = {Evolution of Swarm Robotics Systems with Novelty Search},
  author = {Gomes, Jorge and Urbano, Paulo and Christensen, Anders Lyhne},
  date = {2013-09-01},
  journaltitle = {Swarm Intelligence},
  shortjournal = {Swarm Intell},
  volume = {7},
  number = {2},
  pages = {115--144},
  issn = {1935-3820},
  doi = {10.1007/s11721-013-0081-z},
  url = {https://doi.org/10.1007/s11721-013-0081-z},
  urldate = {2022-11-10},
  abstract = {Novelty search is a recent artificial evolution technique that challenges traditional evolutionary approaches. In novelty search, solutions are rewarded based on their novelty, rather than their quality with respect to a predefined objective. The lack of a predefined objective precludes premature convergence caused by a deceptive fitness function. In this paper, we apply novelty search combined with NEAT to the evolution of neural controllers for homogeneous swarms of robots. Our empirical study is conducted in simulation, and we use a common swarm robotics task—aggregation, and a more challenging task—sharing of an energy recharging station. Our results show that novelty search is unaffected by deception, is notably effective in bootstrapping evolution, can find solutions with lower complexity than fitness-based evolution, and can find a broad diversity of solutions for the same task. Even in non-deceptive setups, novelty search achieves solution qualities similar to those obtained in traditional fitness-based evolution. Our study also encompasses variants of novelty search that work in concert with fitness-based evolution to combine the exploratory character of novelty search with the exploitatory character of objective-based evolution. We show that these variants can further improve the performance of novelty search. Overall, our study shows that novelty search is a promising alternative for the evolution of controllers for robotic swarms.},
  langid = {english},
  keywords = {Behavioural diversity,Deception,Evolutionary robotics,NEAT,Neuroevolution,Novelty search,Swarm robotics},
  file = {/Users/hugo/Papers/pdf/gomesEvolutionSwarmRobotics2013.pdf}
}

@article{gomezIncrementalEvolutionComplex1997,
  title = {Incremental {{Evolution}} of {{Complex General Behavior}}},
  author = {Gomez, Faustino and Miikkulainen, Risto},
  date = {1997-01},
  journaltitle = {Adaptive Behavior},
  shortjournal = {Adaptive Behavior},
  volume = {5},
  number = {3-4},
  pages = {317--342},
  issn = {1059-7123, 1741-2633},
  doi = {10.1177/105971239700500305},
  url = {http://journals.sagepub.com/doi/10.1177/105971239700500305},
  urldate = {2020-01-29},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/383PCLAE/gomezIncrementalEvolutionComplex1997.pdf}
}

@inproceedings{gomezSolvingNonMarkovianControl1999,
  title = {Solving Non-{{Markovian}} Control Tasks with Neuroevolution},
  booktitle = {{{IJCAI}}},
  author = {Gomez, Faustino J. and Miikkulainen, Risto},
  date = {1999},
  volume = {99},
  pages = {1356--1361},
  file = {/Users/hugo/Zotero/storage/L52LYITV/gomezSolvingNonMarkovianControl1999.pdf}
}

@article{goodeSocialNewsCitizen2009,
  title = {Social News, Citizen Journalism and Democracy},
  author = {Goode, Luke},
  date = {2009-12},
  journaltitle = {New Media \& Society},
  volume = {11},
  number = {8},
  pages = {1287--1305},
  issn = {1461-4448, 1461-7315},
  doi = {10.1177/1461444809341393},
  url = {http://journals.sagepub.com/doi/10.1177/1461444809341393},
  urldate = {2018-06-18},
  langid = {english}
}

@inproceedings{goodfellowEmpiricalInvestigationCatastrophic2014,
  title = {An {{Empirical Investigation}} of {{Catastrophic Forgeting}} in {{Gradient-Based Neural Networks}}},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2014, {{Banff}}, {{AB}}, {{Canada}}, {{April}} 14-16, 2014, {{Conference Track Proceedings}}},
  author = {Goodfellow, Ian J. and Mirza, Mehdi and Da, Xia and Courville, Aaron C. and Bengio, Yoshua},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2014},
  url = {http://arxiv.org/abs/1312.6211},
  urldate = {2022-03-04}
}

@unpublished{goodfellowEmpiricalInvestigationCatastrophic2015,
  title = {An {{Empirical Investigation}} of {{Catastrophic Forgetting}} in {{Gradient-Based Neural Networks}}},
  author = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  date = {2015-03-03},
  eprint = {1312.6211},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1312.6211},
  urldate = {2021-10-17},
  abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models “forget” how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm–the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/goodfellowEmpiricalInvestigationCatastrophic2015.pdf}
}

@incollection{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  date = {2014},
  pages = {2672--2680},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
  urldate = {2019-01-18},
  file = {/Users/hugo/Zotero/storage/P9C7MQJX/goodfellowGenerativeAdversarialNets2014.pdf;/Users/hugo/Zotero/storage/MXVB6ILW/5423-generative-adversarial-nets.html}
}

@unpublished{goodmanClassesFastMaximum2001,
  title = {Classes for {{Fast Maximum Entropy Training}}},
  author = {Goodman, Joshua},
  date = {2001-08-09},
  eprint = {cs.CL/0108006},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/cs/0108006},
  urldate = {2019-01-20},
  abstract = {Maximum entropy models are considered by many to be one of the most promising avenues of language modeling research. Unfortunately, long training times make maximum entropy research difficult. We present a novel speedup technique: we change the form of the model to use classes. Our speedup works by creating two maximum entropy models, the first of which predicts the class of each word, and the second of which predicts the word itself. This factoring of the model leads to fewer nonzero indicator functions, and faster normalization, achieving speedups of up to a factor of 35 over one of the best previous techniques. It also results in typically slightly lower perplexities. The same trick can be used to speed training of other machine learning techniques, e.g. neural networks, applied to any problem with a large number of outputs, such as language modeling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,I.2.7},
  file = {/Users/hugo/Papers/pdf/goodmanClassesFastMaximum22.pdf}
}

@article{gosperExploitingRegularitiesLarge1984,
  title = {Exploiting Regularities in Large Cellular Spaces},
  author = {Gosper, R.Wm.},
  date = {1984-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  volume = {10},
  number = {1-2},
  pages = {75--80},
  issn = {01672789},
  doi = {10.1016/0167-2789(84)90251-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0167278984902513},
  urldate = {2019-09-11},
  langid = {english}
}

@inproceedings{goyalLearningInfluenceProbabilities2010,
  title = {Learning {{Influence Probabilities}} in {{Social Networks}}},
  booktitle = {Proceedings of the {{Third ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Goyal, Amit and Bonchi, Francesco and Lakshmanan, Laks V.S.},
  date = {2010},
  series = {{{WSDM}} '10},
  pages = {241--250},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1718487.1718518},
  url = {http://doi.acm.org/10.1145/1718487.1718518},
  urldate = {2019-01-03},
  abstract = {Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.},
  isbn = {978-1-60558-889-6},
  keywords = {influence,social networks,viral marketing},
  file = {/Users/hugo/Papers/pdf/goyalLearningInfluenceProbabilities22.pdf}
}

@software{gpt-neo,
  title = {{{GPT-Neo}}: {{Large}} Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  date = {2021-03},
  doi = {10.5281/zenodo.5297715},
  url = {https://doi.org/10.5281/zenodo.5297715},
  organization = {{Zenodo}},
  version = {1.0}
}

@article{grassbergerLongrangeEffectsElementary1986,
  title = {Long-Range Effects in an Elementary Cellular Automaton},
  author = {Grassberger, Peter},
  date = {1986-10-01},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {45},
  number = {1},
  pages = {27--39},
  issn = {1572-9613},
  doi = {10.1007/BF01033074},
  url = {https://doi.org/10.1007/BF01033074},
  urldate = {2020-11-04},
  abstract = {We present evidence that one of the “elementary” one-dimensional cellular automata in the sense of Wolfram (rule 22 in Wolfram's notation) involves very complex long-range effects, similar to a critical phenomenon. This is in contrast to superficial evidence that would suggest that this rule leads to fairly simple behavior.},
  langid = {english}
}

@article{grassbergerQuantitativeTheorySelfgenerated1986,
  title = {Toward a Quantitative Theory of Self-Generated Complexity},
  author = {Grassberger, Peter},
  date = {1986-09},
  journaltitle = {International Journal of Theoretical Physics},
  volume = {25},
  number = {9},
  pages = {907--938},
  issn = {0020-7748, 1572-9575},
  doi = {10.1007/BF00668821},
  url = {http://link.springer.com/10.1007/BF00668821},
  urldate = {2019-05-26},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/grassbergerQuantitativeTheorySelfgenerated12.pdf}
}

@inproceedings{grassbergerRandomnessInformationComplexity1989,
  title = {Randomness, {{Information}}, and {{Complexity}}},
  booktitle = {Proceedings of the 5th {{Mexican School}} on {{Statistical Physics}}},
  author = {Grassberger, Peter},
  date = {1989},
  eprint = {1208.3459},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1208.3459},
  urldate = {2019-05-26},
  abstract = {We review possible measures of complexity which might in particular be applicable to situations where the complexity seems to arise spontaneously. We point out that not all of them correspond to the intuitive (or “naive”) notion, and that one should not expect a unique observable of complexity. One of the main problems is to distinguish complex from disordered systems. This and the fact that complexity is closely related to information requires that we also give a review of information measures. We finally concentrate on quantities which measure in some way or other the difficulty of classifying and forecasting sequences of discrete symbols, and study them in simple examples.},
  archiveprefix = {arXiv},
  eventtitle = {Mexican {{School}} on {{Statistical Physics}} ({{EMFE}})},
  langid = {english},
  keywords = {Physics - Data Analysis; Statistics and Probability},
  file = {/Users/hugo/Papers/pdf/grassbergerRandomnessInformationComplexity12.pdf}
}

@unpublished{grattarolaLearningGraphCellular2021,
  title = {Learning {{Graph Cellular Automata}}},
  author = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
  date = {2021-10-27},
  eprint = {2110.14237},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.14237},
  urldate = {2022-01-17},
  abstract = {Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/grattarolaLearningGraphCellular2021.pdf;/Users/hugo/Zotero/storage/YLIP3JPR/2110.html}
}

@unpublished{graveLearningWordVectors2018,
  title = {Learning {{Word Vectors}} for 157 {{Languages}}},
  author = {Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  date = {2018-02-19},
  eprint = {1802.06893},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.06893},
  urldate = {2019-01-17},
  abstract = {Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/graveLearningWordVectors22.pdf}
}

@unpublished{gravesAdaptiveComputationTime2017,
  title = {Adaptive {{Computation Time}} for {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2017-02-21},
  eprint = {1603.08983},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1603.08983},
  urldate = {2020-07-20},
  abstract = {This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/gravesAdaptiveComputationTime2017.pdf}
}

@unpublished{gravesNeuralTuringMachines2014,
  title = {Neural Turing Machines},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  date = {2014},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Zotero/storage/K8996JNR/1410.html}
}

@unpublished{grbicSafeReinforcementLearning2020,
  title = {Safe {{Reinforcement Learning}} through {{Meta-learned Instincts}}},
  author = {Grbic, Djordje and Risi, Sebastian},
  date = {2020-05-06},
  eprint = {2005.03233},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2005.03233},
  urldate = {2020-07-27},
  abstract = {An important goal in reinforcement learning is to create agents that can quickly adapt to new goals while avoiding situations that might cause damage to themselves or their environments. One way agents learn is through exploration mechanisms, which are needed to discover new policies. However, in deep reinforcement learning, exploration is normally done by injecting noise in the action space. While performing well in many domains, this setup has the inherent risk that the noisy actions performed by the agent lead to unsafe states in the environment. Here we introduce a novel approach called Meta-Learned Instinctual Networks (MLIN) that allows agents to safely learn during their lifetime while avoiding potentially hazardous states. At the core of the approach is a plastic network trained through reinforcement learning and an evolved "instinctual" network, which does not change during the agent's lifetime but can modulate the noisy output of the plastic network. We test our idea on a simple 2D navigation task with no-go zones, in which the agent has to learn to approach new targets during deployment. MLIN outperforms standard meta-trained networks and allows agents to learn to navigate to new targets without colliding with any of the no-go zones. These results suggest that meta-learning augmented with an instinctual network is a promising new approach for safe AI, which may enable progress in this area on a variety of different domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/grbicSafeReinforcementLearning2020.pdf;/Users/hugo/Zotero/storage/TTV5NLSH/2005.html}
}

@unpublished{greydanusScalingDeepLearning2020,
  title = {Scaling *down* {{Deep Learning}}},
  author = {Greydanus, Sam},
  date = {2020-11-29},
  eprint = {2011.14439},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2011.14439},
  urldate = {2020-12-02},
  abstract = {Though deep learning models have taken on commercial and political relevance, many aspects of their training and operation remain poorly understood. This has sparked interest in "science of deep learning" projects, many of which are run at scale and require enormous amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94\% accuracy respectively (these models obtain 94, 99+, and 99+\% on MNIST). Then we present example use cases which include measuring the spatial inductive biases of lottery tickets, observing deep double descent, and metalearning an activation function.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/DHL7XSVF/Greydanus - 2020 - Scaling down Deep Learning.pdf}
}

@inproceedings{grillBootstrapYourOwn2020,
  title = {Bootstrap {{Your Own Latent A New Approach}} to {{Self-Supervised Learning}}},
  booktitle = {Proceedings of the 34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)},
  author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
  date = {2020},
  pages = {14},
  location = {{Vancouver, Canada}},
  eventtitle = {{{NeurIPS}} 2020},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/grillBootstrapYourOwn2020.pdf}
}

@article{grishmanInformationExtractionCapabilities2012,
  title = {Information {{Extraction}}: {{Capabilities}} and {{Challenges}}},
  author = {Grishman, Ralph},
  date = {2012},
  journaltitle = {International Winter School in Language and Speech Technologies},
  eprint = {8806820},
  eprinttype = {pmid},
  pages = {377},
  issn = {1541-1672},
  doi = {10.1561/1500000003},
  abstract = {The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recog- nition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of struc- ture extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scien- tific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learn- ing, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem. This review is a survey of information extraction research of over two decades from these diverse communities. We},
  file = {/Users/hugo/Papers/pdf/grishmanInformationExtractionCapabilities22.pdf}
}

@article{grizouCuriousFormulationRobot2020,
  title = {A Curious Formulation Robot Enables the Discovery of a Novel Protocell Behavior},
  author = {Grizou, Jonathan and Points, Laurie J. and Sharma, Abhishek and Cronin, Leroy},
  date = {2020-01},
  journaltitle = {Science Advances},
  volume = {6},
  number = {5},
  pages = {eaay4237},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aay4237},
  url = {https://advances.sciencemag.org/lookup/doi/10.1126/sciadv.aay4237},
  urldate = {2020-05-15},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/grizouCuriousFormulationRobot2020.pdf}
}

@book{grunwaldMinimumDescriptionLength2007,
  title = {The Minimum Description Length Principle},
  author = {Grunwald, Peter},
  date = {2007},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-07281-6},
  pagetotal = {703},
  keywords = {Minimum description length (Information theory)},
  annotation = {OCLC: ocm70292149}
}

@unpublished{grunwaldShannonInformationKolmogorov2004,
  title = {Shannon Information and {{Kolmogorov}} Complexity},
  author = {Grunwald, Peter and Vitányi, Paul},
  date = {2004},
  eprint = {cs/0410002},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/grunwaldShannonInformationKolmogorov2004.pdf;/Users/hugo/Zotero/storage/KXDDML85/0410002.html}
}

@unpublished{grunwaldTutorialIntroductionMinimum2004,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  date = {2004-06-04},
  eprint = {math/0406077},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/math/0406077},
  urldate = {2020-03-31},
  abstract = {This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection "Advances in Minimum Description Length: Theory and Application" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).},
  archiveprefix = {arXiv},
  keywords = {6201,6801,68T05,68T10,9401,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory},
  file = {/Users/hugo/Papers/pdf/grunwaldTutorialIntroductionMinimum2004.pdf;/Users/hugo/Zotero/storage/993FVSLK/0406077.html}
}

@unpublished{gulrajaniSearchLostDomain2020,
  title = {In {{Search}} of {{Lost Domain Generalization}}},
  author = {Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2020-07-02},
  eprint = {2007.01434},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.01434},
  urldate = {2020-07-06},
  abstract = {The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions—datasets, architectures, and model selection criteria—render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DOMAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/gulrajaniSearchLostDomain2020.pdf}
}

@article{guMoreReallyDifferent2009,
  title = {More {{Really}} Is {{Different}}},
  author = {Gu, Mile and Weedbrook, Christian and Perales, Alvaro and Nielsen, Michael A.},
  date = {2009-05},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {238},
  number = {9-10},
  eprint = {0809.0151},
  eprinttype = {arxiv},
  pages = {835--839},
  issn = {01672789},
  doi = {10.1016/j.physd.2008.12.016},
  url = {http://arxiv.org/abs/0809.0151},
  urldate = {2021-05-16},
  abstract = {In 1972, P.W.Anderson suggested that `More is Different', meaning that complex physical systems may exhibit behavior that cannot be understood only in terms of the laws governing their microscopic constituents. We strengthen this claim by proving that many macroscopic observable properties of a simple class of physical systems (the infinite periodic Ising lattice) cannot in general be derived from a microscopic description. This provides evidence that emergent behavior occurs in such systems, and indicates that even if a `theory of everything' governing all microscopic interactions were discovered, the understanding of macroscopic order is likely to require additional insights.},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Other Condensed Matter,Quantum Physics},
  file = {/Users/hugo/Papers/pdf/guMoreReallyDifferent2009.pdf;/Users/hugo/Zotero/storage/2J9J7UNN/0809.html}
}

@inproceedings{guoMeteorAdoptSyntactic2019,
  title = {Meteor++ 2.0: {{Adopt Syntactic Level Paraphrase Knowledge}} into {{Machine Translation Evaluation}}},
  shorttitle = {Meteor++ 2.0},
  booktitle = {Proceedings of the {{Fourth Conference}} on {{Machine Translation}}, {{WMT}} 2019, {{Florence}}, {{Italy}}, {{August}} 1-2, 2019 - {{Volume}} 2: {{Shared Task Papers}}, {{Day}} 1},
  author = {Guo, Yinuo and Hu, Junfeng},
  editor = {Bojar, Ondrej and Chatterjee, Rajen and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Jimeno-Yepes, Antonio and Koehn, Philipp and Martins, André and Monz, Christof and Negri, Matteo and Névéol, Aurélie and Neves, Mariana L. and Post, Matt and Turchi, Marco and Verspoor, Karin},
  date = {2019},
  pages = {501--506},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/w19-5357},
  file = {/Users/hugo/Papers/pdf/guoMeteorAdoptSyntactic2019.pdf}
}

@article{gutowitzHierarchicalClassificationCellular1990,
  title = {A Hierarchical Classification of Cellular Automata},
  author = {Gutowitz, Howard A.},
  date = {1990-09},
  journaltitle = {Physica D: Nonlinear Phenomena},
  volume = {45},
  number = {1-3},
  pages = {136--156},
  issn = {01672789},
  doi = {10.1016/0167-2789(90)90179-S},
  url = {https://linkinghub.elsevier.com/retrieve/pii/016727899090179S},
  urldate = {2019-05-26},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/gutowitzHierarchicalClassificationCellular12.pdf}
}

@article{gutowitzLocalStructureTheory1987,
  title = {Local Structure Theory for Cellular Automata},
  author = {Gutowitz, Howard A. and Victor, Jonathan D. and Knight, Bruce W.},
  date = {1987-09-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {28},
  number = {1},
  pages = {18--48},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(87)90120-5},
  url = {http://www.sciencedirect.com/science/article/pii/0167278987901205},
  urldate = {2019-05-26},
  abstract = {The mean-field theory for cellular automata (Wolfram, and Schulman and Seiden) is generalized to the local structure theory. The local structure theory is a sequence of finitely-parameterized models of the statistical features of a cellular automaton's evolution. The nth model in the sequence takes into account correlations in terms of the probability of blocks of n states. A class of measures, the n-block measures, is introduced.The local structure operator of order n maps n-block measures to n-block measures in a manner which reflects the cellular automaton map on blocks of states. The fixed points of the map on measures approximate the invariant measures of the cellular automaton. The ability of the local structure theory to model evolution from uncorrelated initial distributions is studied. The theory gives exact results in simple cases. In more complex cases, Monte Carlo numerical experiments suggest that an accurate statistical portrait of cellular automaton evolution is obtained. The invariant measures of a cellular automaton and the stability of these measures may be obtained from the local structure theory. The local structure theory appears to be a powerful method for characterization and classification of cellular automata. Nearest neighbor cellular automata with two states per cell are studied using this method.},
  file = {/Users/hugo/Zotero/storage/J4FNH3RQ/gutowitzLocalStructureTheory1987.pdf;/Users/hugo/Zotero/storage/XQY4NGR9/0167278987901205.html}
}

@incollection{gutowitzMeanFieldTheory1995,
  title = {Mean Field Theory of the {{Edge}} of {{Chaos}}},
  booktitle = {Advances in {{Artificial Life}}},
  author = {Gutowitz, Howard and Langton, Chris},
  editor = {Morán, Federico and Moreno, Alvaro and Merelo, Juan Julián and Chacón, Pablo},
  date = {1995},
  volume = {929},
  pages = {52--64},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-59496-5_288},
  url = {http://link.springer.com/10.1007/3-540-59496-5_288},
  urldate = {2019-05-26},
  abstract = {Is there an Edge of Chaos, and if so, can evolution take us to it? Many issues have to be settled before any de nitive answer can be given. For quantitative work, we need a good measure of complexity. We suggest that convergence time is an appropriate and useful measure. In the case of cellular automata, one of the advantages of the convergence-time measure is that it can be analytically approximated using a generalized mean eld theory.},
  editorb = {Goos, G. and Hartmanis, J. and Leeuwen, J. and Carbonell, Jaime G. and Siekmann, Jörg},
  editorbtype = {redactor},
  isbn = {978-3-540-59496-3 978-3-540-49286-3},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/RPFBS3AC/gutowitzMeanFieldTheory1995.pdf}
}

@article{gutowitzMethodsDesigningCellular,
  title = {Methods for {{Designing Cellular Automata}} with \textbackslash{{Interesting}}" {{Behavior}}},
  author = {Gutowitz, H A and Langton, C G},
  pages = {8},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/gutowitzMethodsDesigningCellular2.pdf;/Users/hugo/Zotero/storage/XAVBWWHY/gutowitzMethodsDesigningCellular.pdf}
}

@article{gutowitzTransientsCyclesComplexity1991,
  title = {Transients, Cycles, and Complexity in Cellular Automata},
  author = {Gutowitz, Howard A.},
  date = {1991},
  journaltitle = {Physical Review A},
  volume = {44},
  number = {12},
  pages = {R7881},
  publisher = {{APS}},
  file = {/Users/hugo/Papers/pdf/gutowitzTransientsCyclesComplexity1991.pdf}
}

@unpublished{guttenbergBeingCuriousAnswers2018,
  title = {Being Curious about the Answers to Questions: Novelty Search with Learned Attention},
  shorttitle = {Being Curious about the Answers to Questions},
  author = {Guttenberg, Nicholas and Biehl, Martin and Virgo, Nathaniel and Kanai, Ryota},
  date = {2018-06-01},
  eprint = {1806.00201},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.00201},
  urldate = {2019-12-16},
  abstract = {We investigate the use of attentional neural network layers in order to learn a ‘behavior characterization’ which can be used to drive novelty search and curiosity-based policies. The space is structured towards answering a particular distribution of questions, which are used in a supervised way to train the attentional neural network. We find that in a 2d exploration task, the structure of the space successfully encodes local sensory-motor contingencies such that even a greedy local ‘do the most novel action’ policy with no reinforcement learning or evolution can explore the space quickly. We also apply this to a high/low number guessing game task, and find that guessing according to the learned attention profile performs active inference and can discover the correct number more quickly than an exact but passive approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/LVQGLVSD/guttenbergBeingCuriousAnswers2018.pdf}
}

@unpublished{guttenbergPotentialOpenendednessNeural2018,
  title = {On the Potential for Open-Endedness in Neural Networks},
  author = {Guttenberg, Nicholas and Virgo, Nathaniel and Penn, Alexandra},
  date = {2018-12-12},
  eprint = {1812.04907},
  eprinttype = {arxiv},
  primaryclass = {nlin, q-bio},
  url = {http://arxiv.org/abs/1812.04907},
  urldate = {2019-12-16},
  abstract = {Natural evolution gives the impression of leading to an openended process of increasing diversity and complexity. If our goal is to produce such open-endedness artificially, this suggests an approach driven by evolutionary metaphor. On the other hand, techniques from machine learning and artificial intelligence are often considered too narrow to provide the sort of exploratory dynamics associated with evolution. In this paper, we hope to bridge that gap by reviewing common barriers to open-endedness in the evolution-inspired approach and how they are dealt with in the evolutionary case — collapse of diversity, saturation of complexity, and failure to form new kinds of individuality. We then show how these problems map onto similar issues in the machine learning approach, and discuss how the same insights and solutions which alleviated those barriers in evolutionary approaches can be ported over. At the same time, the form these issues take in the machine learning formulation suggests new ways to analyze and resolve barriers to open-endedness. Ultimately, we hope to inspire researchers to be able to interchangeably use evolutionary and gradient-descent-based machine learning methods to approach the design and creation of open-ended systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Populations and Evolution},
  file = {/Users/hugo/Zotero/storage/C2XXX6NB/guttenbergPotentialOpenendednessNeural2018.pdf}
}

@unpublished{guuTraversingKnowledgeGraphs2015,
  title = {Traversing {{Knowledge Graphs}} in {{Vector Space}}},
  author = {Guu, Kelvin and Miller, John and Liang, Percy},
  date = {2015-06-02},
  eprint = {1506.01094},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1506.01094},
  urldate = {2018-04-24},
  abstract = {Path queries on a knowledge graph can be used to answer compositional questions such as "What languages are spoken by people living in Lisbon?". However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new "compositional" training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43\%) and achieving new state-of-the-art results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/guuTraversingKnowledgeGraphs22.pdf;/Users/hugo/Zotero/storage/T3H2A84J/1506.html}
}

@unpublished{hamiltonRepresentationLearningGraphs2017,
  title = {Representation {{Learning}} on {{Graphs}}: {{Methods}} and {{Applications}}},
  shorttitle = {Representation {{Learning}} on {{Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2017-09-16},
  eprint = {1709.05584},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1709.05584},
  urldate = {2018-12-28},
  abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/hugo/Papers/pdf/hamiltonRepresentationLearningGraphs22.pdf;/Users/hugo/Papers/pdf/hamiltonRepresentationLearningGraphs3.pdf;/Users/hugo/Zotero/storage/TVJDDZ8H/1709.html}
}

@article{hammerschmidtLifeCyclesFitness2014,
  title = {Life Cycles, Fitness Decoupling and the Evolution of Multicellularity},
  author = {Hammerschmidt, Katrin and Rose, Caroline J. and Kerr, Benjamin and Rainey, Paul B.},
  date = {2014-11},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {515},
  number = {7525},
  pages = {75--79},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature13884},
  url = {http://www.nature.com/articles/nature13884},
  urldate = {2022-11-01},
  langid = {english}
}

@article{hammingErrorDetectingError1950,
  title = {Error {{Detecting}} and {{Error Correcting Codes}}},
  author = {Hamming, R. W.},
  date = {1950-04},
  journaltitle = {Bell System Technical Journal},
  volume = {29},
  number = {2},
  pages = {147--160},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1950.tb00463.x},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6772729},
  urldate = {2020-06-19},
  langid = {english}
}

@inproceedings{hanLearningBothWeights2015,
  title = {Learning Both Weights and Connections for Efficient Neural Network},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  date = {2015},
  pages = {1135--1143},
  file = {/Users/hugo/Papers/pdf/hanLearningBothWeights2015.pdf;/Users/hugo/Zotero/storage/HJB5XAUB/5784-learning-both-weights-andconnections-.html}
}

@article{hansonAttractorbasinPortraitCellular1992,
  title = {The Attractor-Basin Portrait of a Cellular Automaton},
  shorttitle = {The Attractor?},
  author = {Hanson, James E. and Crutchfield, James P.},
  date = {1992-03},
  journaltitle = {Journal of Statistical Physics},
  volume = {66},
  number = {5-6},
  pages = {1415--1462},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/BF01054429},
  url = {http://link.springer.com/10.1007/BF01054429},
  urldate = {2020-03-12},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hansonAttractorbasinPortraitCellular1992.pdf}
}

@article{hansonCellularAutomataEmergent,
  title = {Cellular {{Automata}}, {{Emergent Phenomena}} In},
  author = {Hanson, James E},
  pages = {11},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hansonCellularAutomataEmergent.pdf}
}

@thesis{hansonComputationalMechanicsCellular1993,
  type = {phdthesis},
  title = {Computational Mechanics of Cellular Automata},
  author = {Hanson, James Edwin and Crutchfield, J. P.},
  date = {1993},
  institution = {{University of California, Berkeley}},
  file = {/Users/hugo/Papers/pdf/hansonComputationalMechanicsCellular1993.pdf;/Users/hugo/Zotero/storage/YDFB2AYA/Hanson and Crutchfield - 1993 - Computational mechanics of cellular automata.gz}
}

@article{hansonComputationalMechanicsCellular1997,
  title = {Computational Mechanics of Cellular Automata: {{An}} Example},
  shorttitle = {Computational Mechanics of Cellular Automata},
  author = {Hanson, James E. and Crutchfield, James P.},
  date = {1997-04-15},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  series = {Lattice {{Dynamics}}},
  volume = {103},
  number = {1},
  pages = {169--189},
  issn = {0167-2789},
  doi = {10.1016/S0167-2789(96)00259-X},
  url = {http://www.sciencedirect.com/science/article/pii/S016727899600259X},
  urldate = {2020-03-24},
  abstract = {We illustrate and extend the techniques of computational mechanics in explicating the structures that emerge in the space-time behavior of elementary one-dimensional cellular automaton rule 54. The dominant regular domain of the cellular automation is identified and a domain filter is constructed to locate and classify defects in the domain. The primary particles are identified and a range of interparticle interactions is studied. The deterministic equation of motion of the filtered space-time behavior is derived. Filters of increasing sophistication are constructed for the efficient gathering of particle statistics and for the identification of higher-level defects, particle interactions, and secondary domains. We define the emergence time at which the space-time behavior condenses into configurations consisting only of domains, particles, and particle interactions. Taken together, these techniques serve as the basis for the investigation of pattern evolution and self-organization in this representative system.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hansonComputationalMechanicsCellular1997.pdf;/Users/hugo/Zotero/storage/BDFIKY33/S016727899600259X.html}
}

@article{haraoConsiderationCellularAutomata1973,
  title = {A Consideration on Cellular Automata with Errors},
  author = {Harao, M. and Noguchi, S.},
  date = {1973},
  journaltitle = {Comittee on Automaton, IECE Japan, AL},
  volume = {73},
  pages = {15}
}

@article{haraoFaultTolerantCellular1975,
  title = {Fault Tolerant Cellular Automata},
  author = {Harao, Masateru and Noguchi, Shoichi},
  date = {1975-10-01},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  volume = {11},
  number = {2},
  pages = {171--185},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(75)80066-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000075800663},
  urldate = {2022-11-10},
  abstract = {Cellular automata have some useful characteristics because of the uniformity of the cellular structure. Here we use geometrical codings to design fault tolerant cellular automata which simulate given cellular automata. First, the concept of a group graph is introduced as a mathematical description of the cellular structure. Geometrical codings are then defined using its algebraic properties. Under the condition of K-separated misoperation, i.e., at each state transition at most one cell in any K-neighborhood of the cellular space can possibly misoperate, it is possible to design cellular automata which have detection capabity or correction capability. Some examples are also presented using the two-dimensional lattice and the hexagonal cellular space.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/haraoFaultTolerantCellular1975.pdf;/Users/hugo/Zotero/storage/4RXVR9PW/S0022000075800663.html}
}

@article{harpDesigningApplicationSpecificNeural1992,
  title = {Designing {{Application-Specific Neural Networks Using}} the {{Genetic Algorithm}}},
  author = {Harp, Steven A and Samad, Tarig and Guha, Aloke},
  date = {1992},
  journaltitle = {Proceedings of the International Conference on Combinations of Genetic Algorithms and Neural Networks},
  pages = {87--96},
  abstract = {We present a general and systematic method for neural network design based on the genetic algorithm. The technique works in conjunction with network learning rules, addressing aspects of the network's gross architecture, connectivity, and learning rule parameters. Networks can be optimiled for various applicationspecific criteria, such as learning speed, generalilation, robustness and connectivity. The approach is model-independent. We describe a prototype system, NeuroGENESYS, that employs the backpropagation learning rule. Experiments on several small problems have been conducted. In each case, NeuroGENESYS has produced networks that perform significantly better than the randomly generated networks of its initial population. The computational feasibility of our approach is discussed.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/KEH479RJ/harpDesigningApplicationSpecificNeural1992.pdf}
}

@unpublished{hartmannSemidiscreteOptimalTransport2017,
  title = {Semi-Discrete Optimal Transport - the Case P=1},
  author = {Hartmann, Valentin and Schuhmacher, Dominic},
  date = {2017-06-23},
  eprint = {1706.07650},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  url = {http://arxiv.org/abs/1706.07650},
  urldate = {2018-12-30},
  abstract = {We consider the problem of finding an optimal transport plan between an absolutely continuous measure \$\textbackslash mu\$ on \$\textbackslash mathcal\{X\} \textbackslash subset \textbackslash mathbb\{R\}\^d\$ and a finitely supported measure \$\textbackslash nu\$ on \$\textbackslash mathbb\{R\}\^d\$ when the transport cost is the Euclidean distance. We may think of this problem as closest distance allocation of some ressource continuously distributed over space to a finite number of processing sites with capacity constraints. This article gives a detailed discussion of the problem, including a comparison with the much better studied case of squared Euclidean cost ("the case \$p=2\$"). We present an algorithm for computing the optimal transport plan, which is similar to the approach for \$p=2\$ by Aurenhammer, Hoffmann and Aronov [Algorithmica 20, 61-76, 1998] and M\textbackslash 'erigot [Computer Graphics Forum 30, 1583--1592, 2011]. We show the necessary results to make the approach work for the Euclidean cost, evaluate its performance on a set of test cases, and give a number of applications. The later include goodness-of-fit partitions, a novel visual tool for assessing whether a finite sample is consistent with a posited probability density.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Numerical Analysis,Statistics - Computation},
  file = {/Users/hugo/Papers/pdf/hartmannSemidiscreteOptimalTransport22.pdf;/Users/hugo/Papers/pdf/hartmannSemidiscreteOptimalTransport3.pdf;/Users/hugo/Zotero/storage/AT5MPZ6Y/1706.html;/Users/hugo/Zotero/storage/SGBDYFJK/1706.html}
}

@unpublished{hasaniLiquidTimeconstantNetworks2020,
  title = {Liquid {{Time-constant Networks}}},
  author = {Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  date = {2020-09-12},
  eprint = {2006.04439},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.04439},
  urldate = {2020-10-21},
  abstract = {We introduce a new class of time-continuous recurrent neural network models. Instead of declaring a learning system's dynamics by implicit nonlinearities, we construct networks of linear first-order dynamical systems modulated via nonlinear interlinked gates. The resulting models represent dynamical systems with varying (i.e., \textbackslash emph\{liquid\}) time-constants coupled to their hidden state, with outputs being computed by numerical differential equation solvers. These neural networks exhibit stable and bounded behavior, yield superior expressivity within the family of neural ordinary differential equations, and give rise to improved performance on time-series prediction tasks. To demonstrate these properties, we first take a theoretical approach to find bounds over their dynamics, and compute their expressive power by the \textbackslash emph\{trajectory length\} measure in a latent trajectory space. We then conduct a series of time-series prediction experiments to manifest the approximation capability of Liquid Time-Constant Networks (LTCs) compared to modern RNNs. Code and data are available at https://github.com/raminmh/liquid\_time\_constant\_networks},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/hasaniLiquidTimeconstantNetworks2020.pdf;/Users/hugo/Zotero/storage/VEP7PMLJ/2006.html}
}

@inproceedings{hassibiSecondOrderDerivatives1993,
  title = {Second Order Derivatives for Network Pruning: {{Optimal}} Brain Surgeon},
  shorttitle = {Second Order Derivatives for Network Pruning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hassibi, Babak and Stork, David G.},
  date = {1993},
  pages = {164--171},
  file = {/Users/hugo/Papers/pdf/hassibiSecondOrderDerivatives1993.pdf}
}

@misc{hatamizadehGlobalContextVision2022,
  title = {Global {{Context Vision Transformers}}},
  author = {Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo},
  date = {2022-06-20},
  number = {arXiv:2206.09959},
  eprint = {2206.09959},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.09959},
  url = {http://arxiv.org/abs/2206.09959},
  urldate = {2022-07-26},
  abstract = {We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization. Our method leverages global context self-attention modules, joint with local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the issue of lack of the inductive bias in ViTs via proposing to use a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the base, small and tiny variants of GC ViT with \$28\$M, \$51\$M and \$90\$M parameters achieve \$\textbackslash textbf\{83.2\textbackslash\%\}\$, \$\textbackslash textbf\{83.9\textbackslash\%\}\$ and \$\textbackslash textbf\{84.4\textbackslash\%\}\$ Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based Swin Transformer by a large margin. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation using MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins. Code available at https://github.com/NVlabs/GCViT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/hatamizadehGlobalContextVision2022.pdf;/Users/hugo/Zotero/storage/FK6KCWEW/2206.html}
}

@article{haWorldModels2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, Jürgen},
  date = {2018-03-27},
  journaltitle = {World Models},
  shortjournal = {World Models},
  volume = {1},
  number = {1},
  pages = {e10},
  doi = {10.5281/zenodo.1207631},
  url = {https://worldmodels.github.io/},
  urldate = {2020-07-29},
  abstract = {Can agents learn inside of their own dreams?},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/haWorldModels2018.pdf;/Users/hugo/Zotero/storage/5H9SMDQA/worldmodels.github.io.html}
}

@article{hayNormalizingRoleRationalist2004,
  title = {The Normalizing Role of Rationalist Assumptions in the Institutional Embedding of Neoliberalism},
  author = {Hay, Colin},
  date = {2004-11-01},
  journaltitle = {Economy and Society},
  volume = {33},
  number = {4},
  pages = {500--527},
  publisher = {{Routledge}},
  issn = {0308-5147},
  doi = {10.1080/0308514042000285260},
  url = {https://doi.org/10.1080/0308514042000285260},
  urldate = {2022-09-06},
  abstract = {The political economy of Britain over the past three decades provides an interesting example of the consolidation, normalization and institutionalization of a new economic paradigm – neoliberalism. As such, it serves as a potentially instructive focus for debate both about the conditions under which economic paradigms are replaced and consolidated and the evolution of such paradigms through the process of institutionalization. In this paper I suggest that the institutionalization of this new economic paradigm has been associated with the shift from a normative to a normalized and necessitarian neoliberalism. I examine the role played by rationalist assumptions in this extended process of normalization-institutionalization. After presenting a stylized account of the evolution of British neoliberalism, I show how New Labour's monetary policy regime is the heir to the legacy of monetarism and its agenda of labour-market reform is the heir to Thatcherism's supply-side economics. I suggest that the time-inconsistency thesis and the business school globalization thesis have played an equivalent role, for New Labour, to that played for the new right by monetarism and supply-side economics in legitimating neoliberalism. In this way neoliberalism has been normalized. In the final sections of the paper I reflect on the implications of the normalized and necessitarian character of neoliberalism in Britain for its contestability and for democratic economic governance more broadly.},
  keywords = {ideas,institutionalization,Neoliberalism,New Labour2,rational choice theory},
  annotation = {\_eprint: https://doi.org/10.1080/0308514042000285260}
}

@article{heatonEvolvingContinuousCellular2019,
  title = {Evolving Continuous Cellular Automata for Aesthetic Objectives},
  author = {Heaton, Jeff},
  date = {2019-03},
  journaltitle = {Genetic Programming and Evolvable Machines},
  volume = {20},
  number = {1},
  pages = {93--125},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-018-9336-1},
  url = {http://link.springer.com/10.1007/s10710-018-9336-1},
  urldate = {2020-01-09},
  abstract = {We present MergeLife, a genetic algorithm (GA) capable of evolving continuous cellular automata (CA) that generate full color dynamic animations according to aesthetic user specifications. A simple 16-byte update rule is introduced that is evolved through an objective function that requires only initial human aesthetic guidelines. This update rule provides a fixed-length genome that can be successfully optimized by a GA. Also introduced are several novel fitness measures that when given human selected aesthetic guidelines encourage the evolution of complex animations that often include spaceships, oscillators, still life, and other complex emergent behavior. The results of this research are several complex and long running update rules and the objective function parameters that produced them. Several update rules produced from this paper exhibit complex emergent behavior through patterns, such as spaceships, guns, oscillators, and Universal Turing Machines. Because the true animated behavior of these CA cannot be observed from static images, we also present an on-line JavaScript viewer that is capable of animating any MergeLife 16-byte update rule.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/FSCKA5NA/heatonEvolvingContinuousCellular2019.pdf;/Users/hugo/Zotero/storage/RKATQR6T/heatonEvolvingContinuousCellular2019.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  url = {http://ieeexplore.ieee.org/document/7780459/},
  urldate = {2021-09-30},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/heDeepResidualLearning2016.pdf}
}

@article{heEarlyPredictionCognitive2018,
  title = {Early Prediction of Cognitive Deficits in Very Preterm Infants Using Functional Connectome Data in an Artificial Neural Network Framework},
  author = {He, Lili and Li, Hailong and Holland, Scott K. and Yuan, Weihong and Altaye, Mekibib and Parikh, Nehal A.},
  date = {2018-01-01},
  journaltitle = {NeuroImage: Clinical},
  shortjournal = {NeuroImage: Clinical},
  volume = {18},
  pages = {290--297},
  issn = {2213-1582},
  doi = {10.1016/j.nicl.2018.01.032},
  url = {http://www.sciencedirect.com/science/article/pii/S2213158218300329},
  urldate = {2019-04-04},
  abstract = {Investigation of the brain's functional connectome can improve our understanding of how an individual brain's organizational changes influence cognitive function and could result in improved individual risk stratification. Brain connectome studies in adults and older children have shown that abnormal network properties may be useful as discriminative features and have exploited machine learning models for early diagnosis in a variety of neurological conditions. However, analogous studies in neonates are rare and with limited significant findings. In this paper, we propose an artificial neural network (ANN) framework for early prediction of cognitive deficits in very preterm infants based on functional connectome data from resting state fMRI. Specifically, we conducted feature selection via stacked sparse autoencoder and outcome prediction via support vector machine (SVM). The proposed ANN model was unsupervised learned using brain connectome data from 884 subjects in autism brain imaging data exchange database and SVM was cross-validated on 28 very preterm infants (born at 23–31\,weeks of gestation and without brain injury; scanned at term-equivalent postmenstrual age). Using 90 regions of interests, we found that the ANN model applied to functional connectome data from very premature infants can predict cognitive outcome at 2\,years of corrected age with an accuracy of 70.6\% and area under receiver operating characteristic curve of 0.76. We also noted that several frontal lobe and somatosensory regions, significantly contributed to prediction of cognitive deficits 2\,years later. Our work can be considered as a proof of concept for utilizing ANN models on functional connectome data to capture the individual variability inherent in the developing brains of preterm infants. The full potential of ANN will be realized and more robust conclusions drawn when applied to much larger neuroimaging datasets, as we plan to do.},
  keywords = {Artificial neural network,Cognitive deficit,Functional MRI,Stacked sparse autoencoder,Support vector machine,Very preterm infants},
  file = {/Users/hugo/Papers/pdf/heEarlyPredictionCognitive22.pdf;/Users/hugo/Zotero/storage/XTYE87YE/S2213158218300329.html}
}

@article{henriquesPerilsEcologicallyUnequal2022,
  title = {The Perils of Ecologically Unequal Exchange: {{Contesting}} Rare-Earth Mining in {{Greenland}}},
  shorttitle = {The Perils of Ecologically Unequal Exchange},
  author = {Henriques, Irene and Böhm, Steffen},
  date = {2022-05},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {349},
  pages = {131378},
  issn = {09596526},
  doi = {10.1016/j.jclepro.2022.131378},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959652622010022},
  urldate = {2022-04-05},
  langid = {english}
}

@inproceedings{herelEmergenceNoveltyEvolutionary2022,
  title = {Emergence of {{Novelty}} in {{Evolutionary Algorithms}}},
  author = {Herel, David and Zogatova, Dominika and Kripner, Matej and Mikolov, Tomas},
  date = {2022-07-18},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00501},
  url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00501/112291/Emergence-of-Novelty-in-Evolutionary-Algorithms},
  urldate = {2022-11-15},
  eventtitle = {{{ALIFE}} 2022: {{The}} 2022 {{Conference}} on {{Artificial Life}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/herelEmergenceNoveltyEvolutionary2022.pdf;/Users/hugo/Zotero/storage/JP58G8LW/112291.html}
}

@article{herold29geneCytogeneticScore2018,
  title = {A 29-Gene and Cytogenetic Score for the Prediction of Resistance to Induction Treatment in Acute Myeloid Leukemia},
  author = {Herold, Tobias and Jurinovic, Vindi and Batcha, Aarif M. N. and Bamopoulos, Stefanos A. and Rothenberg-Thurley, Maja and Ksienzyk, Bianka and Hartmann, Luise and Greif, Philipp A. and Phillippou-Massier, Julia and Krebs, Stefan and Blum, Helmut and Amler, Susanne and Schneider, Stephanie and Konstandin, Nikola and Sauerland, Maria Cristina and Görlich, Dennis and Berdel, Wolfgang E. and Wörmann, Bernhard J. and Tischer, Johanna and Subklewe, Marion and Bohlander, Stefan K. and Braess, Jan and Hiddemann, Wolfgang and Metzeler, Klaus H. and Mansmann, Ulrich and Spiekermann, Karsten},
  date = {2018-03},
  journaltitle = {Haematologica},
  shortjournal = {Haematologica},
  volume = {103},
  number = {3},
  eprint = {29242298},
  eprinttype = {pmid},
  pages = {456--465},
  issn = {1592-8721},
  doi = {10.3324/haematol.2017.178442},
  abstract = {Primary therapy resistance is a major problem in acute myeloid leukemia treatment. We set out to develop a powerful and robust predictor for therapy resistance for intensively treated adult patients. We used two large gene expression data sets (n=856) to develop a predictor of therapy resistance, which was validated in an independent cohort analyzed by RNA sequencing (n=250). In addition to gene expression markers, standard clinical and laboratory variables as well as the mutation status of 68 genes were considered during construction of the model. The final predictor (PS29MRC) consisted of 29 gene expression markers and a cytogenetic risk classification. A continuous predictor is calculated as a weighted linear sum of the individual variables. In addition, a cut off was defined to divide patients into a high-risk and a low-risk group for resistant disease. PS29MRC was highly significant in the validation set, both as a continuous score (OR=2.39, P=8.63·10-9, AUC=0.76) and as a dichotomous classifier (OR=8.03, P=4.29·10-9); accuracy was 77\%. In multivariable models, only TP53 mutation, age and PS29MRC (continuous: OR=1.75, P=0.0011; dichotomous: OR=4.44, P=0.00021) were left as significant variables. PS29MRC dominated all models when compared with currently used predictors, and also predicted overall survival independently of established markers. When integrated into the European LeukemiaNet (ELN) 2017 genetic risk stratification, four groups (median survival of 8, 18, 41 months, and not reached) could be defined (P=4.01·10-10). PS29MRC will make it possible to design trials which stratify induction treatment according to the probability of response, and refines the ELN 2017 classification.},
  langid = {english},
  pmcid = {PMC5830382},
  keywords = {Adolescent,Adult,Aged,Aged; 80 and over,Drug Resistance; Neoplasm,Female,Gene Expression Profiling,Humans,Leukemia; Myeloid; Acute,Machine Learning,Male,Middle Aged,Mutation,Predictive Value of Tests,Prognosis,Randomized Controlled Trials as Topic,Remission Induction,Survival Analysis,Young Adult},
  file = {/Users/hugo/Papers/pdf/herold29geneCytogeneticScore22.pdf}
}

@article{herrmannFastAlgorithmSimulation1986,
  title = {Fast Algorithm for the Simulation of {{Ising}} Models},
  author = {Herrmann, H. J.},
  date = {1986-10-01},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {45},
  number = {1},
  pages = {145--151},
  issn = {1572-9613},
  doi = {10.1007/BF01033083},
  url = {https://doi.org/10.1007/BF01033083},
  urldate = {2022-05-02},
  abstract = {Using a new microcanonical algorithm efficiently vectorized on a Cray XMP, we reach a simulation speed of 1.5 nsec per update of one spin, three times faster than the best previous method known to us. Data for the nonlinear relaxation with conserved energy are presented for the two-dimensional Ising model.},
  langid = {english},
  keywords = {critical dynamics,Ising model,simulation algorithm},
  file = {/Users/hugo/Papers/pdf/herrmannFastAlgorithmSimulation1986.pdf}
}

@article{higginsBetavaeLearningBasic2016,
  title = {Beta-Vae: {{Learning}} Basic Visual Concepts with a Constrained Variational Framework},
  shorttitle = {Beta-Vae},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  date = {2016},
  file = {/Users/hugo/Zotero/storage/EG6EU562/forum.html}
}

@inproceedings{hintonAutoencodersMinimumDescription1993,
  ids = {hintonAutoencodersMinimumDescription},
  title = {Autoencoders, Minimum Description Length and {{Helmholtz}} Free Energy},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Hinton, Geoffrey E. and Zemel, Richard S.},
  date = {1993-11-29},
  series = {{{NIPS}}'93},
  pages = {3--10},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{Denver, Colorado}},
  abstract = {An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
  file = {/Users/hugo/Papers/pdf/hintonAutoencodersMinimumDescription1993.pdf}
}

@article{hintonConnectionistLearningProcedures1989,
  title = {Connectionist Learning Procedures},
  author = {Hinton, Geoffrey E.},
  date = {1989-09},
  journaltitle = {Artificial Intelligence},
  volume = {40},
  number = {1-3},
  pages = {185--234},
  issn = {00043702},
  doi = {10.1016/0004-3702(89)90049-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0004370289900490},
  urldate = {2020-03-05},
  abstract = {A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hintonConnectionistLearningProcedures1989.pdf}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E.},
  date = {2006-07-28},
  journaltitle = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1127647},
  urldate = {2020-03-05},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hintonReducingDimensionalityData2006.pdf}
}

@inproceedings{hintonUsingFastWeights1987,
  title = {Using Fast Weights to Deblur Old Memories},
  booktitle = {Proceedings of the Ninth Annual Conference of the {{Cognitive Science Society}}},
  author = {Hinton, Geoffrey E. and Plaut, David C.},
  date = {1987},
  pages = {177--186},
  file = {/Users/hugo/Papers/pdf/hintonUsingFastWeights1987.pdf;/Users/hugo/Zotero/storage/L95ZXCVI/books.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput.},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  urldate = {2020-10-05},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/Users/hugo/Papers/pdf/hochreiterLongShortTermMemory1997.pdf}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020-12-16},
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2022-10-17},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/hoDenoisingDiffusionProbabilistic2020.pdf}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  date = {2022-03-29},
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.15556},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2022-07-22},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \textbackslash nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \textbackslash chinchilla, that uses the same compute budget as \textbackslash gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. \textbackslash chinchilla uniformly and significantly outperforms \textbackslash Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \textbackslash chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \textbackslash chinchilla reaches a state-of-the-art average accuracy of 67.5\textbackslash\% on the MMLU benchmark, greater than a 7\textbackslash\% improvement over \textbackslash gopher.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/hoffmannTrainingComputeOptimalLarge2022.pdf;/Users/hugo/Zotero/storage/2FGD7YYX/2203.html}
}

@unpublished{hoganKnowledgeGraphs2020,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d' Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  options = {useprefix=true},
  date = {2020-03-04},
  eprint = {2003.02320},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.02320},
  urldate = {2020-03-07},
  abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After a general introduction, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/hoganKnowledgeGraphs2020.pdf}
}

@book{hollandAdaptationNaturalArtificial1992,
  title = {Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence},
  shorttitle = {Adaptation in Natural and Artificial Systems},
  author = {Holland, John H.},
  date = {1992},
  series = {Complex Adaptive Systems},
  edition = {1st MIT Press ed},
  publisher = {{MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-08213-6 978-0-262-58111-0},
  pagetotal = {211},
  keywords = {Adaptation (Biology),Adaptive control systems,Mathematical models}
}

@book{hollandEmergenceChaosOrder2000,
  title = {Emergence: {{From Chaos}} to {{Order}}},
  shorttitle = {Emergence},
  author = {Holland, John H.},
  date = {2000},
  eprint = {VjKtpujRGuAC},
  eprinttype = {googlebooks},
  publisher = {{Oxford University Press}},
  abstract = {In this work, one of today's most innovative thinkers, John H. Holland, explains the theory of emergence, a simple theory that the whole is greater than the sum of its parts. Emergence demonstrates that a small number of rules or laws can generate incredibly complex systems. From the checkers-playing computer that learnt to beat its creator again and again, to a fertilized egg that can program the development of a trillion-cell organism, to the ant colonies that build bridges over chasms and navigate leaf-boats on streams, this book contains wide-ranging implications for science, business, and the arts.},
  isbn = {978-0-19-286211-2},
  langid = {english},
  pagetotal = {276},
  keywords = {Mathematics / Game Theory,Science / Philosophy & Social Aspects,Science / System Theory}
}

@unpublished{hooftCellularAutomatonInterpretation2014,
  title = {The {{Cellular Automaton Interpretation}} of {{Quantum Mechanics}}},
  author = {'t Hooft, Gerard},
  date = {2014-05-07},
  eprint = {1405.1548},
  eprinttype = {arxiv},
  primaryclass = {quant-ph},
  url = {http://arxiv.org/abs/1405.1548},
  urldate = {2019-05-07},
  abstract = {When investigating theories at the tiniest conceivable scales in nature, almost all researchers today revert to the quantum language, accepting the verdict from the Copenhagen doctrine that the only way to describe what is going on will always involve states in Hilbert space, controlled by operator equations. Returning to classical, that is, non quantum mechanical, descriptions will be forever impossible, unless one accepts some extremely contrived theoretical constructions that may or may not reproduce the quantum mechanical phenomena observed in experiments. Dissatisfied, this author investigated how one can look at things differently. This book is an overview of older material, but also contains many new observations and calculations. Quantum mechanics is looked upon as a tool, not as a theory. Examples are displayed of models that are classical in essence, but can be analysed by the use of quantum techniques, and we argue that even the Standard Model, together with gravitational interactions, might be viewed as a quantum mechanical approach to analyse a system that could be classical at its core. We explain how such thoughts can conceivably be reconciled with Bell's theorem, and how the usual objections voiced against the notion of `superdeterminism' can be overcome, at least in principle. Our proposal would eradicate the collapse problem and the measurement problem. Even the existence of an "arrow of time" can perhaps be explained in a more elegant way than usual. Discussions added in v3: the role of the gravitational force, a mathematical physics definition of free will, and an unconventional view on the arrow of time, amongst others.},
  archiveprefix = {arXiv},
  keywords = {Quantum Physics},
  file = {/Users/hugo/Zotero/storage/MXXHX2VT/hooftCellularAutomatonInterpretation2014.pdf;/Users/hugo/Zotero/storage/P3ASUUMQ/1405.html}
}

@unpublished{horibeRegeneratingSoftRobots2021,
  title = {Regenerating {{Soft Robots}} through {{Neural Cellular Automata}}},
  author = {Horibe, Kazuya and Walker, Kathryn and Risi, Sebastian},
  date = {2021-02-07},
  eprint = {2102.02579},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2102.02579},
  urldate = {2021-02-10},
  abstract = {Morphological regeneration is an important feature that highlights the environmental adaptive capacity of biological systems. Lack of this regenerative capacity significantly limits the resilience of machines and the environments they can operate in. To aid in addressing this gap, we develop an approach for simulated soft robots to regrow parts of their morphology when being damaged. Although numerical simulations using soft robots have played an important role in their design, evolving soft robots with regenerative capabilities have so far received comparable little attention. Here we propose a model for soft robots that regenerate through a neural cellular automata. Importantly, this approach only relies on local cell information to regrow damaged components, opening interesting possibilities for physical regenerable soft robots in the future. Our approach allows simulated soft robots that are damaged to partially regenerate their original morphology through local cell interactions alone and regain some of their ability to locomote. These results take a step towards equipping artificial systems with regenerative capacities and could potentially allow for more robust operations in a variety of situations and environments. The code for the experiments in this paper is available at: github.com/KazuyaHoribe/RegeneratingSoftRobots.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  file = {/Users/hugo/Zotero/storage/KFYRNJWS/Horibe et al. - 2021 - Regenerating Soft Robots through Neural Cellular A.pdf}
}

@article{horowitzSpontaneousFinetuningEnvironment2017,
  title = {Spontaneous Fine-Tuning to Environment in Many-Species Chemical Reaction Networks},
  author = {Horowitz, Jordan M. and England, Jeremy L.},
  date = {2017-07-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {114},
  number = {29},
  eprint = {28674005},
  eprinttype = {pmid},
  pages = {7565--7570},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1700617114},
  url = {https://www.pnas.org/content/114/29/7565},
  urldate = {2020-06-29},
  abstract = {A chemical mixture that continually absorbs work from its environment may exhibit steady-state chemical concentrations that deviate from their equilibrium values. Such behavior is particularly interesting in a scenario where the environmental work sources are relatively difficult to access, so that only the proper orchestration of many distinct catalytic actors can power the dissipative flux required to maintain a stable, far-from-equilibrium steady state. In this article, we study the dynamics of an in silico chemical network with random connectivity in an environment that makes strong thermodynamic forcing available only to rare combinations of chemical concentrations. We find that the long-time dynamics of such systems are biased toward states that exhibit a fine-tuned extremization of environmental forcing.},
  langid = {english},
  keywords = {adaptation,chemical reaction networks,energy seeking,nonequilibrium thermodynamics,self-organization},
  file = {/Users/hugo/Papers/pdf/horowitzSpontaneousFinetuningEnvironment2017.pdf;/Users/hugo/Zotero/storage/LIYQVPNY/7565.html}
}

@unpublished{howardSearchingMobileNetV32019,
  title = {Searching for {{MobileNetV3}}},
  author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
  date = {2019-11-20},
  eprint = {1905.02244},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.02244},
  urldate = {2019-12-13},
  abstract = {We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardwareaware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 20\% compared to MobileNetV2. MobileNetV3-Small is 6.6\% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LRASPP is 34\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Zotero/storage/C6LP6BE7/howardSearchingMobileNetV32019.pdf}
}

@article{howisonMorphologicallyProgrammingInteractions2020,
  title = {Morphologically Programming the Interactions of {{V-shaped}} Falling Papers},
  author = {Howison, Toby and Hughes, Josie and Iida, Fumiya},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {359--366},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00306},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00306},
  urldate = {2020-07-27},
  abstract = {The behavioural diversity seen in biological systems is, at the most basic level, driven by interactions between physical materials and their environment. In this context, we investigate the a-life properties of falling paper systems, in which different paper shapes are dropped into free fall and their behaviours observed. These systems have a simple embodiment but highly complex interactions with the environment. Using a synthetic methodology, i.e. understanding by building, we explore how morphology can be used to program certain interactions into the dynamics of a free-falling V-shaped paper. We demonstrate that morphology can encode a stochastic hierarchy of possible behaviours into the system. This hierarchy can be described by a set of conditional switching probabilities and represented in a morphological ‘state machine’. We draw a parallel with developmental processes, showing how these can emerge from interaction with the environment. Next, we demonstrate how Bayesian optimisation can be used to optimise morphology in response to a fitness function, in this case minimizing falling speed. Bayesian optimisation allows us to capture the system stochasticity with minimal sampling. By manipulating non-living raw materials such as paper, we are able to analyse how morphology can be used to control and program interactions with the environment. With this bottom-up approach we ultimately aim to demonstrate principles that turn materials into agents that show non-trivial behaviours comparable to those of living organisms.}
}

@unpublished{huangAdversarialAttacksNeural2017,
  title = {Adversarial {{Attacks}} on {{Neural Network Policies}}},
  author = {Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  date = {2017-02-07},
  eprint = {1702.02284},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1702.02284},
  urldate = {2018-11-27},
  abstract = {Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarialexample attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/huangAdversarialAttacksNeural22.pdf}
}

@article{huangDenselyConnectedConvolutional,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang},
  pages = {9},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/huangDenselyConnectedConvolutional.pdf}
}

@article{huangEnhancingPredictionAcute2018,
  title = {Enhancing the Prediction of Acute Kidney Injury Risk after Percutaneous Coronary Intervention Using Machine Learning Techniques: {{A}} Retrospective Cohort Study},
  shorttitle = {Enhancing the Prediction of Acute Kidney Injury Risk after Percutaneous Coronary Intervention Using Machine Learning Techniques},
  author = {Huang, Chenxi and Murugiah, Karthik and Mahajan, Shiwani and Li, Shu-Xia and Dhruva, Sanket S. and Haimovich, Julian S. and Wang, Yongfei and Schulz, Wade L. and Testani, Jeffrey M. and Wilson, Francis P. and Mena, Carlos I. and Masoudi, Frederick A. and Rumsfeld, John S. and Spertus, John A. and Mortazavi, Bobak J. and Krumholz, Harlan M.},
  date = {2018-11-27},
  journaltitle = {PLOS Medicine},
  shortjournal = {PLOS Medicine},
  volume = {15},
  number = {11},
  pages = {e1002703},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002703},
  url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002703},
  urldate = {2019-04-04},
  abstract = {Background The current acute kidney injury (AKI) risk prediction model for patients undergoing percutaneous coronary intervention (PCI) from the American College of Cardiology (ACC) National Cardiovascular Data Registry (NCDR) employed regression techniques. This study aimed to evaluate whether models using machine learning techniques could significantly improve AKI risk prediction after PCI. Methods and findings We used the same cohort and candidate variables used to develop the current NCDR CathPCI Registry AKI model, including 947,091 patients who underwent PCI procedures between June 1, 2009, and June 30, 2011. The mean age of these patients was 64.8 years, and 32.8\% were women, with a total of 69,826 (7.4\%) AKI events. We replicated the current AKI model as the baseline model and compared it with a series of new models. Temporal validation was performed using data from 970,869 patients undergoing PCIs between July 1, 2016, and March 31, 2017, with a mean age of 65.7 years; 31.9\% were women, and 72,954 (7.5\%) had AKI events. Each model was derived by implementing one of two strategies for preprocessing candidate variables (preselecting and transforming candidate variables or using all candidate variables in their original forms), one of three variable-selection methods (stepwise backward selection, lasso regularization, or permutation-based selection), and one of two methods to model the relationship between variables and outcome (logistic regression or gradient descent boosting). The cohort was divided into different training (70\%) and test (30\%) sets using 100 different random splits, and the performance of the models was evaluated internally in the test sets. The best model, according to the internal evaluation, was derived by using all available candidate variables in their original form, permutation-based variable selection, and gradient descent boosting. Compared with the baseline model that uses 11 variables, the best model used 13 variables and achieved a significantly better area under the receiver operating characteristic curve (AUC) of 0.752 (95\% confidence interval [CI] 0.749–0.754) versus 0.711 (95\% CI 0.708–0.714), a significantly better Brier score of 0.0617 (95\% CI 0.0615–0.0618) versus 0.0636 (95\% CI 0.0634–0.0638), and a better calibration slope of observed versus predicted rate of 1.008 (95\% CI 0.988–1.028) versus 1.036 (95\% CI 1.015–1.056). The best model also had a significantly wider predictive range (25.3\% versus 21.6\%, p {$<$} 0.001) and was more accurate in stratifying AKI risk for patients. Evaluated on a more contemporary CathPCI cohort (July 1, 2015–March 31, 2017), the best model consistently achieved significantly better performance than the baseline model in AUC (0.785 versus 0.753), Brier score (0.0610 versus 0.0627), calibration slope (1.003 versus 1.062), and predictive range (29.4\% versus 26.2\%). The current study does not address implementation for risk calculation at the point of care, and potential challenges include the availability and accessibility of the predictors. Conclusions Machine learning techniques and data-driven approaches resulted in improved prediction of AKI risk after PCI. The results support the potential of these techniques for improving risk prediction models and identification of patients who may benefit from risk-mitigation strategies.},
  langid = {english},
  keywords = {Coronary angioplasty,Diabetes mellitus,Engineering and technology,Forecasting,Kidneys,Machine learning,Permutation,Preprocessing},
  file = {/Users/hugo/Papers/pdf/huangEnhancingPredictionAcute22.pdf;/Users/hugo/Zotero/storage/4B3WTQH2/article.html}
}

@article{huangSelfadaptiveSelfreproductionsCellular2013,
  title = {Self-Adaptive Self-Reproductions in Cellular Automata},
  author = {Huang, Xin and Lee, Jia and Sun, Tian-Hao and Peper, Ferdinand},
  date = {2013-11-15},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {263},
  pages = {11--20},
  issn = {0167-2789},
  doi = {10.1016/j.physd.2013.07.012},
  url = {http://www.sciencedirect.com/science/article/pii/S0167278913002200},
  urldate = {2019-05-28},
  abstract = {Self-reproduction in cellular automata has been extensively studied over the decades, with the aim of exploring the underlying logic structure of reproduction behind living organisms. Most efforts to date, however, are concentrated on how to yield rigid copies of self-replicating structures virtually in the cellular space, whereby even a tiny disturbance occurring around a loop is likely to prevent the loop from completing its reproduction. As with reproduction, self-adaptation is another fundamental feature of biology, which aids in enhancing an organism’s ability of living and reproduction in various environments, via the use of natural sensing and selection. Based upon a natural and greedy-like selection scheme, this paper proposes a novel type of self-reproducing loops that can evolve to gradually adapt their structures to fit the changing environments. Experiments demonstrate that such self-adaptability can actually offer more opportunities for loops to survive and reproduce in a wide variety of regions. In particular, self-adaptation may facilitate the emergence of a remarkable diversity in self-replicating structures, even when the whole reproductions start from a single loop, thereby possibly increasing the maximum occupancy of the cellular space by the population of all loops.},
  keywords = {Asynchronous updating,Cellular automaton,Diversity,Self-adaptation,Self-reproduction}
}

@inproceedings{hudcovaClassificationComplexSystems2020,
  title = {Classification of {{Complex Systems Based}} on {{Transients}}},
  author = {Hudcová, Barbora and Mikolov, Tomáš},
  date = {2020-07-01},
  pages = {367--375},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00260},
  url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00260/98499/Classification-of-Complex-Systems-Based-on},
  urldate = {2022-10-25},
  eventtitle = {{{ALIFE}} 2020: {{The}} 2020 {{Conference}} on {{Artificial Life}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hudcovaClassificationComplexSystems2020.pdf;/Users/hugo/Zotero/storage/4KWUB2R9/98499.html}
}

@inproceedings{hudcovaComputationalHierarchyElementary2021,
  title = {Computational {{Hierarchy}} of {{Elementary Cellular Automata}}},
  author = {Hudcová, Barbora and Mikolov, Tomáš},
  date = {2021-07-19},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00447},
  url = {https://direct.mit.edu/isal/article/doi/10.1162/isal_a_00447/102949/Computational-Hierarchy-of-Elementary-Cellular},
  urldate = {2022-03-14},
  eventtitle = {{{ALIFE}} 2021: {{The}} 2021 {{Conference}} on {{Artificial Life}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/hudcovaComputationalHierarchyElementary2021.pdf}
}

@unpublished{huDrinkingFirehoseContinual2020,
  title = {Drinking from a {{Firehose}}: {{Continual Learning}} with {{Web-scale Natural Language}}},
  shorttitle = {Drinking from a {{Firehose}}},
  author = {Hu, Hexiang and Sener, Ozan and Sha, Fei and Koltun, Vladlen},
  date = {2020-11-01},
  eprint = {2007.09335},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.09335},
  urldate = {2020-11-26},
  abstract = {Continual learning systems will interact with humans, with each other, and with the physical world through time – and continue to learn and adapt as they do. An important open problem for continual learning is a large-scale benchmark which enable realistic evaluation of algorithms. In this paper, we study a natural setting for continual learning on a massive scale. We introduce the problem of personalized online language learning (POLL), which involves fitting personalized language models to a population of users that evolves over time. To facilitate research on POLL, we collect massive datasets of Twitter posts. These datasets, Firehose10M and Firehose100M, comprise 100 million tweets, posted by one million users over six years. Enabled by the Firehose datasets, we present a rigorous evaluation of continual learning algorithms on an unprecedented scale. Based on this analysis, we develop a simple algorithm for continual gradient descent (ConGraD) that outperforms prior continual learning methods on the Firehose datasets as well as earlier benchmarks. Collectively, the POLL problem setting, the Firehose datasets, and the ConGraD algorithm enable a complete benchmark for reproducible research on web-scale continual learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/LC4Q5K38/Hu et al. - 2020 - Drinking from a Firehose Continual Learning with .pdf}
}

@article{huttonEvolvableSelfReproducingCells2007,
  title = {Evolvable {{Self-Reproducing Cells}} in a {{Two-Dimensional Artificial Chemistry}}},
  author = {Hutton, Tim J.},
  date = {2007-01},
  journaltitle = {Artificial Life},
  shortjournal = {Artificial Life},
  volume = {13},
  number = {1},
  pages = {11--30},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/artl.2007.13.1.11},
  url = {https://direct.mit.edu/artl/article/13/1/11-30/2549},
  urldate = {2022-11-08},
  abstract = {We present a novel unit of evolution: a self-reproducing cell in a two-dimensional artificial chemistry. The cells have a strip of genetic material that is used to produce enzymes, each catalyzing a specific reaction that may affect the survival of the cell. The enzymes are kept inside the cell by a loop of membrane, thus ensuring that only the cell that produced them gets their benefit. A set of reaction rules, each simple and local, allows the cells to copy their genetic information and physically divide. The evolutionary possibilities of the cells are explored, and it is suggested that the system provides a useful framework for testing hypotheses about self-driven evolution.},
  langid = {english}
}

@article{icWikidataFreeCollaborative2014,
  title = {Wikidata: {{A Free Collaborative Knowledge Base}}},
  author = {Ic, Denny Vrandec and Krötzsch, Markus},
  date = {2014},
  pages = {7},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/icWikidataFreeCollaborative22.pdf}
}

@online{IgblanLifeUniversal,
  title = {Igblan - {{Life Universal Computer}}},
  url = {http://www.igblan.free-online.co.uk/igblan/ca/},
  urldate = {2022-11-02},
  file = {/Users/hugo/Zotero/storage/DQGDQDPB/ca.html}
}

@article{iiFractalRecurrentBehavior,
  title = {Fractal and {{Recurrent Behavior}} of {{Cellular Automata}}},
  author = {Ii, Karel Culik},
  pages = {15},
  abstract = {In recent years, cellular automata eCA) have been found capable of producing complex behavior. Some examples of cellular automata show remarkably regular behavior on finite configurations. On simple initial configurations, the generated pattern might be fractal or selfsimilar. In this paper, regular evolution of totalistic linear CA is investigated. In particular, it is shown that additive CA will always produce a highly regular behavior on an arbitrary finite configuration faosrmtheoni1ni2tmiaolnseaered.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/iiFractalRecurrentBehavior2.pdf}
}

@software{IncubatorairflowApacheAirflow2018,
  title = {Incubator-Airflow: {{Apache Airflow}} ({{Incubating}})},
  shorttitle = {Incubator-Airflow},
  date = {2018-06-19T13:06:55Z},
  origdate = {2015-04-13T18:04:58Z},
  url = {https://github.com/apache/incubator-airflow},
  urldate = {2018-06-19},
  organization = {{The Apache Software Foundation}}
}

@article{ingolfssonPowerCoarseGraining2014,
  title = {The Power of Coarse Graining in Biomolecular Simulations},
  shorttitle = {The Power of Coarse Graining in Biomolecular Simulations},
  author = {Ingólfsson, Helgi I. and Lopez, Cesar A. and Uusitalo, Jaakko J. and de Jong, Djurre H. and Gopal, Srinivasa M. and Periole, Xavier and Marrink, Siewert J.},
  options = {useprefix=true},
  date = {2014-05},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume = {4},
  number = {3},
  pages = {225--248},
  issn = {17590876},
  doi = {10.1002/wcms.1169},
  url = {http://doi.wiley.com/10.1002/wcms.1169},
  urldate = {2020-03-06},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/ingolfssonPowerCoarseGraining2014.pdf}
}

@online{InternationalConsortiumInvestigative,
  title = {International {{Consortium}} of {{Investigative Journalists}}},
  url = {https://www.icij.org/},
  urldate = {2018-06-07},
  abstract = {The ICIJ is a global network of more than 200 investigative journalists in 70 countries who collaborate on in-depth investigative stories.},
  langid = {american},
  organization = {{ICIJ}},
  file = {/Users/hugo/Zotero/storage/G3W473A7/www.icij.org.html}
}

@unpublished{irieModernSelfReferentialWeight2022,
  title = {A {{Modern Self-Referential Weight Matrix That Learns}} to {{Modify Itself}}},
  author = {Irie, Kazuki and Schlag, Imanol and Csordás, Róbert and Schmidhuber, Jürgen},
  date = {2022-02-11},
  eprint = {2202.05780},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.05780},
  urldate = {2022-04-21},
  abstract = {The weight matrix (WM) of a neural network (NN) is its program. The programs of many traditional NNs are learned through gradient descent in some error function, then remain fixed. The WM of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs can meta-learn to learn, and metameta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While NN architectures potentially capable of implementing such behavior have been proposed since the ’90s, there have been few if any practical studies. Here we revisit such NNs, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential WM (SRWM) that uses outer products and the delta update rule to modify itself. We evaluate our SRWM in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed SRWM. Our code is public†.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/irieModernSelfReferentialWeight2022.pdf}
}

@article{ishiguroNeuromodulatedControlBipedal2003,
  title = {Neuromodulated {{Control}} of {{Bipedal Locomotion Using}} a {{Polymorphic CPG Circuit}}},
  author = {Ishiguro, Akio and Fujii, Akinobu and Hotz, Peter Eggenberger},
  date = {2003-03-01},
  journaltitle = {Adaptive Behavior},
  shortjournal = {Adaptive Behavior},
  volume = {11},
  number = {1},
  pages = {7--17},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {1059-7123},
  doi = {10.1177/10597123030111001},
  url = {https://doi.org/10.1177/10597123030111001},
  urldate = {2020-06-15},
  abstract = {To date, various methods using the concept of neural circuit or so-called central pattern generators (CPGs) have been proposed to create agile locomotion for legged robots. In contrast to these approaches, in this article we propose a polymorphic neural circuit that allows the dynamic change of its properties according to the current situation in real time to be employed instead. To this end, the concept of neuromodulation is introduced. To verify the feasibility of this approach, this concept is applied to the control of a three-dimensional biped robot that is intrinsically unstable. The importance of an adaptive controller is illustrated with the simulations of biped walking on uneven terrain, and the results show that the biped robot successfully copes with environmental perturbation by dynamically changing the torque outputs applied to the joints. Furthermore, the proposed approach outperforms a monolithic CPG model with sensory feedback.},
  langid = {english}
}

@inproceedings{isolaImagetoImageTranslationConditional2017,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2017},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.07004},
  urldate = {2018-11-30},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  eventtitle = {{{CVPR}}},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/isolaImagetoImageTranslationConditional22.pdf;/Users/hugo/Papers/pdf/isolaImagetoImageTranslationConditional3.pdf;/Users/hugo/Zotero/storage/2QL693ZD/1611.html}
}

@article{israeliCoarsegrainingCellularAutomata2006,
  title = {Coarse-Graining of Cellular Automata, Emergence, and the Predictability of Complex Systems},
  author = {Israeli, Navot and Goldenfeld, Nigel},
  date = {2006-02-06},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {73},
  number = {2},
  eprint = {nlin/0508033},
  eprinttype = {arxiv},
  pages = {026203},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.73.026203},
  url = {http://arxiv.org/abs/nlin/0508033},
  urldate = {2020-03-05},
  abstract = {We study the predictability of emergent phenomena in complex systems. Using nearest neighbor, one-dimensional Cellular Automata (CA) as an example, we show how to construct local coarse-grained descriptions of CA in all classes of Wolfram's classification. The resulting coarse-grained CA that we construct are capable of emulating the large-scale behavior of the original systems without accounting for small-scale details. Several CA that can be coarse-grained by this construction are known to be universal Turing machines; they can emulate any CA or other computing devices and are therefore undecidable. We thus show that because in practice one only seeks coarse-grained information, complex physical systems can be predictable and even decidable at some level of description. The renormalization group flows that we construct induce a hierarchy of CA rules. This hierarchy agrees well with apparent rule complexity and is therefore a good candidate for a complexity measure and a classification method. Finally we argue that the large scale dynamics of CA can be very simple, at least when measured by the Kolmogorov complexity of the large scale update rule, and moreover exhibits a novel scaling law. We show that because of this large-scale simplicity, the probability of finding a coarse-grained description of CA approaches unity as one goes to increasingly coarser scales. We interpret this large scale simplicity as a pattern formation mechanism in which large scale patterns are forced upon the system by the simplicity of the rules that govern the large scale dynamics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Cellular Automata and Lattice Gases,Nonlinear Sciences - Pattern Formation and Solitons},
  file = {/Users/hugo/Papers/pdf/israeliCoarsegrainingCellularAutomata2006.pdf}
}

@article{israeliComputationalIrreducibilityPredictability2004,
  title = {On Computational Irreducibility and the Predictability of Complex Physical Systems},
  author = {Israeli, Navot and Goldenfeld, Nigel},
  date = {2004-02-20},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {92},
  number = {7},
  eprint = {nlin/0309047},
  eprinttype = {arxiv},
  pages = {074105},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.92.074105},
  url = {http://arxiv.org/abs/nlin/0309047},
  urldate = {2020-03-05},
  abstract = {Using elementary cellular automata (CA) as an example, we show how to coarse-grain CA in all classes of Wolfram's classification. We find that computationally irreducible (CIR) physical processes can be predictable and even computationally reducible at a coarse-grained level of description. The resulting coarse-grained CA which we construct emulate the large-scale behavior of the original systems without accounting for small-scale details. At least one of the CA that can be coarse-grained is irreducible and known to be a universal Turing machine.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/israeliComputationalIrreducibilityPredictability2004.pdf}
}

@article{jacksonEpiphenomenalQualia1982,
  title = {Epiphenomenal {{Qualia}}},
  author = {Jackson, Frank},
  date = {1982-04},
  journaltitle = {The Philosophical Quarterly},
  shortjournal = {The Philosophical Quarterly},
  volume = {32},
  number = {127},
  pages = {127},
  issn = {00318094},
  doi = {10.2307/2960077},
  url = {https://academic.oup.com/pq/article-lookup/doi/10.2307/2960077},
  urldate = {2022-04-26},
  file = {/Users/hugo/Papers/pdf/jacksonEpiphenomenalQualia1982.pdf}
}

@article{jaegerEchoStateApproach2001,
  title = {The “Echo State” Approach to Analysing and Training Recurrent Neural Networks-with an Erratum Note},
  author = {Jaeger, Herbert},
  date = {2001},
  journaltitle = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  volume = {148},
  number = {34},
  pages = {13},
  publisher = {{Bonn}},
  file = {/Users/hugo/Papers/pdf/jaegerEchoStateApproach2001.pdf}
}

@article{jaegerHarnessingNonlinearityPredicting2004,
  title = {Harnessing {{Nonlinearity}}: {{Predicting Chaotic Systems}} and {{Saving Energy}} in {{Wireless Communication}}},
  shorttitle = {Harnessing {{Nonlinearity}}},
  author = {Jaeger, H.},
  date = {2004-04-02},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {304},
  number = {5667},
  pages = {78--80},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1091277},
  url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1091277},
  urldate = {2020-10-02},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jaegerHarnessingNonlinearityPredicting2004.pdf}
}

@report{jaegerLongShortTermMemory2012,
  title = {Long {{Short-Term Memory}} in {{Echo State Networks}}: {{Details}} of a {{Simulation Study}}},
  shorttitle = {Long {{Short-Term Memory}} in {{Echo State Networks}}},
  author = {Jaeger, Herbert},
  date = {2012-02-01},
  institution = {{Jacobs University Bremen}},
  abstract = {Echo State Networks (ESNs) is an approach to design and train recurrent neural networks in supervised learning tasks. An important objective in many such tasks is to learn to exploit long-time dependencies in the processed signals ("long short-term memory" performance). Here we expose ESNs to a series of synthetic benchmark tasks that have been used in the literature to study the learnability of long-range temporal dependencies. This report provides all the detail necessary to replicate these experiments. It is intended to serve as the technical companion to a journal submission paper where the findings are analysed and compared to results obtained elsewhere with other learning paradigms.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jaegerLongShortTermMemory2012.pdf;/Users/hugo/Zotero/storage/FSWQAVUD/638.html}
}

@article{jaegerSpecialIssueEcho2007,
  title = {Special Issue on Echo State Networks and Liquid State Machines},
  author = {Jaeger, Herbert and Maass, Wolfgang and Principe, Jose},
  date = {2007-04},
  journaltitle = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {287--289},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608007000305},
  urldate = {2020-10-02},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jaegerSpecialIssueEcho2007.pdf}
}

@book{jaegerTutorialTrainingRecurrent2002,
  title = {Tutorial on Training Recurrent Neural Networks, Covering {{BPPT}}, {{RTRL}}, {{EKF}} and the" Echo State Network" Approach},
  author = {Jaeger, Herbert},
  date = {2002},
  volume = {5},
  publisher = {{GMD-Forschungszentrum Informationstechnik Bonn}},
  file = {/Users/hugo/Papers/pdf/jaegerTutorialTrainingRecurrent2002.pdf}
}

@inproceedings{jaggiRevisitingFrankWolfeProjectionFree2013,
  title = {Revisiting {{Frank-Wolfe}}: {{Projection-Free Sparse Convex Optimization}}},
  shorttitle = {Revisiting {{Frank-Wolfe}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2013, {{Atlanta}}, {{GA}}, {{USA}}, 16-21 {{June}} 2013},
  author = {Jaggi, Martin},
  date = {2013},
  series = {{{JMLR Workshop}} and {{Conference Proceedings}}},
  volume = {28},
  pages = {427--435},
  publisher = {{JMLR.org}},
  url = {http://proceedings.mlr.press/v28/jaggi13.html},
  urldate = {2022-08-30}
}

@misc{jannerOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} as {{One Big Sequence Modeling Problem}}},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  date = {2021-11-28},
  number = {arXiv:2106.02039},
  eprint = {2106.02039},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.02039},
  url = {http://arxiv.org/abs/2106.02039},
  urldate = {2022-07-27},
  abstract = {Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/jannerOfflineReinforcementLearning2021.pdf;/Users/hugo/Zotero/storage/TQ7LVZ5Q/2106.html}
}

@unpublished{jannerReinforcementLearningOne2021,
  title = {Reinforcement {{Learning}} as {{One Big Sequence Modeling Problem}}},
  author = {Janner, Michael and Li, Qiyang and Levine, Sergey},
  date = {2021-06-03},
  eprint = {2106.02039},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.02039},
  urldate = {2021-06-14},
  abstract = {Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as "one big sequence modeling" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/jannerReinforcementLearningOne2021.pdf;/Users/hugo/Zotero/storage/YXYPAUNS/2106.html}
}

@incollection{jegouHammingEmbeddingWeak2008,
  title = {Hamming {{Embedding}} and {{Weak Geometric Consistency}} for {{Large Scale Image Search}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2008},
  author = {Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  date = {2008},
  volume = {5302},
  pages = {304--317},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-88682-2_24},
  url = {http://link.springer.com/10.1007/978-3-540-88682-2_24},
  urldate = {2019-01-02},
  abstract = {This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the suboptimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.},
  isbn = {978-3-540-88681-5 978-3-540-88682-2},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jegouHammingEmbeddingWeak22.pdf}
}

@inproceedings{jenningsAgentBasedComputingPromise1999,
  title = {Agent-{{Based Computing}}: {{Promise}} and {{Perils}}},
  shorttitle = {Agent-{{Based Computing}}},
  author = {Jennings, N. R.},
  date = {1999},
  pages = {1429--1436},
  issn = {1429-1436},
  url = {https://eprints.soton.ac.uk/252172/},
  urldate = {2022-11-08},
  abstract = {Agent-based computing represents an exciting new synthesis both for Artificial Intelligence (AI) and, more generally, Computer Science. It has the potential to significantly improve the theory and practice of modelling, designing and implementing complex systems. Yet, to date, there has been little systematic analysis of what makes an agent such an appealing and powerful conceptual model. Moreover, even less effort has been devoted to exploring the inherent disadvantages that stem from adopting an agent-oriented view. Here both sets of issues are explored. The standpoint of this analysis is the role of agent-based software in solving complex, real-world problems. In particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level social interactions, and that can operate within flexible organisational structures.},
  editora = {Jennings, N. R.},
  editoratype = {collaborator},
  eventtitle = {16th {{Int}}. {{Joint Conf}}. on {{Artificial Intelligence}} ({{IJCAI-99}}) (01/01/99)},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jenningsAgentBasedComputingPromise1999.pdf;/Users/hugo/Zotero/storage/56LDI674/252172.html}
}

@article{jensenReservoirComputingArtificial2020,
  title = {Reservoir {{Computing}} in {{Artificial Spin Ice}}},
  author = {Jensen, Johannes H. and Tufte, Gunnar},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {376--383},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00268},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00268},
  urldate = {2020-07-27},
  abstract = {Artificial spin ice (ASI) are systems of coupled nanomagnets arranged on a 2D lattice. ASIs are promising computing substrates due to the rich variety of emergent behavior, accompanied by considerable control and flexibility. Computational models may exploit the small-scale dynamics of the individual elements, or large-scale emergent behavior of the resulting metamaterial. We investigate the computational capabilities of “pinwheel” ASI, whose emergent ferromagnetic patterns can be observed at different scales. Within a reservoir computing framework, we examine how key system parameters affect performance using well-established reservoir quality metrics. As reservoir output, we consider system state at different granularities, ranging from individual magnets to the collective state of multiple magnets. Our results show that pinwheel ASI exhibits excellent computing capacity, including evidence of fading memory. Interestingly, a wide range of output granularities result in good performance, offering new insights into the scalability and robustness of reservoirs based on self-organized collective behavior. The apparent flexibility in output granularity show that ASIs have computational properties at different abstraction levels, from the small-scale dynamics of simple elements, to the large-scale spatial patterns of the metamaterial.},
  file = {/Users/hugo/Papers/pdf/jensenReservoirComputingArtificial2020.pdf;/Users/hugo/Zotero/storage/7YWQTJVL/isal_a_00268.html}
}

@inproceedings{jiangLearningRefineAutomatically2012,
  title = {Learning to {{Refine}} an {{Automatically Extracted Knowledge Base Using Markov Logic}}},
  author = {Jiang, Shangpu and Lowd, Daniel and Dou, Dejing},
  date = {2012-12},
  pages = {912--917},
  publisher = {{IEEE}},
  doi = {10.1109/ICDM.2012.156},
  url = {http://ieeexplore.ieee.org/document/6413833/},
  urldate = {2018-04-24},
  abstract = {A number of text mining and information extraction projects such as TextRunner and NELL seek to automatically build knowledge bases from the rapidly growing amount of information on the web. In order to scale to the size of the web, these projects often employ ad hoc heuristics to reason about uncertain and contradictory information rather than reasoning jointly about all candidate facts. In this paper, we present a Markov logic-based system for cleaning an extracted knowledge base. This allows a scalable system such as NELL to take advantage of joint probabilistic inference, or, conversely, allows Markov logic to be applied to a web scale problem. Our system uses only the ontological constraints and confidence values of the original system, along with humanlabeled data if available. The labeled data can be used to calibrate the confidence scores from the original system or learn the effectiveness of individual extraction patterns. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL’s knowledge base, we evaluate several variants of our approach and find that they improve both F1 and area under the precision-recall curve.},
  isbn = {978-1-4673-4649-8 978-0-7695-4905-7},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/jiangLearningRefineAutomatically22.pdf}
}

@inproceedings{jinBERTReallyRobust2020,
  title = {Is {{BERT Really Robust}}? {{A Strong Baseline}} for {{Natural Language Attack}} on {{Text Classification}} and {{Entailment}}},
  shorttitle = {Is {{BERT Really Robust}}?},
  booktitle = {The {{Thirty-Fourth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2020, {{The Thirty-Second Innovative Applications}} of {{Artificial Intelligence Conference}}, {{IAAI}} 2020, {{The Tenth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2020, {{New York}}, {{NY}}, {{USA}}, {{February}} 7-12, 2020},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  date = {2020},
  pages = {8018--8025},
  publisher = {{AAAI Press}},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/6311},
  urldate = {2022-08-02},
  file = {/Users/hugo/Papers/pdf/jinBERTReallyRobust2020.pdf}
}

@unpublished{johnsonLearningGraphStructure2020,
  title = {Learning {{Graph Structure With A Finite-State Automaton Layer}}},
  author = {Johnson, Daniel D. and Larochelle, Hugo and Tarlow, Daniel},
  date = {2020-07-09},
  eprint = {2007.04929},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.04929},
  urldate = {2020-10-13},
  abstract = {Graph-based neural network models are producing strong results in a number of domains, in part because graphs provide flexibility to encode domain knowledge in the form of relational structure (edges) between nodes in the graph. In practice, edges are used both to represent intrinsic structure (e.g., abstract syntax trees of programs) and more abstract relations that aid reasoning for a downstream task (e.g., results of relevant program analyses). In this work, we study the problem of learning to derive abstract relations from the intrinsic graph structure. Motivated by their power in program analyses, we consider relations defined by paths on the base graph accepted by a finite-state automaton. We show how to learn these relations end-to-end by relaxing the problem into learning finite-state automata policies on a graph-based POMDP and then training these policies using implicit differentiation. The result is a differentiable Graph Finite-State Automaton (GFSA) layer that adds a new edge type (expressed as a weighted adjacency matrix) to a base graph. We demonstrate that this layer can find shortcuts in grid-world graphs and reproduce simple static analyses on Python programs. Additionally, we combine the GFSA layer with a larger graph-based model trained end-to-end on the variable misuse program understanding task, and find that using the GFSA layer leads to better performance than using hand-engineered semantic edges or other baseline methods for adding learned edge types.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/P844YXVR/Johnson et al. - 2020 - Learning Graph Structure With A Finite-State Autom.pdf}
}

@book{johnsonSimplyComplexityClear2009,
  title = {Simply Complexity: A Clear Guide to Complexity Theory},
  shorttitle = {Simply Complexity},
  author = {Johnson, Neil F. and Johnson, Neil F.},
  date = {2009},
  publisher = {{Oneworld}},
  location = {{Oxford}},
  isbn = {978-1-85168-630-8},
  pagetotal = {236},
  keywords = {Chaotic behavior in systems,Complexity (Philosophy)},
  annotation = {OCLC: ocn647072479}
}

@article{jonasCouldNeuroscientistUnderstand2017,
  title = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author = {Jonas, Eric and Kording, Konrad Paul},
  date = {2017-01-12},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {13},
  number = {1},
  pages = {e1005268},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005268},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268},
  urldate = {2022-08-03},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  langid = {english},
  keywords = {Behavior,Behavioral neuroscience,Computational neuroscience,Connectomics,Microprocessors,Neuronal tuning,Neurons,Neuroscience},
  file = {/Users/hugo/Papers/pdf/jonasCouldNeuroscientistUnderstand2017.pdf;/Users/hugo/Zotero/storage/3TIMP9GD/article.html}
}

@incollection{jordanSerialOrderParallel1997,
  title = {Serial {{Order}}: {{A Parallel Distributed Processing Approach}}},
  shorttitle = {Serial {{Order}}},
  booktitle = {Advances in {{Psychology}}},
  author = {Jordan, Michael I.},
  date = {1997},
  volume = {121},
  pages = {471--495},
  publisher = {{Elsevier}},
  doi = {10.1016/S0166-4115(97)80111-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166411597801112},
  urldate = {2022-11-15},
  isbn = {978-0-444-81931-4},
  langid = {english}
}

@unpublished{joulinBagTricksEfficient2016,
  title = {Bag of {{Tricks}} for {{Efficient Text Classification}}},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  date = {2016-07-06},
  eprint = {1607.01759},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1607.01759},
  urldate = {2019-01-17},
  abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/joulinBagTricksEfficient22.pdf}
}

@unpublished{joulinFastTextZipCompressing2016,
  title = {{{FastText}}.Zip: {{Compressing}} Text Classification Models},
  shorttitle = {{{FastText}}.Zip},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and Jégou, Hérve and Mikolov, Tomas},
  date = {2016-12-12},
  eprint = {1612.03651},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1612.03651},
  urldate = {2019-01-28},
  abstract = {We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/joulinFastTextZipCompressing22.pdf}
}

@inproceedings{joulinInferringAlgorithmicPatterns2015,
  title = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Joulin, Armand and Mikolov, Tomas},
  date = {2015-12-07},
  series = {{{NIPS}}'15},
  pages = {190--198},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
  file = {/Users/hugo/Papers/pdf/joulinInferringAlgorithmicPatterns22.pdf}
}

@inproceedings{justesenMAPElitesNoisyDomains2019,
  title = {{{MAP-Elites}} for Noisy Domains by Adaptive Sampling},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Justesen, Niels and Risi, Sebastian and Mouret, Jean-Baptiste},
  date = {2019-07-13},
  series = {{{GECCO}} '19},
  pages = {121--122},
  publisher = {{Association for Computing Machinery}},
  location = {{Prague, Czech Republic}},
  doi = {10.1145/3319619.3321904},
  url = {https://doi.org/10.1145/3319619.3321904},
  urldate = {2020-07-27},
  abstract = {Quality Diversity algorithms (QD) evolve a set of high-performing phenotypes that each behaves as differently as possible. However, current algorithms are all elitist, which make them unable to cope with stochastic fitness functions and behavior evaluations. In fact, many of the promising applications of QD algorithms, for instance, games and robotics, are stochastic. Here we propose two new extensions to the QD-algorithm MAP-Elites --- adaptive sampling and drifting-elites --- and demonstrate empirically that these extensions increase the quality of solutions in a noisy artificial test function and the behavioral diversity in a 2D bipedal walker environment.},
  isbn = {978-1-4503-6748-6}
}

@article{kadishAdaptingChangingEnvironment2020,
  title = {Adapting to a Changing Environment: {{Simulating}} the Effects of Noise on Animal Sonification},
  shorttitle = {Adapting to a Changing Environment},
  author = {Kadish, David and Risi, Sebastian},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {687--695},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00320},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00320},
  urldate = {2020-07-27},
  abstract = {Adaptation is an important capability in a fast-changing world. What factors allow an animal population to adapt to external changes in their environments? What effects do those changes have on the animal populations that do adapt? This paper explores these questions in the context of intraspecies communication in a noisy soundscape. Using a simulated soundscape and populations generated using Neuroevolution of Augmenting Topologies (NEAT), the same scenario is played through many times to understand the range of possible outcomes given an initial population and a set of noise conditions. While noise is found to have minimal effect on the best possible scenario, it affects how often that scenario is reached. The onset of noise is also found to impact the complexity of the evolved neural networks.}
}

@article{kampsConnectivityOriginsDomain2020,
  title = {Connectivity at the Origins of Domain Specificity in the Cortical Face and Place Networks},
  author = {Kamps, Frederik S. and Hendrix, Cassandra L. and Brennan, Patricia A. and Dilks, Daniel D.},
  date = {2020-03-02},
  journaltitle = {Proceedings of the National Academy of Sciences},
  pages = {201911359},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1911359117},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1911359117},
  urldate = {2020-03-03},
  langid = {english},
  file = {/home/hugo/Papers/pdf/kampsConnectivityOriginsDomain2020.pdf}
}

@article{kanazawaGeneralIntelligenceDomainspecific2004,
  title = {General Intelligence as a Domain-Specific Adaptation.},
  author = {Kanazawa, Satoshi},
  date = {2004},
  journaltitle = {Psychological review},
  volume = {111},
  number = {2},
  pages = {512},
  publisher = {{American Psychological Association}},
  file = {/Users/hugo/Papers/pdf/kanazawaGeneralIntelligenceDomainspecific2004.pdf;/Users/hugo/Zotero/storage/S7FXKD8B/2004-12248-010.html}
}

@article{kanoAdaptiveOnedimensionalCrawling2020,
  title = {Adaptive {{One-dimensional Crawling Robot Driven}} by {{Simple Decentralized Control Mechanism}}},
  author = {Kano, Takeshi and Senofieni, Rodrigo and Fukuhara, Akira and Ishiguro, Akio},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {696--698},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00250},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00250},
  urldate = {2020-07-27},
  abstract = {One-dimensional crawling locomotion, which is employed by creatures such as earthworms and slugs, has garnered the attention of roboticists because it can be used to negotiate narrow spaces for propulsion. However, the previously developed robots are not adaptive enough to the environment. To address this problem, we have recently proposed a simple decentralized control scheme, and showed through simulations that it can adapt to the inclination angle of the terrain. In this study, we demonstrate that a one-dimensional crawling robot that implements a control scheme slightly modified from the previous one can move adaptively in the real world.}
}

@unpublished{kasaiFinetuningPretrainedTransformers2021,
  title = {Finetuning {{Pretrained Transformers}} into {{RNNs}}},
  author = {Kasai, Jungo and Peng, Hao and Zhang, Yizhe and Yogatama, Dani and Ilharco, Gabriel and Pappas, Nikolaos and Mao, Yi and Chen, Weizhu and Smith, Noah A.},
  date = {2021-03-24},
  eprint = {2103.13076},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.13076},
  urldate = {2021-05-26},
  abstract = {Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. This comes with a significant computational overhead, as the attention mechanism scales with a quadratic complexity in sequence length. Efficient transformer variants have received increasing interest from recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train or yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving the efficiency while retaining the accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process needs lower training cost than training these recurrent variants from scratch. As many recent models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Zotero/storage/G3YKADJX/Kasai et al. - 2021 - Finetuning Pretrained Transformers into RNNs.pdf}
}

@unpublished{katharopoulosTransformersAreRNNs2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
  date = {2020-06-29},
  eprint = {2006.16236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.16236},
  urldate = {2020-06-30},
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/katharopoulosTransformersAreRNNs2020.pdf}
}

@book{kauffmanHomeUniverseSearch1995,
  title = {At {{Home}} in the {{Universe}}: {{The Search}} for {{Laws}} of {{Self-organization}} and {{Complexity}}},
  shorttitle = {At {{Home}} in the {{Universe}}},
  author = {Kauffman, Stuart and Kauffman, Stuart A. and Kauffman, Professor of Biochemistry {and} Biophysics Stuart},
  date = {1995},
  publisher = {{Oxford University Press}},
  abstract = {A major scientific revolution has begun, a new paradigm that rivals Darwin's theory in importance. At its heart is the discovery of the order that lies deep within the most complex of systems, from the origin of life, to the workings of giant corporations, to the rise and fall of great civilizations. And more than anyone else, this revolution is the work of one man, Stuart Kauffman, a MacArthur Fellow and visionary pioneer of the new science of complexity. Now, in At Home in the Universe, Kauffman brilliantly weaves together the excitement of intellectual discovery and a fertile mix of insights to give the general reader a fascinating look at this new science--and at the forces for order that lie at the edge of chaos. We all know of instances of spontaneous order in nature--an oil droplet in water forms a sphere, snowflakes have a six-fold symmetry. What we are only now discovering, Kauffman says, is that the range of spontaneous order is enormously greater than we had supposed. Indeed, self-organization is a great undiscovered principle of nature. But how does this spontaneous order arise? Kauffman contends that complexity itself triggers self-organization, or what he calls "order for free," that if enough different molecules pass a certain threshold of complexity, they begin to self-organize into a new entity--a living cell. Kauffman uses the analogy of a thousand buttons on a rug--join two buttons randomly with thread, then another two, and so on. At first, you have isolated pairs; later, small clusters; but suddenly at around the 500th repetition, a remarkable transformation occurs--much like the phase transition when water abruptly turns to ice--and the buttons link up in one giant network. Likewise, life may have originated when the mix of different molecules in the primordial soup passed a certain level of complexity and self-organized into living entities (if so, then life is not a highly improbable chance event, but almost inevitable). Kauffman uses the basic insight of "order for free" to illuminate a staggering range of phenomena. We see how a single-celled embryo can grow to a highly complex organism with over two hundred different cell types. We learn how the science of complexity extends Darwin's theory of evolution by natural selection: that self-organization, selection, and chance are the engines of the biosphere. And we gain insights into biotechnology, the stunning magic of the new frontier of genetic engineering--generating trillions of novel molecules to find new drugs, vaccines, enzymes, biosensors, and more. Indeed, Kauffman shows that ecosystems, economic systems, and even cultural systems may all evolve according to similar general laws, that tissues and terra cotta evolve in similar ways. And finally, there is a profoundly spiritual element to Kauffman's thought. If, as he argues, life were bound to arise, not as an incalculably improbable accident, but as an expected fulfillment of the natural order, then we truly are at home in the universe. Kauffman's earlier volume, The Origins of Order, written for specialists, received lavish praise. Stephen Jay Gould called it "a landmark and a classic." And Nobel Laureate Philip Anderson wrote that "there are few people in this world who ever ask the right questions of science, and they are the ones who affect its future most profoundly. Stuart Kauffman is one of these." In At Home in the Universe, this visionary thinker takes you along as he explores new insights into the nature of life.},
  isbn = {978-0-19-509599-9},
  langid = {english},
  pagetotal = {348},
  keywords = {History / United States / 19th Century}
}

@unpublished{kaushikUnderstandingCatastrophicForgetting2021,
  title = {Understanding {{Catastrophic Forgetting}} and {{Remembering}} in {{Continual Learning}} with {{Optimal Relevance Mapping}}},
  author = {Kaushik, Prakhar and Gain, Alex and Kortylewski, Adam and Yuille, Alan},
  date = {2021-02-22},
  eprint = {2102.11343},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.11343},
  urldate = {2021-10-18},
  abstract = {Catastrophic forgetting in neural networks is a significant problem for continual learning. A majority of the current methods replay previous data during training, which violates the constraints of an ideal continual learning system. Additionally, current approaches that deal with forgetting ignore the problem of catastrophic remembering, i.e. the worsening ability to discriminate between data from different tasks. In our work, we introduce Relevance Mapping Networks (RMNs) which are inspired by the Optimal Overlap Hypothesis. The mappings reflects the relevance of the weights for the task at hand by assigning large weights to essential parameters. We show that RMNs learn an optimized representational overlap that overcomes the twin problem of catastrophic forgetting and remembering. Our approach achieves state-ofthe-art performance across all common continual learning datasets, even significantly outperforming data replay methods while not violating the constraints for an ideal continual learning system. Moreover, RMNs retain the ability to detect data from new tasks in an unsupervised manner, thus proving their resilience against catastrophic remembering.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/YFPP78ZQ/Kaushik et al. - 2021 - Understanding Catastrophic Forgetting and Remember.pdf}
}

@incollection{kazakovEvolvingGameLife2005,
  title = {Evolving the {{Game}} of {{Life}}},
  booktitle = {Adaptive {{Agents}} and {{Multi-Agent Systems II}}},
  author = {Kazakov, Dimitar and Sweet, Matthew},
  editor = {Kudenko, Daniel and Kazakov, Dimitar and Alonso, Eduardo},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3394},
  pages = {132--146},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-32274-0_9},
  url = {http://link.springer.com/10.1007/978-3-540-32274-0_9},
  urldate = {2020-06-09},
  abstract = {It is difficult to define a set of rules for a cellular automaton (CA) such that creatures with life-like properties (stability and dynamic behaviour, reproducton and self-repair) can be grown from a large number of initial configurations. This work describes an evolutionary framework for the search of a CA with these properties. Instead of encoding them directly into the fitness function, we propose one, which maximises the variance of entropy across the CA grid. This fitness function promotes the existence of areas on the verge of chaos, where life is expected to thrive. The results are reported for the case of CA in which cells are in one of four possible states. We also describe a mechanism for fitness sharing that successfully speeds up the genetic search, both in terms of number of generations and CPU time.},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-540-25260-3 978-3-540-32274-0},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/kazakovEvolvingGameLife2005.pdf}
}

@inproceedings{kelsoDynamicPatternsComplex1988a,
  title = {Dynamic {{Patterns}} in {{Complex Systems}}: {{Proceedings}} of a {{Conference}}, Sponsored by {{Florida Atlantic University}}, Held in Honor of {{Hermann Haken}} on the Occasion of His 60th Birthday},
  shorttitle = {{{DYNAMIC PATTERNS IN COMPLEX SYSTEMS}}},
  booktitle = {Dynamic {{Patterns}} in {{Complex Systems}}},
  author = {Kelso, J.A.S. and Mandell, A.J. and Shlesinger, M.F.},
  date = {1988-04},
  pages = {1--432},
  publisher = {{WORLD SCIENTIFIC}},
  location = {{Fort Lauderdale, USA}},
  doi = {10.1142/9789814542043},
  url = {http://www.worldscientific.com/doi/abs/10.1142/9789814542043},
  urldate = {2022-11-16},
  eventtitle = {Conference in {{Honor}} of {{Hermann Haken}} on the {{Occasion}} of {{His}} 60th {{Birthday}}},
  isbn = {978-9971-5-0485-4 978-981-4542-04-3},
  langid = {english}
}

@inproceedings{kempeMaximizingSpreadInfluence2003,
  title = {Maximizing the Spread of Influence through a Social Network},
  booktitle = {Proceedings of the Ninth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining  - {{KDD}} '03},
  author = {Kempe, David and Kleinberg, Jon and Tardos, Éva},
  date = {2003},
  pages = {137},
  publisher = {{ACM Press}},
  location = {{Washington, D.C.}},
  doi = {10.1145/956750.956769},
  url = {http://portal.acm.org/citation.cfm?doid=956750.956769},
  urldate = {2018-12-28},
  eventtitle = {The Ninth {{ACM SIGKDD}} International Conference},
  isbn = {978-1-58113-737-8},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/kempeMaximizingSpreadInfluence22.pdf}
}

@misc{keskarCTRLConditionalTransformer2019,
  title = {{{CTRL}}: {{A Conditional Transformer Language Model}} for {{Controllable Generation}}},
  shorttitle = {{{CTRL}}},
  author = {Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
  date = {2019-09-20},
  number = {arXiv:1909.05858},
  eprint = {1909.05858},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.05858},
  url = {http://arxiv.org/abs/1909.05858},
  urldate = {2022-07-22},
  abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/keskarCTRLConditionalTransformer2019.pdf;/Users/hugo/Zotero/storage/5HPDW4H5/1909.html}
}

@unpublished{khalifaPCGRLProceduralContent2020,
  title = {{{PCGRL}}: {{Procedural Content Generation}} via {{Reinforcement Learning}}},
  shorttitle = {{{PCGRL}}},
  author = {Khalifa, Ahmed and Bontrager, Philip and Earle, Sam and Togelius, Julian},
  date = {2020-01-24},
  eprint = {2001.09212},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.09212},
  urldate = {2020-01-28},
  abstract = {We investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/KB34MDDH/khalifaPCGRLProceduralContent2020.pdf;/Users/hugo/Zotero/storage/UIVIE4DG/2001.html}
}

@incollection{khalilLearningCombinatorialOptimization2017,
  title = {Learning {{Combinatorial Optimization Algorithms}} over {{Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {6348--6358},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/7214-learning-combinatorial-optimization-algorithms-over-graphs.pdf},
  urldate = {2019-01-07},
  file = {/Users/hugo/Papers/pdf/khalilLearningCombinatorialOptimization22.pdf;/Users/hugo/Zotero/storage/LPUBFGRJ/7214-learning-combinatorial-optimization-algorithms-over-graphs.html}
}

@unpublished{kielaEfficientLargeScaleMultiModal2018,
  title = {Efficient {{Large-Scale Multi-Modal Classification}}},
  author = {Kiela, D. and Grave, E. and Joulin, A. and Mikolov, T.},
  date = {2018-02-06},
  eprint = {1802.02892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.02892},
  urldate = {2019-01-17},
  abstract = {While the incipient internet was largely text-based, the modern digital world is becoming increasingly multi-modal. Here, we examine multi-modal classification where one modality is discrete, e.g. text, and the other is continuous, e.g. visual representations transferred from a convolutional neural network. In particular, we focus on scenarios where we have to be able to classify large quantities of data quickly. We investigate various methods for performing multi-modal fusion and analyze their trade-offs in terms of classification accuracy and computational efficiency. Our findings indicate that the inclusion of continuous information improves performance over text-only on a range of multi-modal classification tasks, even with simple fusion methods. In addition, we experiment with discretizing the continuous features in order to speed up and simplify the fusion process even further. Our results show that fusion with discretized features outperforms text-only classification, at a fraction of the computational cost of full multimodal fusion, with the additional benefit of improved interpretability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/kielaEfficientLargeScaleMultiModal22.pdf}
}

@inproceedings{kiferDetectingChangeData2004,
  title = {Detecting Change in Data Streams},
  booktitle = {{{VLDB}}},
  author = {Kifer, Daniel and Ben-David, Shai and Gehrke, Johannes},
  date = {2004},
  volume = {4},
  pages = {180--191},
  publisher = {{Toronto, Canada}}
}

@unpublished{kimNeuralProgrammingLanguage2022,
  title = {A {{Neural Programming Language}} for the {{Reservoir Computer}}},
  author = {Kim, Jason Z. and Bassett, Dani S.},
  date = {2022-03-09},
  eprint = {2203.05032},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, physics:nlin},
  url = {http://arxiv.org/abs/2203.05032},
  urldate = {2022-03-14},
  abstract = {From logical reasoning to mental simulation, biological and artificial neural systems possess an incredible capacity for computation. Such neural computers offer a fundamentally novel computing paradigm by representing data continuously and processing information in a natively parallel and distributed manner. To harness this computation, prior work has developed extensive training techniques to understand existing neural networks. However, the lack of a concrete and low-level programming language for neural networks precludes us from taking full advantage of a neural computing framework. Here, we provide such a programming language using reservoir computing—a simple recurrent neural network—and close the gap between how we conceptualize and implement neural computers and silicon computers. By decomposing the reservoir’s internal representation and dynamics into a symbolic basis of its inputs, we define a low-level neural machine code that we use to program the reservoir to solve complex equations and store chaotic dynamical systems as random access memory (dRAM). Using this representation, we provide a fully distributed neural implementation of software virtualization and logical circuits, and even program a playable game of pong inside of a reservoir computer. Taken together, we define a concrete, practical, and fully generalizable implementation of neural computation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Dynamical Systems,Nonlinear Sciences - Chaotic Dynamics},
  file = {/Users/hugo/Papers/pdf/kimNeuralProgrammingLanguage2022.pdf}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2015},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2022-02-14}
}

@unpublished{kipfNeuralRelationalInference2018,
  title = {Neural {{Relational Inference}} for {{Interacting Systems}}},
  author = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
  date = {2018-02-13},
  eprint = {1802.04687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.04687},
  urldate = {2018-12-28},
  abstract = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/kipfNeuralRelationalInference22.pdf;/Users/hugo/Zotero/storage/GL4KUT32/1802.html}
}

@unpublished{kipfSemiSupervisedClassificationGraph2016,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-09-09},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2018-12-28},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/UADRAETM/kipfSemiSupervisedClassificationGraph2016.pdf;/Users/hugo/Zotero/storage/I9Z7JGQJ/1609.html}
}

@unpublished{kirkpatrickOvercomingCatastrophicForgetting2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  date = {2017-01-25},
  eprint = {1612.00796},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1612.00796},
  urldate = {2021-10-15},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/kirkpatrickOvercomingCatastrophicForgetting2017.pdf}
}

@unpublished{kirschMetaLearningBackpropagation2021,
  title = {Meta {{Learning Backpropagation And Improving It}}},
  author = {Kirsch, Louis and Schmidhuber, Jürgen},
  date = {2021-02-16},
  eprint = {2012.14905},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2012.14905},
  urldate = {2021-06-14},
  abstract = {Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to control fast weights, hyper networks, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VS-ML) unifies the above and demonstrates that simple weight-sharing and sparsity in an NN is sufficient to express powerful learning algorithms (LAs) in a reusable fashion. A simple implementation of VS-ML called VS-ML RNN allows for implementing the backpropagation LA solely by running an RNN in forward-mode. It can even meta-learn new LAs that improve upon backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta-learned LAs learn qualitatively different from gradient descent through fast association.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/4JLQQM44/Kirsch and Schmidhuber - 2021 - Meta Learning Backpropagation And Improving It.pdf}
}

@inproceedings{kistemakerCriticalFactorsPerformance2011,
  title = {Critical Factors in the Performance of Novelty Search},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Kistemaker, Steijn and Whiteson, Shimon},
  date = {2011-07-12},
  series = {{{GECCO}} '11},
  pages = {965--972},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2001576.2001708},
  url = {https://doi.org/10.1145/2001576.2001708},
  urldate = {2022-11-15},
  abstract = {Novelty search is a recently proposed method for evolutionary computation designed to avoid the problem of deception, in which the fitness function guides the search process away from global optima. Novelty search replaces fitness-based selection with novelty-based selection, where novelty is measured by comparing an individual's behavior to that of the current population and an archive of past novel individuals. Though there is substantial evidence that novelty search can overcome the problem of deception, the critical factors in its performance remain poorly understood. This paper helps to bridge this gap by analyzing how the behavior function, which maps each genotype to a behavior, affects performance. We propose the notion of descendant fitness probability (DFP), which describes how likely a genotype's descendants are to have a certain fitness, and formulate two hypotheses about when changes to the behavior function will improve novelty search's performance, based on the effect of those changes on behavior and DFP. Experiments in both artificial and deceptive maze domains provide substantial empirical support for these hypotheses.},
  isbn = {978-1-4503-0557-0},
  keywords = {evolutionary computation,neural networks,neuroevolution,novelty search,problem of deception}
}

@article{kitagawaConvergenceNewtonAlgorithm2016,
  title = {Convergence of a {{Newton}} Algorithm for Semi-Discrete Optimal Transport},
  author = {Kitagawa, Jun and Mérigot, Quentin and Thibert, Boris},
  date = {2016-03-17},
  url = {https://arxiv.org/abs/1603.05579v2},
  urldate = {2018-11-22},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/kitagawaConvergenceNewtonAlgorithm22.pdf;/Users/hugo/Zotero/storage/IL937Q5T/1603.html}
}

@unpublished{kleykoCellularAutomataCan2020,
  title = {Cellular {{Automata Can Reduce Memory Requirements}} of {{Collective-State Computing}}},
  author = {Kleyko, Denis and Frady, E. Paxon and Sommer, Friedrich T.},
  date = {2020-10-07},
  eprint = {2010.03585},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.03585},
  urldate = {2021-12-12},
  abstract = {Various non-classical approaches of distributed information processing, such as neural networks, computation with Ising models, reservoir computing, vector symbolic architectures, and others, employ the principle of collective-state computing. In this type of computing, the variables relevant in a computation are superimposed into a single high-dimensional state vector, the collective-state. The variable encoding uses a fixed set of random patterns, which has to be stored and kept available during the computation. Here we show that an elementary cellular automaton with rule 90 (CA90) enables space-time tradeoff for collective-state computing models that use random dense binary representations, i.e., memory requirements can be traded off with computation running CA90. We investigate the randomization behavior of CA90, in particular, the relation between the length of the randomization period and the size of the grid, and how CA90 preserves similarity in the presence of the initialization noise. Based on these analyses we discuss how to optimize a collective-state computing model, in which CA90 expands representations on the fly from short seed patterns - rather than storing the full set of random patterns. The CA90 expansion is applied and tested in concrete scenarios using reservoir computing and vector symbolic architectures. Our experimental results show that collective-state computing with CA90 expansion performs similarly compared to traditional collective-state models, in which random patterns are generated initially by a pseudo-random number generator and then stored in a large memory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/kleykoCellularAutomataCan2020.pdf;/Users/hugo/Zotero/storage/6LA4U5ED/2010.html}
}

@inproceedings{kleykoModalityClassificationMedical2017,
  title = {Modality Classification of Medical Images with Distributed Representations Based on Cellular Automata Reservoir Computing},
  booktitle = {2017 {{IEEE}} 14th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2017)},
  author = {Kleyko, Denis and Khan, Sumeer and Osipov, Evgeny and Yong, Suet-Peng},
  date = {2017-04},
  pages = {1053--1056},
  issn = {1945-8452},
  doi = {10.1109/ISBI.2017.7950697},
  abstract = {Modality corresponding to medical images is a vital filter in medical image retrieval systems. This article presents the classification of modalities of medical images based on the usage of principles of hyper-dimensional computing and reservoir computing. It is demonstrated that the highest classification accuracy of the proposed method is on a par with the best classical method for the given dataset (83\% vs. 84\%). The major positive property of the proposed method is that it does not require any optimization routine during the training phase and naturally allows for incremental learning upon the availability of new training data.},
  eventtitle = {2017 {{IEEE}} 14th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2017)},
  keywords = {Automata,Biomedical imaging,cellular automata,cellular automata reservoir computing,Feature extraction,hyperdimensional computing,image classification,image retrieval,medical image classification,medical image processing,medical image retrieval systems,Optimization,optimization routine,Prototypes,Reservoirs,Training},
  file = {/Users/hugo/Zotero/storage/QIYUVY99/7950697.html}
}

@article{klosDynamicalLearningDynamics2020,
  title = {Dynamical {{Learning}} of {{Dynamics}}},
  author = {Klos, Christian and Kalle Kossio, Yaroslav Felipe and Goedeke, Sven and Gilra, Aditya and Memmesheimer, Raoul-Martin},
  date = {2020-08-19},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {125},
  number = {8},
  pages = {088103},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.125.088103},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.125.088103},
  urldate = {2020-10-15},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/klosDynamicalLearningDynamics2020.pdf}
}

@article{kmiecikCoarseGrainedProteinModels2016,
  title = {Coarse-{{Grained Protein Models}} and {{Their Applications}}},
  author = {Kmiecik, Sebastian and Gront, Dominik and Kolinski, Michal and Wieteska, Lukasz and Dawid, Aleksandra Elzbieta and Kolinski, Andrzej},
  date = {2016-07-27},
  journaltitle = {Chemical Reviews},
  volume = {116},
  number = {14},
  pages = {7898--7936},
  issn = {0009-2665, 1520-6890},
  doi = {10.1021/acs.chemrev.6b00163},
  url = {https://pubs.acs.org/doi/10.1021/acs.chemrev.6b00163},
  urldate = {2020-03-06},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/kmiecikCoarseGrainedProteinModels2016.pdf}
}

@inproceedings{kobayashiContinualLearningExploiting2019,
  title = {Continual {{Learning Exploiting Structure}} of {{Fractal Reservoir Computing}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2019: {{Workshop}} and {{Special Sessions}}},
  author = {Kobayashi, Taisuke and Sugino, Toshiki},
  editor = {Tetko, Igor V. and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {35--47},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-30493-5_4},
  abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be fired corresponding to each task, since only readout weights are updated according to the degree of firing of neurons. We therefore propose the way to design reservoir computing such that the firing neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely different networks.},
  isbn = {978-3-030-30493-5},
  langid = {english},
  keywords = {Continual learning,Fractal network,Reservoir computing},
  file = {/Users/hugo/Papers/pdf/kobayashiContinualLearningExploiting2019.pdf}
}

@inproceedings{koehnMosesOpenSource2007,
  title = {Moses: {{Open Source Toolkit}} for {{Statistical Machine Translation}}},
  shorttitle = {Moses},
  booktitle = {{{ACL}} 2007, {{Proceedings}} of the 45th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}, {{June}} 23-30, 2007, {{Prague}}, {{Czech Republic}}},
  author = {Koehn, Philipp and Hoang, Hieu and Birch, Alexandra and Callison-Burch, Chris and Federico, Marcello and Bertoldi, Nicola and Cowan, Brooke and Shen, Wade and Moran, Christine and Zens, Richard and Dyer, Chris and Bojar, Ondrej and Constantin, Alexandra and Herbst, Evan},
  editor = {Carroll, John A. and van den Bosch, Antal and Zaenen, Annie},
  date = {2007},
  publisher = {{The Association for Computational Linguistics}},
  url = {https://aclanthology.org/P07-2045/},
  urldate = {2022-08-04}
}

@article{kolmogorovCombinatorialFoundationsInformation1983,
  title = {Combinatorial Foundations of Information Theory and the Calculus of Probabilities},
  author = {Kolmogorov, A N},
  date = {1983-08-31},
  journaltitle = {Russian Mathematical Surveys},
  volume = {38},
  number = {4},
  pages = {29--40},
  issn = {0036-0279, 1468-4829},
  doi = {10.1070/RM1983v038n04ABEH004203},
  url = {http://stacks.iop.org/0036-0279/38/i=4/a=R04?key=crossref.ca30d8580a400e9172f2eb06726073b0},
  urldate = {2020-03-07},
  file = {/Users/hugo/Papers/pdf/kolmogorovCombinatorialFoundationsInformation1983.pdf}
}

@article{kolmogorovThreeApproachesQuantitative1968,
  title = {Three Approaches to the Quantitative Definition of Information},
  author = {Kolmogorov, A. N.},
  date = {1968-01},
  journaltitle = {International Journal of Computer Mathematics},
  volume = {2},
  number = {1-4},
  pages = {157--168},
  issn = {0020-7160, 1029-0265},
  doi = {10.1080/00207166808803030},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00207166808803030},
  urldate = {2019-05-28},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/X7JKKXGH/kolmogorovThreeApproachesQuantitative1968.pdf}
}

@article{kompaMolecularLogicGate2001,
  title = {A Molecular Logic Gate},
  author = {Kompa, K. L. and Levine, R. D.},
  date = {2001-01-16},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {98},
  number = {2},
  pages = {410--414},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.98.2.410},
  url = {https://www.pnas.org/doi/10.1073/pnas.98.2.410},
  urldate = {2022-03-15},
  file = {/Users/hugo/Papers/pdf/kompaMolecularLogicGate2001.pdf}
}

@unpublished{koolAttentionSolvesYour2018,
  title = {Attention {{Solves Your TSP}}, {{Approximately}}},
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  options = {useprefix=true},
  date = {2018-03-22},
  eprint = {1803.08475},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.08475},
  urldate = {2019-01-07},
  abstract = {The development of efficient (heuristic) algorithms for practical combinatorial optimization problems is costly, so we want to automatically learn them instead. We show the feasibility of this approach on the important Travelling Salesman Problem (TSP). We learn a heuristic algorithm that uses a Neural Network policy to construct a tour. As an alternative to the Pointer Network, our model is based entirely on (graph) attention layers and is invariant to the input order of the nodes. We train the model efficiently using REINFORCE with a simple and robust baseline based on a deterministic (greedy) rollout of the best policy so far. We significantly improve over results from previous works that consider learned heuristics for the TSP, reducing the optimality gap for a single tour construction from 1.51\% to 0.32\% for instances with 20 nodes, from 4.59\% to 1.71\% for 50 nodes and from 6.89\% to 4.43\% for 100 nodes. Additionally, we improve over a recent Reinforcement Learning framework for two variants of the Vehicle Routing Problem (VRP).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/koolAttentionSolvesYour22.pdf;/Users/hugo/Zotero/storage/VEQ6988C/1803.html}
}

@article{koppelAlmostMachineindependentTheory1991,
  title = {An Almost Machine-Independent Theory of Program-Length Complexity, Sophistication, and Induction},
  author = {Koppel, Moshe and Atlan, Henri},
  date = {1991-08-01},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {56},
  number = {1},
  pages = {23--33},
  issn = {0020-0255},
  doi = {10.1016/0020-0255(91)90021-L},
  url = {https://www.sciencedirect.com/science/article/pii/002002559190021L},
  urldate = {2022-02-28},
  abstract = {The purpose of this paper is to use a variant of program-length complexity to formally define the structure of a binary string, where the structure of an object is taken to mean the aggregate of its projectible properties.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/4B8K5XSI/002002559190021L.html}
}

@article{kosDelvingAdversarialAttacks2017,
  title = {Delving into Adversarial Attacks on Deep Policies},
  author = {Kos, Jernej and Song, Dawn},
  date = {2017-05-18},
  url = {https://arxiv.org/abs/1705.06452},
  urldate = {2018-11-27},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/kosDelvingAdversarialAttacks22.pdf;/Users/hugo/Zotero/storage/DWJG6986/1705.html}
}

@inproceedings{kowaliwMeasuresComplexityArtificial2008,
  title = {Measures of Complexity for Artificial Embryogeny},
  booktitle = {Proceedings of the 10th Annual Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '08},
  author = {Kowaliw, Taras},
  date = {2008},
  pages = {843},
  publisher = {{ACM Press}},
  location = {{Atlanta, GA, USA}},
  doi = {10.1145/1389095.1389259},
  url = {http://portal.acm.org/citation.cfm?doid=1389095.1389259},
  urldate = {2019-10-07},
  eventtitle = {The 10th Annual Conference},
  isbn = {978-1-60558-130-9},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/WUUZY668/kowaliwMeasuresComplexityArtificial2008.pdf}
}

@inproceedings{kozaEvolutionSubsumptionUsing1992,
  title = {Evolution of Subsumption Using Genetic Programming},
  booktitle = {Proc. {{First European Conference}} on {{Artificial Life Towards}} a {{Practice}} of {{Autonomous Systems}}, 1992},
  author = {Koza, J},
  date = {1992},
  publisher = {{MIT Press}},
  url = {https://cir.nii.ac.jp/crid/1570854175390434048},
  urldate = {2022-11-16},
  eventtitle = {European {{Conference}} on {{Artificial Life Towards}} a {{Practice}} of {{Autonomous Systems}}},
  file = {/Users/hugo/Papers/pdf/kozaEvolutionSubsumptionUsing1992.pdf;/Users/hugo/Zotero/storage/QFDLYE48/1570854175390434048.html}
}

@article{krakauerInformationTheoryIndividuality2020,
  title = {The Information Theory of Individuality},
  author = {Krakauer, David and Bertschinger, Nils and Olbrich, Eckehard and Flack, Jessica C. and Ay, Nihat},
  date = {2020-06-01},
  journaltitle = {Theory in Biosciences},
  shortjournal = {Theory Biosci.},
  volume = {139},
  number = {2},
  pages = {209--223},
  issn = {1611-7530},
  doi = {10.1007/s12064-020-00313-7},
  url = {https://doi.org/10.1007/s12064-020-00313-7},
  urldate = {2020-07-19},
  abstract = {Despite the near universal assumption of individuality in biology, there is little agreement about what individuals are and few rigorous quantitative methods for their identification. Here, we propose that individuals are aggregates that preserve a measure of temporal integrity, i.e., “propagate” information from their past into their futures. We formalize this idea using information theory and graphical models. This mathematical formulation yields three principled and distinct forms of individuality—an organismal, a colonial, and a driven form—each of which varies in the degree of environmental dependence and inherited information. This approach can be thought of as a Gestalt approach to evolution where selection makes figure-ground (agent–environment) distinctions using suitable information-theoretic lenses. A benefit of the approach is that it expands the scope of allowable individuals to include adaptive aggregations in systems that are multi-scale, highly distributed, and do not necessarily have physical boundaries such as cell walls or clonal somatic tissue. Such individuals might be visible to selection but hard to detect by observers without suitable measurement principles. The information theory of individuality allows for the identification of individuals at all levels of organization from molecular to cultural and provides a basis for testing assumptions about the natural scales of a system and argues for the importance of uncertainty reduction through coarse-graining in adaptive systems.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/krakauerInformationTheoryIndividuality2020.pdf}
}

@article{kramerNonlinearPrincipalComponent1991,
  title = {Nonlinear Principal Component Analysis Using Autoassociative Neural Networks},
  author = {Kramer, Mark A.},
  date = {1991-02},
  journaltitle = {AIChE Journal},
  volume = {37},
  number = {2},
  pages = {233--243},
  issn = {0001-1541, 1547-5905},
  doi = {10.1002/aic.690370209},
  url = {http://doi.wiley.com/10.1002/aic.690370209},
  urldate = {2020-03-05},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/kramerNonlinearPrincipalComponent1991.pdf}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {00010782},
  doi = {10.1145/3065386},
  url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
  urldate = {2019-07-01},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/HNTTGVCM/krizhevskyImageNetClassificationDeep2017.pdf}
}

@report{krizhevskyLearningMultipleLayers2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  date = {2009},
  institution = {{University of Toronto}},
  file = {/Users/hugo/Papers/pdf/krizhevskyLearningMultipleLayers2009.pdf}
}

@unpublished{kruszewskiCombinatoryChemistrySimple2020,
  title = {Combinatory {{Chemistry}}: {{Towards}} a {{Simple Model}} of {{Emergent Evolution}}},
  shorttitle = {Combinatory {{Chemistry}}},
  author = {Kruszewski, Germán and Mikolov, Tomas},
  date = {2020-03-17},
  eprint = {2003.07916},
  eprinttype = {arxiv},
  primaryclass = {nlin, q-bio},
  url = {http://arxiv.org/abs/2003.07916},
  urldate = {2020-03-26},
  abstract = {Researching the conditions for the emergence of life –not necessarily as it is, but as it could be– is one of the main goals of Artificial Life. Answering this question requires a model that can first explain the emergence of evolvable units, namely, structures that (1) preserve themselves in time (2) self-reproduce and (3) can tolerate a certain amount of variation when reproducing. To tackle this challenge, here we introduce Combinatory Chemistry, an Algorithmic Artificial Chemistry based on a simple computational paradigm named Combinatory Logic. The dynamics of this system comprise very few rules, it is initialized with an elementary tabula rasa state, and features conservation laws replicating natural resource constraints. Our experiments show that a single run of this dynamical system discovers a wide range of emergent patterns with no external intervention. All these structures rely on acquiring basic constituents from the environment and decomposing them in a process that is remarkably similar to biological metabolisms. These patterns involve autopoietic structures that maintain their organisation, recursive ones that grow in linear chains or binary-branching trees, and most notably, patterns able to reproduce themselves, duplicating their number at each generation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Molecular Networks},
  file = {/Users/hugo/Papers/pdf/kruszewskiCombinatoryChemistrySimple2020.pdf}
}

@unpublished{kruszewskiEmergenceSelfreproducingMetabolisms2021,
  title = {Emergence of Self-Reproducing Metabolisms as Recursive Algorithms in an {{Artificial Chemistry}}},
  author = {Kruszewski, Germán and Mikolov, Tomas},
  date = {2021-03-15},
  eprint = {2103.08245},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/2103.08245},
  urldate = {2021-05-04},
  abstract = {Researching the conditions for the emergence of life –not necessarily as it is, but as it could be– is one of the main goals of Artificial Life. Artificial Chemistries are one of the most important tools in this endeavour, as they allow us to investigate the process by which metabolisms capable of self-reproduction and –ultimately– of evolving, might have emerged. While previous work has shown promising results in this direction, it is still unclear which are the fundamental properties of a chemical system that enable emergent structures to arise. To this end, here we present an Artificial Chemistry based on Combinatory Logic, a Turing-complete rewriting system, which relies on a minimal set of possible reactions. Our experiments show that a single run of this chemistry starting from a tabula rasa state discovers with no external intervention a wide range of emergent structures, including autopoietic structures that maintain their organisation unchanged, others that grow recursively, and most notably, patterns that reproduce themselves, duplicating their number on each cycle. All of these structures take the form of recursive algorithms that acquire basic constituents from the environment and decompose them in a process that is remarkably similar to biological metabolisms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {/Users/hugo/Zotero/storage/MGYITVZ9/Kruszewski and Mikolov - 2021 - Emergence of self-reproducing metabolisms as recur.pdf}
}

@unpublished{kupynDeblurGANv2DeblurringOrdersofMagnitude2019,
  title = {{{DeblurGAN-v2}}: {{Deblurring}} ({{Orders-of-Magnitude}}) {{Faster}} and {{Better}}},
  shorttitle = {{{DeblurGAN-v2}}},
  author = {Kupyn, Orest and Martyniuk, Tetiana and Wu, Junru and Wang, Zhangyang},
  date = {2019-08-10},
  eprint = {1908.03826},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1908.03826},
  urldate = {2020-09-29},
  abstract = {We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-v2, which considerably boosts state-of-the-art deblurring efficiency, quality, and flexibility. DeblurGAN-v2 is based on a relativistic conditional GAN with a double-scale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-v2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plug-in of sophisticated backbones (e.g., Inception-ResNet-v2) can lead to solid state-of-the-art deblurring. Meanwhile, with light-weight backbones (e.g., MobileNet and its variants), DeblurGAN-v2 reaches 10-100 times faster than the nearest competitors, while maintaining close to state-of-the-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-v2 obtains very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. Besides, we show the architecture to be effective for general image restoration tasks too. Our codes, models and data are available at: https://github.com/KupynOrest/DeblurGANv2},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/kupynDeblurGANv2DeblurringOrdersofMagnitude2019.pdf;/Users/hugo/Zotero/storage/YC82LVGX/1908.html}
}

@inproceedings{kusnerWordEmbeddingsDocument2015,
  title = {From {{Word Embeddings}} to {{Document Distances}}},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
  date = {2015},
  series = {{{ICML}}'15},
  pages = {957--966},
  publisher = {{JMLR.org}},
  location = {{Lille, France}},
  url = {http://dl.acm.org/citation.cfm?id=3045118.3045221},
  urldate = {2019-01-01},
  abstract = {We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to "travel" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.},
  file = {/Users/hugo/Papers/pdf/kusnerWordEmbeddingsDocument22.pdf}
}

@article{kuznetsAmericanEconomicReview1955,
  title = {The American Economic Review},
  author = {Kuznets', S.},
  date = {1955},
  journaltitle = {American Economic Association},
  volume = {45},
  number = {1},
  pages = {1--28},
  langid = {undefined},
  file = {/Users/hugo/Zotero/storage/NQTSV2SL/display.html}
}

@article{lacoste-julienSiGMaSimpleGreedy2013,
  title = {{{SiGMa}}: {{Simple Greedy Matching}} for {{Aligning Large Knowledge Bases}}},
  author = {Lacoste-Julien, Simon and Palla, Konstantina and Davies, Alex and Kasneci, Gjergji and Graepel, Thore and Ghahramani, Zoubin},
  date = {2013},
  journaltitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
  pages = {572},
  issn = {9781450321747},
  doi = {10.1145/2487575.2487592},
  url = {http://dl.acm.org/citation.cfm?doid=2487575.2487592},
  abstract = {The Internet has enabled the creation of a growing number of large-scale knowl-edge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algo-rithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search, thus making it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional ex-periments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency.},
  keywords = {alignment,entity,knowledge base,large-scale,relationship}
}

@inproceedings{laiRecurrentConvolutionalNeural2015,
  title = {Recurrent Convolutional Neural Networks for Text Classification},
  booktitle = {Proceedings of the {{Twenty-Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
  date = {2015-01-25},
  series = {{{AAAI}}'15},
  pages = {2267--2273},
  publisher = {{AAAI Press}},
  location = {{Austin, Texas}},
  abstract = {Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
  isbn = {978-0-262-51129-2},
  file = {/Users/hugo/Papers/pdf/laiRecurrentConvolutionalNeural2015.pdf}
}

@unpublished{lampleDeepLearningSymbolic2019,
  title = {Deep {{Learning}} for {{Symbolic Mathematics}}},
  author = {Lample, Guillaume and Charton, François},
  date = {2019-12-02},
  eprint = {1912.01412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.01412},
  urldate = {2019-12-18},
  abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {/Users/hugo/Zotero/storage/PW457936/lampleDeepLearningSymbolic2019.pdf}
}

@misc{lanALBERTLiteBERT2020,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  date = {2020-02-08},
  number = {arXiv:1909.11942},
  eprint = {1909.11942},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.11942},
  url = {http://arxiv.org/abs/1909.11942},
  urldate = {2022-07-22},
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \textbackslash squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/lanALBERTLiteBERT2020.pdf;/Users/hugo/Zotero/storage/Z99LJ9XY/1909.html}
}

@book{langtonArtificialLifeProceedings1989,
  title = {Artificial Life: The Proceedings of an {{Interdisciplinary Workshop}} on the {{Synthesis}} and {{Simulation}} of {{Living Systems}}, Held {{September}}, 1987, in {{Los Alamos}}, {{New Mexico}}},
  shorttitle = {Artificial Life},
  editor = {Langton, Christopher G.},
  date = {1989},
  series = {Santa {{Fe Institute}} Studies in the Sciences of Complexity},
  number = {v. 6},
  publisher = {{Addison-Wesley Pub. Co., Advanced Book Program}},
  location = {{Redwood City, Calif}},
  eventtitle = {Interdisciplinary {{Workshop}} on the {{Synthesis}} and {{Simulation}} of {{Living Systems}}},
  isbn = {978-0-201-09346-9 978-0-201-09356-8},
  pagetotal = {655},
  keywords = {Biological systems,Computer simulation Congresses,Simulation methods Congresses}
}

@article{langtonComputationEdgeChaos1990,
  title = {Computation at the Edge of Chaos: {{Phase}} Transitions and Emergent Computation},
  shorttitle = {Computation at the Edge of Chaos},
  author = {Langton, Chris G.},
  date = {1990-06},
  journaltitle = {Physica D: Nonlinear Phenomena},
  volume = {42},
  number = {1-3},
  pages = {12--37},
  issn = {01672789},
  doi = {10.1016/0167-2789(90)90064-V},
  url = {https://linkinghub.elsevier.com/retrieve/pii/016727899090064V},
  urldate = {2019-04-25},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/langtonComputationEdgeChaos12.pdf}
}

@article{langtonSelfreproductionCellularAutomata1984,
  title = {Self-Reproduction in Cellular Automata},
  author = {Langton, Christopher G.},
  date = {1984-01-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {10},
  number = {1},
  pages = {135--144},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(84)90256-2},
  url = {http://www.sciencedirect.com/science/article/pii/0167278984902562},
  urldate = {2019-05-28},
  abstract = {Self-reproduction in cellular automata is discussed with reference to the models of von Neumann and Codd. The conclusion is drawn that although the capacity for universal construction is a sufficient condition for self-reproduction, it is not a necessary condition. Slightly more “liberal” criteria for what constitutes genuine self-reproduction are introduced, and a simple self-reproducing structure is exhibited which satisfies these new criteria. This structure achieves its simplicity by storing its description in a dynamic “loop”, rather than on a static “tape”.},
  file = {/Users/hugo/Papers/pdf/langtonSelfreproductionCellularAutomata12.pdf}
}

@article{langtonStudyingArtificialLife1986,
  title = {Studying Artificial Life with Cellular Automata},
  author = {Langton, Christopher G},
  date = {1986-10-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  series = {Proceedings of the {{Fifth Annual International Conference}}},
  volume = {22},
  number = {1},
  pages = {120--149},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(86)90237-X},
  url = {http://www.sciencedirect.com/science/article/pii/016727898690237X},
  urldate = {2019-05-28},
  abstract = {Biochemistry studies the way in which life emerges from the interaction of inanimate molecules. In this paper we look into the possibility that life could emerge from the interaction of inanimate artificial molecules. Cellular automata provide us with the logical universes within which we can embed artificial molecules in the form of propagating, virtual automata. We suggest that since virtual automata have the computational capacity to fill many of the functional roles played by the primary biomolecules, there is a strong possibility that the ‘molecular logic’ of life can be embedded within cellular automata and that, therefore, artificial life is a distinct possibility within these highly parallel computer structures.},
  file = {/Users/hugo/Papers/pdf/langtonStudyingArtificialLife12.pdf}
}

@article{laoRandomWalkInference,
  title = {Random {{Walk Inference}} and {{Learning}} in {{A Large Scale Knowledge Base}}},
  author = {Lao, Ni and Mitchell, Tom and Cohen, William W},
  pages = {11},
  abstract = {We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/laoRandomWalkInference2.pdf}
}

@article{laoRelationalRetrievalUsing2010,
  title = {Relational Retrieval Using a Combination of Path-Constrained Random Walks},
  author = {Lao, Ni and Cohen, William W.},
  date = {2010-10},
  journaltitle = {Machine Learning},
  volume = {81},
  number = {1},
  pages = {53--67},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-010-5205-8},
  url = {http://link.springer.com/10.1007/s10994-010-5205-8},
  urldate = {2018-06-13},
  abstract = {Scientific literature with rich metadata can be represented as a labeled directed graph. This graph representation enables a number of scientific tasks such as ad hoc retrieval or named entity recognition (NER) to be formulated as typed proximity queries in the graph. One popular proximity measure is called Random Walk with Restart (RWR), and much work has been done on the supervised learning of RWR measures by associating each edge label with a parameter. In this paper, we describe a novel learnable proximity measure which instead uses one weight per edge label sequence: proximity is defined by a weighted combination of simple “path experts”, each corresponding to following a particular sequence of labeled edges. Experiments on eight tasks in two subdomains of biology show that the new learning method significantly outperforms the RWR model (both trained and untrained). We also extend the method to support two additional types of experts to model intrinsic properties of entities: query-independent experts, which generalize the PageRank measure, and popular entity experts which allow rankings to be adjusted for particular entities that are especially important.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/laoRelationalRetrievalUsing22.pdf}
}

@unpublished{laurentRecurrentNeuralNetwork2016,
  title = {A Recurrent Neural Network without Chaos},
  author = {Laurent, Thomas and von Brecht, James},
  options = {useprefix=true},
  date = {2016-12-19},
  eprint = {1612.06212},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1612.06212},
  urldate = {2021-04-27},
  abstract = {We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/laurentRecurrentNeuralNetwork2016.pdf;/Users/hugo/Zotero/storage/H3K8KVTR/1612.html}
}

@book{lawrenceMakingFlyGenetics1992,
  title = {The Making of a Fly: The Genetics of Animal Design},
  shorttitle = {The Making of a Fly},
  author = {Lawrence, Peter A.},
  date = {1992},
  publisher = {{Blackwell Science}},
  location = {{Oxford [England] ; Cambridge, Mass., USA}},
  isbn = {978-0-632-03048-4},
  pagetotal = {228},
  keywords = {Drosophila melanogaster,Embryology,Genetics,Insects}
}

@article{lazebnikCanBiologistFix2002,
  title = {Can a Biologist Fix a Radio?—{{Or}}, What {{I}} Learned While Studying Apoptosis},
  shorttitle = {Can a Biologist Fix a Radio?},
  author = {Lazebnik, Yuri},
  date = {2002-09-01},
  journaltitle = {Cancer Cell},
  shortjournal = {Cancer Cell},
  volume = {2},
  number = {3},
  eprint = {12242150},
  eprinttype = {pmid},
  pages = {179--182},
  publisher = {{Elsevier}},
  issn = {1535-6108, 1878-3686},
  doi = {10.1016/S1535-6108(02)00133-2},
  url = {https://www.cell.com/cancer-cell/abstract/S1535-6108(02)00133-2},
  urldate = {2022-08-03},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/lazebnikCanBiologistFix2002.pdf;/Users/hugo/Zotero/storage/Y2IPGWHH/S1535-6108(02)00133-2.html}
}

@article{lazebnikCanBiologistFix2002a,
  title = {Can a Biologist Fix a Radio?—{{Or}}, What {{I}} Learned While Studying Apoptosis},
  shorttitle = {Can a Biologist Fix a Radio?},
  author = {Lazebnik, Yuri},
  date = {2002-09-01},
  journaltitle = {Cancer Cell},
  shortjournal = {Cancer Cell},
  volume = {2},
  number = {3},
  eprint = {12242150},
  eprinttype = {pmid},
  pages = {179--182},
  publisher = {{Elsevier}},
  issn = {1535-6108, 1878-3686},
  doi = {10.1016/S1535-6108(02)00133-2},
  url = {https://www.cell.com/cancer-cell/abstract/S1535-6108(02)00133-2},
  urldate = {2022-08-03},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/lazebnikCanBiologistFix2002a.pdf}
}

@article{lechnerNeuralCircuitPolicies2020,
  title = {Neural Circuit Policies Enabling Auditable Autonomy},
  author = {Lechner, Mathias and Hasani, Ramin and Amini, Alexander and Henzinger, Thomas A. and Rus, Daniela and Grosu, Radu},
  date = {2020-10},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {2},
  number = {10},
  pages = {642--652},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00237-3},
  url = {http://www.nature.com/articles/s42256-020-00237-3},
  urldate = {2020-10-21},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/lechnerNeuralCircuitPolicies2020.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2019-07-03},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/JJY3B7VH/nature14539.html}
}

@inproceedings{lecunOptimalBrainDamage1990,
  title = {Optimal Brain Damage},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {LeCun, Yann and Denker, John S. and Solla, Sara A.},
  date = {1990},
  pages = {598--605},
  file = {/Users/hugo/Papers/pdf/lecunOptimalBrainDamage1990.pdf}
}

@article{legoffSampleTimeEfficient2020,
  title = {Sample and Time Efficient Policy Learning with {{CMA-ES}} and {{Bayesian Optimisation}}},
  author = {Le Goff, Léni K. and Buchanan, Edgar and Hart, Emma and Eiben, Agoston E. and Li, Wei and de Carlo, Matteo and Hale, Matthew F. and Angus, Mike and Woolley, Robert and Timmis, Jon and Winfield, Alan and Tyrrell, Andrew M.},
  options = {useprefix=true},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {432--440},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00299},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00299},
  urldate = {2020-07-27},
  abstract = {In evolutionary robot systems where morphologies and controllers of real robots are simultaneously evolved, it is clear that there is likely to be requirements to refine the inherited controller of a ‘newborn’ robot in order to better align it to its newly generated morphology. This can be accomplished via a learning mechanism applied to each individual robot: for practical reasons, such a mechanism should be both sample and time-efficient. In this paper, We investigate two ways to improve the sample and time efficiency of the well-known learner CMA-ES on navigation tasks. The first approach combines CMA-ES with Novelty Search, and includes an adaptive restart mechanism with increasing population size. The second bootstraps CMA-ES using Bayesian Optimisation, known for its sample efficiency. Results using two robots built with the ARE project's modules and four environments show that novelty reduces the number of samples needed to converge, as does the custom restart mechanism; the latter also has better sample and time efficiency than the hybridised Bayesian/Evolutionary method.},
  file = {/Users/hugo/Papers/pdf/legoffSampleTimeEfficient2020.pdf;/Users/hugo/Zotero/storage/MGC2LAE2/isal_a_00299.html}
}

@article{lehmanAbandoningObjectivesEvolution2011,
  title = {Abandoning {{Objectives}}: {{Evolution Through}} the {{Search}} for {{Novelty Alone}}},
  shorttitle = {Abandoning {{Objectives}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  date = {2011-06},
  journaltitle = {Evolutionary Computation},
  shortjournal = {Evolutionary Computation},
  volume = {19},
  number = {2},
  pages = {189--223},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/EVCO_a_00025},
  url = {https://direct.mit.edu/evco/article/19/2/189-223/1365},
  urldate = {2021-09-07},
  abstract = {In evolutionary computation, the fitness function normally measures progress toward an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search toward dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution. Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objectivebased search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/2WES4UZB/Lehman and Stanley - 2011 - Abandoning Objectives Evolution Through the Searc.pdf}
}

@article{lehmanAnarchyMethodsCurrent2014,
  title = {An Anarchy of Methods: {{Current}} Trends in How Intelligence Is Abstracted in Ai},
  shorttitle = {An Anarchy of Methods},
  author = {Lehman, Joel and Clune, Jeff and Risi, Sebastian},
  date = {2014},
  journaltitle = {IEEE Intelligent Systems},
  volume = {29},
  number = {6},
  pages = {56--62},
  publisher = {{IEEE}},
  file = {/Users/hugo/Papers/pdf/lehmanAnarchyMethodsCurrent2014.pdf;/Users/hugo/Zotero/storage/9YDCCFT4/6982117.html}
}

@inproceedings{lehmanEfficientlyEvolvingPrograms2010,
  title = {Efficiently Evolving Programs through the Search for Novelty},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  date = {2010-07-07},
  series = {{{GECCO}} '10},
  pages = {837--844},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1830483.1830638},
  url = {https://doi.org/10.1145/1830483.1830638},
  urldate = {2022-11-15},
  abstract = {A significant challenge in genetic programming is premature convergence to local optima, which often prevents evolution from solving problems. This paper introduces to genetic programming a method that originated in neuroevolution (i.e. the evolution of artificial neural networks) that circumvents the problem of deceptive local optima. The main idea is to search only for behavioral novelty instead of for higher fitness values. Although such novelty search abandons following the gradient of the fitness function, if such gradients are deceptive they may actually occlude paths through the search space towards the objective. Because there are only so many ways to behave, the search for behavioral novelty is often computationally feasible and differs significantly from random search. Counterintuitively, in both a deceptive maze navigation task and the artificial ant benchmark task, genetic programming with novelty search, which ignores the objective, outperforms traditional genetic programming that directly searches for optimal behavior. Additionally, novelty search evolves smaller program trees in every variation of the test domains. Novelty search thus appears less susceptible to bloat, another significant problem in genetic programming. The conclusion is that novelty search is a viable new tool for efficiently solving some deceptive problems in genetic programming.},
  isbn = {978-1-4503-0072-8},
  keywords = {genetic programming,novelty search,premature convergence,program bloat}
}

@inproceedings{lehmanEvolvingDiversityVirtual2011,
  title = {Evolving a Diversity of Virtual Creatures through Novelty Search and Local Competition},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  date = {2011},
  pages = {211--218},
  file = {/Users/hugo/Papers/pdf/lehmanEvolvingDiversityVirtual2011.pdf;/Users/hugo/Zotero/storage/32DRV6PF/2001576.html}
}

@inproceedings{lehreDevelopmentalMappingsPhenotypic2003,
  title = {Developmental Mappings and Phenotypic Complexity},
  booktitle = {The 2003 {{Congress}} on {{Evolutionary Computation}}, 2003. {{CEC}} '03.},
  author = {Lehre, P. K. and Haddow, P. C.},
  date = {2003-12},
  volume = {1},
  pages = {62-68 Vol.1},
  doi = {10.1109/CEC.2003.1299557},
  abstract = {The effect of phenotypic complexity on distance correlation plots is investigated for two developmental mappings, a mapping based on L-systems, and a 2D cellular automata mapping. Our treatment of complexity is based on the theory of Kolmogorov complexity. A new genotype sampling algorithm called cross section walk is introduced.},
  eventtitle = {The 2003 {{Congress}} on {{Evolutionary Computation}}, 2003. {{CEC}} '03.},
  keywords = {2D cellular automata mapping,artificial life,Artificial neural networks,Biological information theory,cellular automata,computational complexity,cross section walk,developmental mappings,distance correlation plots,Embryo,Encoding,evolutionary computation,Evolutionary computation,genotype sampling algorithm,Hardware,Information science,Kolmogorov complexity,L-systems,Organisms,phenotypic complexity,Sampling methods,Scalability},
  file = {/Users/hugo/Zotero/storage/RG63J4ZK/lehreDevelopmentalMappingsPhenotypic2003.pdf}
}

@article{lempelComplexityFiniteSequences1976,
  title = {On the {{Complexity}} of {{Finite Sequences}}},
  author = {Lempel, A. and Ziv, J.},
  date = {1976-01},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {22},
  number = {1},
  pages = {75--81},
  issn = {0018-9448},
  doi = {10.1109/TIT.1976.1055501},
  abstract = {A new approach to the problem of evaluating the complexity ("randomness") of finite sequences is presented. The proposed complexity measure is related to the number of steps in a self-delimiting production process by which a given sequence is presumed to be generated. It is further related to the number of distinct substrings and the rate of their occurrence along the sequence. The derived properties of the proposed measure are discussed and motivated in conjunction with other well-established complexity criteria.},
  keywords = {Sequences}
}

@article{levesqueWinogradSchemaChallenge,
  title = {The {{Winograd Schema Challenge}}},
  author = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  pages = {10},
  abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Winograd schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/levesqueWinogradSchemaChallenge.pdf}
}

@article{levittComputerSimulationProtein1975,
  title = {Computer Simulation of Protein Folding},
  author = {Levitt, Michael and Warshel, Arieh},
  date = {1975-02},
  journaltitle = {Nature},
  volume = {253},
  number = {5494},
  pages = {694--698},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/253694a0},
  url = {http://www.nature.com/articles/253694a0},
  urldate = {2020-03-06},
  langid = {english}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  date = {2019-10-29},
  number = {arXiv:1910.13461},
  eprint = {1910.13461},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.13461},
  url = {http://arxiv.org/abs/1910.13461},
  urldate = {2022-07-22},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/lewisBARTDenoisingSequencetoSequence2019.pdf;/Users/hugo/Zotero/storage/2ZEKUAGC/1910.html}
}

@misc{lewkowyczSolvingQuantitativeReasoning2022,
  title = {Solving {{Quantitative Reasoning Problems}} with {{Language Models}}},
  author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
  date = {2022-06-30},
  number = {arXiv:2206.14858},
  eprint = {2206.14858},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2206.14858},
  urldate = {2022-07-26},
  abstract = {Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/lewkowyczSolvingQuantitativeReasoning2022.pdf;/Users/hugo/Zotero/storage/N8FSMYEZ/2206.html}
}

@unpublished{liAnalysisNonlinearDynamics2019,
  title = {Analysis on the {{Nonlinear Dynamics}} of {{Deep Neural Networks}}: {{Topological Entropy}} and {{Chaos}}},
  shorttitle = {Analysis on the {{Nonlinear Dynamics}} of {{Deep Neural Networks}}},
  author = {Li, Husheng},
  date = {2019-01-07},
  eprint = {1804.03987},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.03987},
  urldate = {2021-04-26},
  abstract = {The theoretical explanation for the great success of deep neural network (DNN) is still an open problem. In this paper DNN is considered as a discrete-time dynamical system due to its layered structure. The complexity provided by the nonlinearity in the DNN dynamics is analyzed in terms of topological entropy and chaos characterized by Lyapunov exponents. The properties revealed for the dynamics of DNN are applied to analyze the corresponding capabilities of classification and generalization. In particular, for both the hyperbolic tangent function and the rectified linear units (ReLU), the Lyapunov exponents are both positive given proper DNN parameters, which implies the chaotic behavior of the dynamics. Moreover, the Vapnik-Chervonenkis (VC) dimension of DNN is also analyzed, based on the layered and recursive structure. The conclusions from the viewpoint of dynamical systems are expected to open a new dimension for the understanding of DNN.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/liAnalysisNonlinearDynamics2019.pdf}
}

@article{liapisConstrainedNoveltySearch2015,
  title = {Constrained Novelty Search: {{A}} Study on Game Content Generation},
  shorttitle = {Constrained Novelty Search},
  author = {Liapis, Antonios and Yannakakis, Georgios N. and Togelius, Julian},
  date = {2015-03-01},
  journaltitle = {Evolutionary Computation},
  shortjournal = {Evol. Comput.},
  volume = {23},
  number = {1},
  pages = {101--129},
  issn = {1063-6560},
  doi = {10.1162/EVCO_a_00123},
  url = {https://doi.org/10.1162/EVCO_a_00123},
  urldate = {2022-11-15},
  abstract = {Novelty search is a recent algorithm geared toward exploring search spaces without regard to objectives. When the presence of constraints divides a search space into feasible space and infeasible space, interesting implications arise regarding how novelty search explores such spaces. This paper elaborates on the problem of constrained novelty search and proposes two novelty search algorithms which search within both the feasible and the infeasible space. Inspired by the FI-2pop genetic algorithm, both algorithms maintain and evolve two separate populations, one with feasible and one with infeasible individuals, while each population can use its own selection method. The proposed algorithms are applied to the problem of generating diverse but playable game levels, which is representative of the larger problem of procedural game content generation. Results show that the two-population constrained novelty search methods can create, under certain conditions, larger and more diverse sets of feasible game levels than current methods of novelty search, whether constrained or unconstrained. However, the best algorithm is contingent on the particularities of the search space and the genetic operators used. Additionally, the proposed enhancement of offspring boosting is shown to enhance performance in all cases of two-population novelty search.},
  keywords = {computer games,constrained optimization,Genetic algorithms,level design,novelty search,procedural content generation,two-population genetic algorithm},
  file = {/Users/hugo/Papers/pdf/liapisConstrainedNoveltySearch2015.pdf}
}

@inproceedings{liCombiningMarkovRandom2016,
  title = {Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Chuan and Wand, Michael},
  date = {2016},
  pages = {2479--2486},
  file = {/Users/hugo/Papers/pdf/liCombiningMarkovRandom22.pdf;/Users/hugo/Zotero/storage/HXANJ9LF/Li_Combining_Markov_Random_CVPR_2016_paper.html}
}

@misc{liDQBARTEfficientSequencetoSequence2022,
  title = {{{DQ-BART}}: {{Efficient Sequence-to-Sequence Model}} via {{Joint Distillation}} and {{Quantization}}},
  shorttitle = {{{DQ-BART}}},
  author = {Li, Zheng and Wang, Zijian and Tan, Ming and Nallapati, Ramesh and Bhatia, Parminder and Arnold, Andrew and Xiang, Bing and Roth, Dan},
  date = {2022-03-21},
  number = {arXiv:2203.11239},
  eprint = {2203.11239},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.11239},
  url = {http://arxiv.org/abs/2203.11239},
  urldate = {2022-07-26},
  abstract = {Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/liDQBARTEfficientSequencetoSequence2022.pdf;/Users/hugo/Zotero/storage/A2IQJKQX/2203.html}
}

@article{lindgrenComplexityMeasuresCellular1988,
  title = {Complexity {{Measures}} and {{Cellular Automata}}},
  author = {Lindgren, Kristian and G. Nordahl, Mats},
  date = {1988},
  pages = {32},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/lindgrenComplexityMeasuresCellular12.pdf}
}

@unpublished{lindseyLearningLearnFeedback2020,
  title = {Learning to {{Learn}} with {{Feedback}} and {{Local Plasticity}}},
  author = {Lindsey, Jack and Litwin-Kumar, Ashok},
  date = {2020-06-16},
  eprint = {2006.09549},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2006.09549},
  urldate = {2020-06-28},
  abstract = {Interest in biologically inspired alternatives to backpropagation is driven by the desire to both advance connections between deep learning and neuroscience and address backpropagation's shortcomings on tasks such as online, continual learning. However, local synaptic learning rules like those employed by the brain have so far failed to match the performance of backpropagation in deep networks. In this study, we employ meta-learning to discover networks that learn using feedback connections and local, biologically inspired learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding biologically implausible weight transport. Our experiments show that meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Surprisingly, this approach matches or exceeds a state-of-the-art gradient-based online meta-learning algorithm on regression and classification tasks, excelling in particular at continual learning. Analysis of the weight updates employed by these models reveals that they differ qualitatively from gradient descent in a way that reduces interference between updates. Our results suggest the existence of a class of biologically plausible learning mechanisms that not only match gradient descent-based learning, but also overcome its limitations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/hugo/Papers/pdf/lindseyLearningLearnFeedback2020.pdf}
}

@article{linKnowledgeRepresentationLearning,
  title = {Knowledge {{Representation Learning}} with {{Entities}}, {{Attributes}} and {{Relations}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
  pages = {7},
  abstract = {Distributed knowledge representation (KR) encodes both entities and relations in a lowdimensional semantic space, which has significantly promoted the performance of relation extraction and knowledge reasoning. In many knowledge graphs (KG), some relations indicate attributes of entities (attributes) and others indicate relations between entities (relations). Existing KR models regard all relations equally, and usually suffer from poor accuracies when modeling one-to-many and many-to-one relations, mostly composed of attribute. In this paper, we distinguish existing KGrelations into attributes and relations, and propose a new KR model with entities, attributes and relations (KR-EAR). The experiment results show that, by special modeling of attribute, KR-EAR can significantly outperform state-of-the-art KR models in prediction of entities, attributes and relations. The source code of this paper can be obtained from https://github.com/thunlp/KR-EAR.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/linKnowledgeRepresentationLearning2.pdf}
}

@inproceedings{linLearningEntityRelation2017,
  title = {Learning {{Entity}} and {{Relation Embeddings}} for {{Knowledge Resolution}}},
  booktitle = {Procedia {{Computer Science}}},
  author = {Lin, Hailun and Liu, Yong and Wang, Weiping and Yue, Yinliang and Lin, Zheng},
  date = {2017},
  volume = {108},
  eprint = {2605732},
  eprinttype = {pmid},
  pages = {345--354},
  doi = {10.1016/j.procs.2017.05.045},
  abstract = {Knowledge resolution is the task of clustering knowledge mentions, e.g., entity and relation mentions into several disjoint groups with each group representing a unique entity or relation. Such resolution is a central step in constructing high-quality knowledge graph from unstructured text. Previous research has tackled this problem by making use of various textual and structural features from a semantic dictionary or a knowledge graph. This may lead to poor performance on knowledge mentions with poor or not well-known contexts. In addition, it is also limited by the coverage of the semantic dictionary or knowledge graph. In this work, we propose ETransR, a method which automatically learns entity and relation feature representations in continuous vector spaces, in order to measure the semantic relatedness of knowledge mentions for knowledge resolution. Experimental results on two benchmark datasets show that our proposed method delivers significant improvements compared with the state-of-the-art baselines on the task of knowledge resolution.},
  isbn = {978-1-57735-701-8},
  keywords = {entity embedding,knowledge graph,knowledge representation,knowledge resolution,relation embedding}
}

@unpublished{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  date = {2014-05-01},
  eprint = {1405.0312},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1405.0312},
  urldate = {2019-08-23},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Zotero/storage/UNE2JRQU/linMicrosoftCOCOCommon2014.pdf;/Users/hugo/Zotero/storage/KX5F384N/1405.html}
}

@inproceedings{linROUGEPackageAutomatic2004,
  title = {{{ROUGE}}: {{A Package}} for {{Automatic Evaluation}} of {{Summaries}}},
  shorttitle = {{{ROUGE}}},
  booktitle = {Text {{Summarization Branches Out}}},
  author = {Lin, Chin-Yew},
  date = {2004-07},
  pages = {74--81},
  publisher = {{Association for Computational Linguistics}},
  location = {{Barcelona, Spain}},
  url = {https://aclanthology.org/W04-1013},
  urldate = {2022-08-04},
  file = {/Users/hugo/Papers/pdf/linROUGEPackageAutomatic2004.pdf}
}

@article{linTacticsAdversarialAttack2017,
  title = {Tactics of {{Adversarial Attack}} on {{Deep Reinforcement Learning Agents}}},
  author = {Lin, Yen-Chen and Hong, Zhang-Wei and Liao, Yuan-Hong and Shih, Meng-Li and Liu, Ming-Yu and Sun, Min},
  date = {2017-03-08},
  url = {https://arxiv.org/abs/1703.06748},
  urldate = {2018-11-27},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/linTacticsAdversarialAttack22.pdf;/Users/hugo/Papers/pdf/linTacticsAdversarialAttack3.pdf;/Users/hugo/Zotero/storage/LFNDTGA5/1703.html;/Users/hugo/Zotero/storage/WXQMKLXY/1703.html}
}

@unpublished{liPruningFiltersEfficient2016,
  title = {Pruning Filters for Efficient Convnets},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  date = {2016},
  eprint = {1608.08710},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/liPruningFiltersEfficient2016.pdf;/Users/hugo/Zotero/storage/QN7BM9KL/1608.html}
}

@unpublished{liRandomSearchReproducibility2019,
  title = {Random {{Search}} and {{Reproducibility}} for {{Neural Architecture Search}}},
  author = {Li, Liam and Talwalkar, Ameet},
  date = {2019-07-30},
  eprint = {1902.07638},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.07638},
  urldate = {2020-02-11},
  abstract = {Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks—PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS [41], a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on multiple runs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/YCRABRS9/liRandomSearchReproducibility2019.pdf}
}

@article{liskiBackstopTechnologyAdoption2006,
  title = {Backstop {{Technology Adoption}}},
  author = {Liski, Matti and Murto, Pauli},
  date = {2006-01},
  pages = {44},
  abstract = {We consider how efficient markets adopt technologies that reduce dependence on volatile factors such as oil. We find a relationship between volatility and technology overlap: new technology entry rate exceeds old technology exit rate under sufficient uncertainty. From this follows that efficient adoption is characterized by prolonged coexistence of alternative technologies and that uncertainty increasingly propagates from input to output market despite the declining use of the volatile factor in production. The properties depend on (i) the option to remain idle rather than exit, (ii) heterogeneity in factor supply, and (iii) factor market volatility.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/liskiBackstopTechnologyAdoption2006.pdf}
}

@unpublished{liStrongGeneralizationEfficiency2020,
  title = {Strong {{Generalization}} and {{Efficiency}} in {{Neural Programs}}},
  author = {Li, Yujia and Gimeno, Felix and Kohli, Pushmeet and Vinyals, Oriol},
  date = {2020-07-08},
  eprint = {2007.03629},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.03629},
  urldate = {2020-07-10},
  abstract = {We study the problem of learning efficient algorithms that strongly generalize in the framework of neural program induction. By carefully designing the input / output interfaces of the neural model and through imitation, we are able to learn models that produce correct results for arbitrary input sizes, achieving strong generalization. Moreover, by using reinforcement learning, we optimize for program efficiency metrics, and discover new algorithms that surpass the teacher used in imitation. With this, our approach can learn to outperform custom-written solutions for a variety of problems, as we tested it on sorting, searching in ordered lists and the NP-complete 0/1 knapsack problem, which sets a notable milestone in the field of Neural Program Induction. As highlights, our learned model can perform sorting perfectly on any input data size we tested on, with \$O(n log n)\$ complexity, whilst outperforming hand-coded algorithms, including quick sort, in number of operations even for list sizes far beyond those seen during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/liStrongGeneralizationEfficiency2020.pdf;/Users/hugo/Zotero/storage/SWBG4FYP/2007.html}
}

@inproceedings{liTextBuggerGeneratingAdversarial2019,
  title = {{{TextBugger}}: {{Generating Adversarial Text Against Real-world Applications}}},
  shorttitle = {{{TextBugger}}},
  booktitle = {26th {{Annual Network}} and {{Distributed System Security Symposium}}, {{NDSS}} 2019, {{San Diego}}, {{California}}, {{USA}}, {{February}} 24-27, 2019},
  author = {Li, Jinfeng and Ji, Shouling and Du, Tianyu and Li, Bo and Wang, Ting},
  date = {2019},
  publisher = {{The Internet Society}},
  url = {https://www.ndss-symposium.org/ndss-paper/textbugger-generating-adversarial-text-against-real-world-applications/},
  urldate = {2022-08-02}
}

@article{litjensSurveyDeepLearning2017,
  title = {A Survey on Deep Learning in Medical Image Analysis},
  author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sánchez, Clara I.},
  options = {useprefix=true},
  date = {2017-12-01},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Medical Image Analysis},
  volume = {42},
  pages = {60--88},
  issn = {1361-8415},
  doi = {10.1016/j.media.2017.07.005},
  url = {https://www.sciencedirect.com/science/article/pii/S1361841517301135},
  urldate = {2022-05-11},
  abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
  langid = {english},
  keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
  file = {/Users/hugo/Papers/pdf/litjensSurveyDeepLearning2017.pdf;/Users/hugo/Zotero/storage/Z6FVT7B2/S1361841517301135.html}
}

@article{liTransitionPhenomenaCellular1990,
  title = {Transition Phenomena in Cellular Automata Rule Space},
  author = {Li, Wentian and Packard, Norman H. and Langton, Chris G.},
  date = {1990-09-02},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {45},
  number = {1},
  pages = {77--94},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(90)90175-O},
  url = {http://www.sciencedirect.com/science/article/pii/016727899090175O},
  urldate = {2019-09-04},
  abstract = {We define several qualitative classes of cellular automata (CA) behavior, based on various statistical measures, and describe how the space of all cellular automata is organized. As a cellular automaton is changed by varying entries in its rule table, abrupt changes in qualitative behavior may occur. These abrupt changes have the character of bifurcations in smooth dynamical systems, or of phase transitions in statistical mechanical systems. The most complex CA rules exhibit many of the characteristics of second-order transitions, suggesting an association between computation, complexity, and critical phenomena.},
  file = {/Users/hugo/Papers/pdf/liTransitionPhenomenaCellular12.pdf}
}

@inproceedings{liuBadGlobalMinima2020,
  title = {Bad {{Global Minima Exist}} and {{SGD Can Reach Them}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33: {{Annual Conference}} on {{Neural Information Processing Systems}} 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Liu, Shengchao and Papailiopoulos, Dimitris S. and Achlioptas, Dimitris},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  date = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/618491e20a9b686b79e158c293ab4f91-Abstract.html},
  urldate = {2022-03-07}
}

@unpublished{liuDARTSDifferentiableArchitecture2019,
  title = {{{DARTS}}: {{Differentiable Architecture Search}}},
  shorttitle = {{{DARTS}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  date = {2019-04-23},
  eprint = {1806.09055},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.09055},
  urldate = {2019-12-13},
  abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/RVM982EC/liuDARTSDifferentiableArchitecture2019.pdf}
}

@misc{liuMultilingualDenoisingPretraining2020,
  title = {Multilingual {{Denoising Pre-training}} for {{Neural Machine Translation}}},
  author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  date = {2020-01-23},
  number = {arXiv:2001.08210},
  eprint = {2001.08210},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.08210},
  url = {http://arxiv.org/abs/2001.08210},
  urldate = {2022-07-26},
  abstract = {This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is one of the first methods for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/liuMultilingualDenoisingPretraining2020.pdf;/Users/hugo/Zotero/storage/VES3SHF9/2001.html}
}

@article{liUnderstandingNeuralNetworks2016,
  title = {Understanding {{Neural Networks}} through {{Representation Erasure}}},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1612.08220},
  eprint = {1612.08220},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1612.08220},
  urldate = {2022-08-02},
  archiveprefix = {arXiv}
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2019-07-26},
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2022-07-26},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/liuRoBERTaRobustlyOptimized2019.pdf;/Users/hugo/Zotero/storage/X8V8BB3V/1907.html}
}

@misc{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  number = {arXiv:2103.14030},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.14030},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2022-07-27},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/liuSwinTransformerHierarchical2021.pdf;/Users/hugo/Zotero/storage/5QLDLBAY/2103.html}
}

@inproceedings{liuUnsupervisedImagetoImageTranslation2017,
  title = {Unsupervised {{Image-to-Image Translation Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
  date = {2017},
  pages = {9},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.},
  eventtitle = {31st {{Conference}} on {{Neural Information Processing Systems}} ({{NIPS}} 2017)},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/liuUnsupervisedImagetoImageTranslation22.pdf}
}

@unpublished{loboCURIECellularAutomaton2020,
  title = {{{CURIE}}: {{A Cellular Automaton}} for {{Concept Drift Detection}}},
  shorttitle = {{{CURIE}}},
  author = {Lobo, Jesus L. and Del Ser, Javier and Osaba, Eneko and Bifet, Albert and Herrera, Francisco},
  date = {2020-09-21},
  eprint = {2009.09677},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.09677},
  urldate = {2020-09-22},
  abstract = {Data stream mining extracts information from large quantities of data flowing fast and continuously (data streams). They are usually affected by changes in the data distribution, giving rise to a phenomenon referred to as concept drift. Thus, learning models must detect and adapt to such changes, so as to exhibit a good predictive performance after a drift has occurred. In this regard, the development of effective drift detection algorithms becomes a key factor in data stream mining. In this work we propose CU RIE, a drift detector relying on cellular automata. Specifically, in CU RIE the distribution of the data stream is represented in the grid of a cellular automata, whose neighborhood rule can then be utilized to detect possible distribution changes over the stream. Computer simulations are presented and discussed to show that CU RIE, when hybridized with other base learners, renders a competitive behavior in terms of detection metrics and classification accuracy. CU RIE is compared with well-established drift detectors over synthetic datasets with varying drift characteristics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/loboCURIECellularAutomaton2020.pdf}
}

@article{lopez-pazGradientEpisodicMemory,
  title = {Gradient {{Episodic Memory}} for {{Continual Learning}}},
  author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
  pages = {10},
  abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/lopez-pazGradientEpisodicMemory.pdf}
}

@article{lorenzEmergenceModularityBiological2011,
  title = {The {{Emergence}} of {{Modularity}} in {{Biological Systems}}},
  author = {Lorenz, Dirk M. and Jeng, Alice and Deem, Michael W.},
  date = {2011-06},
  journaltitle = {Physics of life reviews},
  shortjournal = {Phys Life Rev},
  volume = {8},
  number = {2},
  eprint = {21353651},
  eprinttype = {pmid},
  pages = {129--160},
  issn = {1571-0645},
  doi = {10.1016/j.plrev.2011.02.003},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4477837/},
  urldate = {2022-11-08},
  abstract = {In this review, we discuss modularity and hierarchy in biological systems. We review examples from protein structure, genetics, and biological networks of modular partitioning of the geometry of biological space. We review theories to explain modular organization of biology, with a focus on explaining how biology may spontaneously organize to a structured form. That is, we seek to explain how biology nucleated from among the many possibilities in chemistry. The emergence of modular organization of biological structure will be described as a symmetry-breaking phase transition, with modularity as the order parameter. Experimental support for this description will be reviewed. Examples will be presented from pathogen structure, metabolic networks, gene networks, and protein-protein interaction networks. Additional examples will be presented from ecological food networks, developmental pathways, physiology, and social networks. There once were two watchmakers, named Hora and Tempus, who manufactured very fine watches. Both of them were highly regarded, and the phones in their workshops rang frequently — new customers were constantly calling them. However, Hora prospered, while Tempus became poorer and poorer and finally lost his shop. What was the reason? The watches the men made consisted of about 1,000 parts each. Tempus had so constructed his that if he had one partly assembled and had to put it down — to answer the phone say— it immediately fell to pieces and had to be reassembled from the elements. The better the customers liked his watches, the more they phoned him, the more difficult it became for him to find enough uninterrupted time to finish a watch. The watches that Hora made were no less complex than those of Tempus. But he had designed them so that he could put together subassemblies of about ten elements each. Ten of these subassemblies, again, could be put together into a larger subassembly; and a system of ten of the latter sub-assemblies constituted the whole watch. Hence, when Hora had to put down a partly assembled watch in order to answer the phone, he lost only a small part of his work, and he assembled his watches in only a fraction of the man-hours it took Tempus.”H. A. Simon, The Architecture of Complexity, 1962 [].},
  pmcid = {PMC4477837},
  file = {/Users/hugo/Papers/pdf/lorenzEmergenceModularityBiological2011.pdf}
}

@article{lukoseviciusReservoirComputingApproaches2009,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  author = {Lukoševičius, Mantas and Jaeger, Herbert},
  date = {2009},
  journaltitle = {Computer Science Review},
  volume = {3},
  number = {3},
  pages = {127--149},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Zotero/storage/IC3YJ3JM/S1574013709000173.html}
}

@article{lukoseviciusReservoirComputingTrends2012,
  title = {Reservoir Computing Trends},
  author = {Lukoševičius, Mantas and Jaeger, Herbert and Schrauwen, Benjamin},
  date = {2012},
  journaltitle = {KI-Künstliche Intelligenz},
  volume = {26},
  number = {4},
  pages = {365--371},
  publisher = {{Springer}},
  file = {/Users/hugo/Papers/pdf/lukoseviciusReservoirComputingTrends2012.pdf}
}

@unpublished{luPretrainedTransformersUniversal2021,
  title = {Pretrained {{Transformers}} as {{Universal Computation Engines}}},
  author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  date = {2021-03-09},
  eprint = {2103.05247},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.05247},
  urldate = {2021-03-12},
  abstract = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/CKYRV8Y6/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf;/Users/hugo/Zotero/storage/ZXTH9FB2/2103.html}
}

@unpublished{luPretrainedTransformersUniversal2021a,
  title = {Pretrained {{Transformers}} as {{Universal Computation Engines}}},
  author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  date = {2021-03-09},
  eprint = {2103.05247},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.05247},
  urldate = {2021-03-29},
  abstract = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language improves performance and compute efficiency on non-language downstream tasks. In particular, we find that such pretraining enables FPT to generalize in zero-shot to these modalities, matching the performance of a transformer fully trained on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/CST43SCC/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf;/Users/hugo/Zotero/storage/GVGVMNIG/2103.html}
}

@article{luvalleEffectsBoundaryConditions2019,
  title = {The {{Effects}} of {{Boundary Conditions}} on {{Cellular Automata}}},
  author = {LuValle, Brian J.},
  date = {2019-03-28},
  journaltitle = {Complex Systems},
  volume = {28},
  number = {1},
  pages = {97--124},
  issn = {08912513},
  doi = {10.25088/ComplexSystems.28.1.97},
  url = {https://www.complex-systems.com/abstracts/v28_i01_a05/},
  urldate = {2019-06-21},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/TDKDRV97/luvalleEffectsBoundaryConditions2019.pdf}
}

@inproceedings{maasLearningWordVectors2011,
  title = {Learning {{Word Vectors}} for {{Sentiment Analysis}}},
  booktitle = {The 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Proceedings}} of the {{Conference}}, 19-24 {{June}}, 2011, {{Portland}}, {{Oregon}}, {{USA}}},
  author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  editor = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
  date = {2011},
  pages = {142--150},
  publisher = {{The Association for Computer Linguistics}},
  url = {https://aclanthology.org/P11-1015/},
  urldate = {2022-05-10}
}

@article{maassRealTimeComputingStable2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
  date = {2002-11-01},
  journaltitle = {Neural Computation},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/089976602760407955},
  urldate = {2020-10-05},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/maassRealTimeComputingStable2002.pdf}
}

@article{mackayBayesianNeuralNetworks1995,
  title = {Bayesian Neural Networks and Density Networks},
  author = {MacKay, David JC},
  date = {1995},
  journaltitle = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  volume = {354},
  number = {1},
  pages = {73--80},
  publisher = {{Elsevier}}
}

@unpublished{maddoxRethinkingParameterCounting2020,
  title = {Rethinking {{Parameter Counting}} in {{Deep Models}}: {{Effective Dimensionality Revisited}}},
  shorttitle = {Rethinking {{Parameter Counting}} in {{Deep Models}}},
  author = {Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
  date = {2020-05-25},
  eprint = {2003.02139},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2003.02139},
  urldate = {2022-03-20},
  abstract = {Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, width-depth tradeoffs, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models. We also show that effective dimensionality compares favourably to alternative norm- and flatness- based generalization measures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/maddoxRethinkingParameterCounting2020.pdf;/Users/hugo/Zotero/storage/T3AU8IMU/2003.html}
}

@article{madhavanWebscaleDataIntegration2007,
  title = {Web-Scale {{Data Integration}} : {{You}} Can Only Afford to {{Pay As You Go}}},
  author = {Madhavan, Jayant and Jeffery, Shawn R and Cohen, Shirley and Dong, Xin (Luna) and Ko, David and Yu, Cong and Halevy, Alon},
  date = {2007},
  journaltitle = {Cidr 2007},
  volume = {7},
  pages = {342--350},
  url = {https://web.eecs.umich.edu/ congy/work/cidr07.pdf},
  abstract = {The World Wide Web is witnessing an increase in the amount of structured content vast heterogeneous collections of structured data are on the rise due to the DeepWeb, annotation schemes like Flickr, and sites like Google Base. While this phenomenon is cre- ating an opportunity for structured data management, dealing with heterogeneity on the web-scale presents many new challenges. In this paper, we highlight these challenges in two scenarios the Deep Web and Google Base. We contend that traditional data in- tegration techniques are no longer valid in the face of such hetero- geneity and scale. We propose a new data integration architecture, PAYGO, which is inspired by the concept of dataspaces and em- phasizes pay-as-you-go data management as means for achieving web-scale data integration.},
  file = {/Users/hugo/Papers/pdf/madhavanWebscaleDataIntegration22.pdf}
}

@inproceedings{magieraNovelAlgorithmCoarseGraining2014,
  title = {A {{Novel Algorithm}} for {{Coarse-Graining}} of {{Cellular Automata}}},
  booktitle = {Cellular {{Automata}}},
  author = {Magiera, Krzysztof and Dzwinel, Witold},
  editor = {Was, Jaroslaw and Sirakoulis, Georgios Ch. and Bandini, Stefania},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {258--267},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-11520-7_27},
  abstract = {The coarse-graining is an approximation procedure widely used for simplification of mathematical and numerical models of multiscale systems. It reduces superfluous – microscopic – degrees of freedom. Israeli and Goldenfeld demonstrated in [1,2] that the coarse-graining can be employed for elementary cellular automata (CA), producing interesting interdependences between them. However, extending their investigation on more complex CA rules appeared to be impossible due to the high computational complexity of the coarse-graining algorithm. We demonstrate here that this complexity can be substantially decreased. It allows for scrutinizing much broader class of cellular automata in terms of their coarse graining. By using our algorithm we found out that the ratio of the numbers of elementary CAs having coarse grained representation to “degenerate” – irreducible – cellular automata, strongly increases with increasing the “grain” size of the approximation procedure. This rises principal questions about the formal limits in modeling of realistic multiscale systems.},
  isbn = {978-3-319-11520-7},
  langid = {english},
  keywords = {cellular automata,coarse graining,multiscale systems},
  file = {/Users/hugo/Papers/pdf/magieraNovelAlgorithmCoarseGraining2014.pdf}
}

@inproceedings{maheshwaryGeneratingNaturalLanguage2021,
  title = {Generating {{Natural Language Attacks}} in a {{Hard Label Black Box Setting}}},
  booktitle = {Thirty-{{Fifth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2021, {{Thirty-Third Conference}} on {{Innovative Applications}} of {{Artificial Intelligence}}, {{IAAI}} 2021, {{The Eleventh Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2021, {{Virtual Event}}, {{February}} 2-9, 2021},
  author = {Maheshwary, Rishabh and Maheshwary, Saket and Pudi, Vikram},
  date = {2021},
  pages = {13525--13533},
  publisher = {{AAAI Press}},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17595},
  urldate = {2022-08-02}
}

@article{mahoneyAdaptiveWeighingContext2005,
  title = {Adaptive {{Weighing}} of {{Context Models}} for {{Lossless Data Compression}}},
  author = {Mahoney, Matthew V},
  date = {2005},
  journaltitle = {-},
  pages = {6},
  abstract = {Until recently the state of the art in lossless data compression was prediction by partial match (PPM). A PPM model estimates the next-symbol probability distribution by combining statistics from the longest matching contiguous contexts in which each symbol value is found. We introduce a context mixing model which improves on PPM by allowing contexts which are arbitrary functions of the history. Each model independently estimates a probability and confidence that the next bit of data will be 0 or 1. Predictions are combined by weighted averaging. After a bit is arithmetic coded, the weights are adjusted along the cost gradient in weight space to favor the most accurate models. Context mixing compressors, as implemented by the open source PAQ project, are now top ranked on several independent benchmarks.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mahoneyAdaptiveWeighingContext22.pdf}
}

@inproceedings{mahoneyFastTextCompression2000,
  title = {Fast {{Text Compression}} with {{Neural Networks}}},
  booktitle = {{{FLAIRS}}},
  author = {Mahoney, Matthew V},
  date = {2000},
  pages = {5},
  abstract = {Neural networks have the potential to extend data compression algorithms beyond the character level n-gram models now in use, but have usually been avoided because they are too slow to be practical. We introduce a model that produces better compression than popular Limpel-Ziv compressors (zip, gzip, compress), and is competitive in time, space, and compression ratio with PPM and BurrowsWheeler algorithms, currently the best known. The compressor, a bit-level predictive arithmetic encoder using a 2 layer, 4 × 106 by 1 network, is fast (about 104 characters/second) because only 4-5 connections are simultaneously active and because it uses a variable learning rate optimized for one-pass training.},
  eventtitle = {{{FLAIRS Conference}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mahoneyFastTextCompression22.pdf}
}

@article{mahoneyPAQ1DataCompression2002,
  title = {The {{PAQ1}} Data Compression Program},
  author = {Mahoney, Matthew V.},
  date = {2002},
  journaltitle = {Draft, Jan},
  volume = {20},
  file = {/Users/hugo/Papers/pdf/mahoneyPAQ1DataCompression22.pdf;/Users/hugo/Papers/pdf/mahoneyPAQ1DataCompression3.pdf}
}

@inproceedings{mahoneyTextCompressionTest1999,
  title = {Text {{Compression}} as a {{Test}} for {{Artificial Intelligence}}},
  booktitle = {Proceedings of {{AAAI-1999}}},
  author = {Mahoney, Matthew V},
  date = {1999},
  pages = {3},
  eventtitle = {{{AAAI}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mahoneyTextCompressionTest1999.pdf}
}

@article{makinMachineTranslationCortical2020,
  title = {Machine Translation of Cortical Activity to Text with an Encoder–Decoder Framework},
  author = {Makin, Joseph G. and Moses, David A. and Chang, Edward F.},
  date = {2020-03-30},
  journaltitle = {Nature Neuroscience},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/s41593-020-0608-8},
  url = {https://www.nature.com/articles/s41593-020-0608-8},
  urldate = {2020-03-31},
  abstract = {A decade after speech was first decoded from human brain signals, accuracy and speed remain far below that of natural speech. Here we show how to decode the electrocorticogram with high accuracy and at natural-speech rates. Taking a cue from recent advances in machine translation, we train a recurrent neural network to encode each sentence-length sequence of neural activity into an abstract representation, and then to decode this representation, word by word, into an English sentence. For each participant, data consist of several spoken repeats of a set of 30–50 sentences, along with the contemporaneous signals from \textasciitilde 250 electrodes distributed over peri-Sylvian cortices. Average word error rates across a held-out repeat set are as low as 3\%. Finally, we show how decoding with limited data can be improved with transfer learning, by training certain layers of the network under multiple participants’ data. Makin and colleagues decode speech from neural signals recorded during a preoperative procedure, using an algorithm inspired by machine translation. For one participant reading from a closed set of 50 sentences, decoding accuracy is nearly perfect.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/LSE5UN8T/s41593-020-0608-8.html}
}

@inproceedings{maleyFourStepsOpenended1999,
  title = {Four Steps toward Open-Ended Evolution},
  booktitle = {Proceedings of the 1st {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}} - {{Volume}} 2},
  author = {Maley, C. C.},
  date = {1999-07-13},
  series = {{{GECCO}}'99},
  pages = {1336--1343},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{San Francisco, CA, USA}},
  abstract = {In 1998, Bedau et al. defined a set of metrics for characterizing the long-term evolutionary dynamics of a system. They argued that no known artificial system demonstrates the unbounded evolutionary activity observed in the fossil record. In response we have developed a series of toy models that approach and eventually succeed in demonstrating unbounded evolutionary activity. The underwhelming success of the models suggests that there must be more to open-ended evolution than just unbounded evolutionary activity as the term is currently defined. We derive some potential extensions to the metrics and requirements for developing open-ended evolution in an artificial system.},
  isbn = {978-1-55860-611-1},
  file = {/Users/hugo/Papers/pdf/maleyFourStepsOpenended1999.pdf}
}

@article{manukyanLivingMesoscopicCellular2017a,
  title = {A Living Mesoscopic Cellular Automaton Made of Skin Scales},
  author = {Manukyan, Liana and Montandon, Sophie A. and Fofonjka, Anamarija and Smirnov, Stanislav and Milinkovitch, Michel C.},
  date = {2017-04},
  journaltitle = {Nature},
  volume = {544},
  number = {7649},
  pages = {173--179},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature22031},
  url = {http://www.nature.com/articles/nature22031},
  urldate = {2020-02-26},
  langid = {english}
}

@report{margemExperimentalStudyCellular2017,
  title = {An Experimental Study on Cellular Automata Reservoir in Pathological Sequence Learning Tasks},
  author = {Margem, Mrwan and Yilmaz, Ozgür},
  date = {2017},
  institution = {{Jul}},
  abstract = {In this study, we are providing a recurrent architecture based on Cellular Automata state evolution. The states of the cells are used as the reservoir of activities as in Echo State Networks. The projection of the input onto this reservoir medium provides a systematic way of remembering previous inputs and combining the memory with a continuous stream of inputs. This is an essential capability for a wide collection of intelligence tasks such as language, continuous vision (i.e. video), symbolic manipulation in a knowledge base etc. We devise intuitive methods for enlarging the memory capacity of the cellular autmata state space and reducing the interference between memory traces. Cellular Automata reservoir constructs a novel bridge between computational theory of automata and neural architectures. The proposed framework is tested on classical synthetic pathological tasks that are widely used in evaluating recurrent algorithms. We show that the proposed algorithm achieves zero error in all tasks, giving a similar performance with Echo State Networks, but even better in many different aspects. Two other methods are also introduced to training recurrent neural networks; “Covariance representation” that has second order attribute statistics and “Stack representation” that has a local representation, and compare them with Cellular Automata based Reservoir Comuting framework that has higher attribute statistics and distributed representation. This raises the question of whether real valued neuron units are essential for solving complex problems that are distributed over time. Our results suggest that, even very sparsely connected binary units with simple computational rules can provide the required computation for intelligent behavior.}
}

@article{margemReservoirComputingBased2018,
  title = {Reservoir {{Computing}} Based on {{Cellular Automata}} ({{ReCA}}) in {{Sequence Learning}}},
  author = {Margem, Mrwan and Gedik, Osman},
  date = {2018-12-17},
  journaltitle = {Journal of cellular automata},
  shortjournal = {Journal of cellular automata},
  volume = {14},
  pages = {153--170},
  abstract = {ReCA is a reservoir computing architecture based on cellular automata in which the inputs pass on a cellular automaton instead of a recurrent neural network reservoir. ReCA has been tested using pathological synthetic sequence tasks (well-known benchmark tasks within the reservoir computing (RC) community) and has been showing promising results that reduce complexity compared with other RC approaches such as echo state networks (ESNs). In this paper, a number of methods for feature extraction from the cellular automata (CA) reservoir are introduced to improve ReCA by reducing its complexity while maintaining accuracy. The proposed method reduces the feature dimension by using a few states from every time step (EACH) in the reservoir and/or using only one side of the CA evolution (HALF) and/or reducing the CA evolution in space (expansion ratio ƒ). Due to the rich dynamics of the CA reservoir, the three methods of reduction (EACH, HALF, and ƒ) can be used together to reduce the feature dimension by up to 98\% in some pathological tasks compared with the state-of-the-art ReCA results.}
}

@article{margensternPolynomialSolution3SAT1999,
  title = {A {{Polynomial Solution}} for 3-{{SAT}} in the {{Space}} of {{Cellular Automata}} in the {{Hyperbolic Plane}}.},
  author = {Margenstern, Maurice},
  date = {1999},
  journaltitle = {J. UCS},
  volume = {5},
  number = {9},
  pages = {563--573},
  doi = {10.3217/jucs-005-09-0563},
  file = {/Users/hugo/Papers/pdf/margensternPolynomialSolution3SAT1999.pdf;/Users/hugo/Zotero/storage/NHAIL9YM/MargensternMaurice.html}
}

@article{marshallQuantifyingPathwaysLife2019,
  title = {Quantifying the Pathways to Life Using Assembly Spaces},
  author = {Marshall, Stuart M and Moore, Douglas G and Murray, Alastair R G and Walker, Sara I},
  date = {2019-07},
  pages = {30},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/marshallQuantifyingPathwaysLife2019.pdf}
}

@article{martinezCompleteCharacterizationStructure2014,
  title = {Complete Characterization of Structure of Rule 54},
  author = {Martinez, Genaro J. and Adamatzky, Andrew and McIntosh, Harold V.},
  date = {2014-10-08},
  journaltitle = {Complex Systems},
  volume = {23},
  number = {3},
  issn = {0891-2513},
  url = {https://uwe-repository.worktribe.com/output/810658},
  urldate = {2020-06-16},
  abstract = {The dynamics of rule 54 one-dimensional two-state cellular automaton (CA) are a discrete analog of a space-time dynamics of excitations in nonlinear active medium with mutual inhibition. A cell switches its state 0 to state 1 if one of its two neighbors is in state 1 (propagation of a perturbation) and a cell remains in state 1 only if its two neighbors are in state 0. A lateral inhibition is because a 1-state neighbor causes a 1-state cell to switch to state 0. The rule produces a rich spectrum of space-time dynamics, including gliders and glider guns just from four primitive gliders. We construct a catalogue of gliders and describe them by tiles. We calculate a subset of regular expressions \$\textbackslash Psi\_\{R54\}\$ to encode gliders. The regular expressions are derived from de Bruijn diagrams, tile-based representation of gliders, and cycle diagrams sometimes. We construct an abstract machine that recognizes regular expressions of gliders in rule 54 and validate \$\textbackslash Psi\_\{R54\}\$. We also propose a way to code initial configurations of gliders to depict any type of collision between the gliders and explore self-organization of gliders, formation of larger tiles, and soliton-like interactions of gliders and computable devices.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/martinezCompleteCharacterizationStructure2014.pdf;/Users/hugo/Zotero/storage/GX24LJJM/810658.html}
}

@unpublished{martinezComputationUniversalityClass2013,
  title = {Computation and {{Universality}}: {{Class IV}} versus {{Class III Cellular Automata}}},
  shorttitle = {Computation and {{Universality}}},
  author = {Martinez, Genaro J. and Seck-Tuoh-Mora, Juan C. and Zenil, Hector},
  date = {2013-04-04},
  eprint = {1304.1242},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1304.1242},
  urldate = {2019-05-30},
  abstract = {This paper examines the claim that cellular automata (CA) belonging to Class III (in Wolfram’s classification) are capable of (Turing universal) computation. We explore some chaotic CA (believed to belong to Class III) reported over the course of the CA history, that may be candidates for universal computation, hence spurring the discussion on Turing universality on both Wolfram’s classes III and IV.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Zotero/storage/YNWARAYX/martinezComputationUniversalityClass2013.pdf}
}

@article{martinezHowMakeDull2010,
  title = {How to Make Dull Cellular Automata Complex by Adding Memory: {{Rule}} 126 Case Study},
  shorttitle = {How to Make Dull Cellular Automata Complex by Adding Memory},
  author = {Martínez, Genaro J. and Adamatzky, Andrew and Seck-Tuoh-Mora, Juan C. and Alonso-Sanz, Ramon},
  date = {2010},
  journaltitle = {Complexity},
  shortjournal = {Complexity},
  pages = {NA-NA},
  issn = {10762787, 10990526},
  doi = {10.1002/cplx.20311},
  url = {http://doi.wiley.com/10.1002/cplx.20311},
  urldate = {2020-06-19},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/martinezHowMakeDull2010.pdf}
}

@unpublished{martinezNoteElementaryCellular2013,
  title = {A {{Note}} on {{Elementary Cellular Automata Classification}}},
  author = {Martinez, Genaro J.},
  date = {2013-06-24},
  eprint = {1306.5577},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1306.5577},
  urldate = {2019-09-04},
  abstract = {We overview and compare classifications of elementary cellular automata, including Wolfram’s, Wuensche’s, Li and Packard, communication complexity, power spectral, topological, surface, compression, lattices, and morphological diversity classifications. This paper summarises several classifications of elementary cellular automata (ECA) and compares them with a newly proposed one, that induced by endowing rules with memory.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/martinezNoteElementaryCellular22.pdf}
}

@article{martinGroupInterpretationParticles2000,
  title = {A {{Group Interpretation Of Particles Generated By One-Dimensional Cellular Automaton}}, {{Wolfram}}'{{S Rule}} 54},
  author = {Martin, Bruno},
  date = {2000},
  journaltitle = {International Journal of Modern Physics C (IJMPC)},
  volume = {11},
  number = {01},
  pages = {101--123},
  publisher = {{World Scientific Publishing Co. Pte. Ltd.}},
  url = {https://ideas.repec.org/a/wsi/ijmpcx/v11y2000i01ns0129183100000109.html},
  urldate = {2020-03-31},
  abstract = {One-dimensional cellular automata are known to be able to present complex behaviors. In some cases, their evolution may be understood as movings, collisions, or creations of particles. In the case of the special Wolfram's rule 54, Boccara has previously pointed out basic particles. In this paper, we introduce a group which allows the formal study of interactions between these particles. Coming back to the complexity of rule 54 and using the new algebraic classification of Rapaport, we prove that rule 54 is not simple.},
  langid = {english},
  keywords = {Cellular Automaton,Filter,Particle,Tiling},
  file = {/Users/hugo/Papers/pdf/martinGroupInterpretationParticles2000.pdf;/Users/hugo/Zotero/storage/PJD83EEX/v11y2000i01ns0129183100000109.html}
}

@unpublished{mattheakisUnsupervisedReservoirComputing2021,
  title = {Unsupervised {{Reservoir Computing}} for {{Solving Ordinary Differential Equations}}},
  author = {Mattheakis, Marios and Joy, Hayden and Protopapas, Pavlos},
  date = {2021-08-25},
  eprint = {2108.11417},
  eprinttype = {arxiv},
  primaryclass = {physics},
  url = {http://arxiv.org/abs/2108.11417},
  urldate = {2021-08-30},
  abstract = {There is a wave of interest in using unsupervised neural networks for solving differential equations. The existing methods are based on feed-forward networks, while recurrent neural network differential equation solvers have not yet been reported. We introduce an unsupervised reservoir computing (RC), an echo-state recurrent neural network capable of discovering approximate solutions that satisfy ordinary differential equations (ODEs). We suggest an approach to calculate time derivatives of recurrent neural network outputs without using backpropagation. The internal weights of an RC are fixed, while only a linear output layer is trained, yielding efficient training. However, RC performance strongly depends on finding the optimal hyper-parameters, which is a computationally expensive process. We use Bayesian optimization to efficiently discover optimal sets in a high-dimensional hyper-parameter space and numerically show that one set is robust and can be used to solve an ODE for different initial conditions and time ranges. A closed-form formula for the optimal output weights is derived to solve first order linear equations in a backpropagation-free learning process. We extend the RC approach by solving nonlinear system of ODEs using a hybrid optimization method consisting of gradient descent and Bayesian optimization. Evaluation of linear and nonlinear systems of equations demonstrates the efficiency of the RC ODE solver.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics},
  file = {/Users/hugo/Papers/pdf/mattheakisUnsupervisedReservoirComputing2021.pdf}
}

@article{maturanaOrganizationLivingTheory1975,
  title = {The Organization of the Living: {{A}} Theory of the Living Organization},
  shorttitle = {The Organization of the Living},
  author = {Maturana, Humberto R.},
  date = {1975-05-01},
  journaltitle = {International Journal of Man-Machine Studies},
  shortjournal = {International Journal of Man-Machine Studies},
  volume = {7},
  number = {3},
  pages = {313--332},
  issn = {0020-7373},
  doi = {10.1016/S0020-7373(75)80015-0},
  url = {http://www.sciencedirect.com/science/article/pii/S0020737375800150},
  urldate = {2020-07-20},
  abstract = {The fundamental feature that characterizes living systems is autonomy, and any account of their organization as systems that can exist as individual unities must show what autonomy is as a phenomenon proper to them, and how it arises in their operation as such unities. Accordingly the following is proposed. (1) That autonomy in living systems is a feature of self-production (autopoiesis), and that a living system is properly characterized only as a network of processes of production of components that is continuously, and recursively, generated and realized as a concrete entity (unity) in the physical space, by the interactions of the same components that it produces as such a network. This organization I call the autopoietic organization, and any system that exhibits it is an autopoietic system in the space in which its components exist; in this sense living systems are autopoietic systems in the physical space. (2) That the basic consequence of the autopoietic organization is that everything that takes place in an autopoietic system is subordinated to the realization of its autopoiesis, otherwise it disintegrates. (3) That the fundamental feature that characterizes the nervous system is that it is a closed network of interacting neurons in which every state of neuronal activity generates other states of neuronal activity. Since the nervous system is a component subsystem in an autopoietic unity, it operates by generating states of relative neuronal activity that participate in the realization of the autopoiesis of the organism which it integrates. (4) That the autopoietic states that an organism adopts are determined by its structure (the structure of the nervous system included), and that the structure of the organism (including its nervous system) is at any instant the result of its evolutionary and ontogenic structural coupling with the medium in which it is autopoietic, obtained while the autopoiesis is realized. (5) That language arises as phenomenon proper to living systems from the reciprocal structural coupling of at least two organisms with nervous systems, and that self-consciousness arises as an individual phenomenon from the recursive structural coupling of an organism with language with its own structure through recursive self-description.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/maturanaOrganizationLivingTheory1975.pdf;/Users/hugo/Zotero/storage/XDCPEI5T/S0020737375800150.html}
}

@unpublished{maziarkaMoleculeAttentionTransformer2020,
  title = {Molecule {{Attention Transformer}}},
  author = {Maziarka, Łukasz and Danel, Tomasz and Mucha, Sławomir and Rataj, Krzysztof and Tabor, Jacek and Jastrzębski, Stanisław},
  date = {2020-02-19},
  eprint = {2002.08264},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  url = {http://arxiv.org/abs/2002.08264},
  urldate = {2020-02-21},
  abstract = {Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/R2HLRI4W/maziarkaMoleculeAttentionTransformer2020.pdf}
}

@inproceedings{mcdonaldReservoirComputingExtreme2017,
  title = {Reservoir Computing Extreme Learning Machines Using Pairs of Cellular Automata Rules},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {McDonald, Nathan},
  date = {2017-05},
  pages = {2429--2436},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966151},
  abstract = {A framework for implementing reservoir computing (RC) and extreme learning machines (ELMs), two types of artificial neural networks, based on 1D elementary Cellular Automata (CA) is presented, in which two separate CA rules explicitly implement the minimum computational requirements of the reservoir layer: hyperdimensional projection and short-term memory. CAs are cell-based state machines, which evolve in time in accordance with local rules based on a cell's current state and those of its neighbors. Notably, simple single cell shift rules as the memory rule in a fixed edge CA afforded reasonable success in conjunction with a variety of projection rules, potentially significantly reducing the optimal solution search space. Optimal iteration counts for the CA rule pairs can be estimated for some tasks based upon the category of the projection rule. Initial results support future hardware realization, where CAs potentially afford orders of magnitude reduction in size, weight, and power (SWaP) requirements compared with floating point RC implementations.},
  eventtitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {1D elementary cellular automata,artificial neural networks,Automata,CA rules,cell-based state machines,cellular automata,cellular automata (CA),cellular automata based reservoirs (ReCA),cellular automata rules,Chaos,ELM,Encoding,extreme learning machine (ELM),extreme learning machines,feedforward neural nets,finite state machines,Hardware,hyperdimensional projection,iterative methods,Learning automata,memory rule,Neurons,optimal iteration counts,optimal solution search space,optimisation,projection rules,reservoir computing,reservoir computing (RC),reservoir layer,Reservoirs,short-term memory,single cell shift rules,size-weight-and-power,SWaP},
  file = {/Users/hugo/Zotero/storage/YJB9U7IU/7966151.html}
}

@inproceedings{mcdonaldReservoirComputingExtreme2017a,
  title = {Reservoir Computing \& Extreme Learning Machines Using Pairs of Cellular Automata Rules},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {McDonald, Nathan},
  date = {2017-05},
  pages = {2429--2436},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966151},
  abstract = {A framework for implementing reservoir computing (RC) and extreme learning machines (ELMs), two types of artificial neural networks, based on 1D elementary Cellular Automata (CA) is presented, in which two separate CA rules explicitly implement the minimum computational requirements of the reservoir layer: hyperdimensional projection and short-term memory. CAs are cell-based state machines, which evolve in time in accordance with local rules based on a cell's current state and those of its neighbors. Notably, simple single cell shift rules as the memory rule in a fixed edge CA afforded reasonable success in conjunction with a variety of projection rules, potentially significantly reducing the optimal solution search space. Optimal iteration counts for the CA rule pairs can be estimated for some tasks based upon the category of the projection rule. Initial results support future hardware realization, where CAs potentially afford orders of magnitude reduction in size, weight, and power (SWaP) requirements compared with floating point RC implementations.},
  eventtitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  keywords = {Automata,cellular automata (CA),cellular automata based reservoirs (ReCA),Chaos,Encoding,extreme learning machine (ELM),Hardware,Learning automata,Neurons,reservoir computing (RC),Reservoirs},
  file = {/Users/hugo/Zotero/storage/R5A4KXJ7/7966151.html}
}

@article{mcglothlinRDFKBSemanticWeb,
  title = {{{RDFKB}}: {{A Semantic Web Knowledge Base}}},
  author = {McGlothlin, James P and Khan, Latifur and Thuraisingham, Bhavani},
  pages = {2},
  abstract = {There are many significant research projects focused on providing semantic web repositories that are scalable and efficient. However, the true value of the semantic web architecture is its ability to represent meaningful knowledge and not just data. Therefore, a semantic web knowledge base should do more than retrieve collections of triples. We propose RDFKB (Resource Description Knowledge Base), a complete semantic web knowledge case. RDFKB is a solution for managing, persisting and querying semantic web knowledge. Our experiments with real world and synthetic datasets demonstrate that RDFKB achieves superior query performance to other stateof-the-art solutions. The key features of RDFKB that differentiate it from other solutions are: 1) a simple and efficient process for data additions, deletions and updates that does not involve reprocessing the dataset; 2) materialization of inferred triples at addition time without performance degradation; 3) materialization of uncertain information and support for queries involving probabilities; 4) distributed inference across datasets; 5) ability to apply alignments to the dataset and perform queries against multiple sources using alignment. RDFKB allows more knowledge to be stored and retrieved; it is a repository not just for RDF datasets, but also for inferred triples, probability information, and lineage information. RDFKB provides a complete and efficient RDF data repository and knowledge base.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mcglothlinRDFKBSemanticWeb2.pdf}
}

@article{mcintoshWolframClassIV1990,
  title = {Wolfram's Class {{IV}} Automata and a Good Life},
  author = {McIntosh, Harold V.},
  date = {1990-09},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {45},
  number = {1-3},
  pages = {105--121},
  issn = {01672789},
  doi = {10.1016/0167-2789(90)90177-Q},
  url = {https://linkinghub.elsevier.com/retrieve/pii/016727899090177Q},
  urldate = {2020-05-06},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mcintoshWolframClassIV1990.pdf}
}

@inproceedings{mcmullinRediscoveringComputationalAutopoiesis1997,
  title = {Rediscovering Computational Autopoiesis},
  booktitle = {Fourth {{European Conference}} on {{Artificial Life}}},
  author = {McMullin, Barry and Varela, Francisco J.},
  date = {1997},
  pages = {38--47},
  file = {/Users/hugo/Papers/pdf/mcmullinRediscoveringComputationalAutopoiesis1997.pdf;/Users/hugo/Zotero/storage/DHT9YXNJ/books.html}
}

@article{mcmullinThirtyYearsComputational2004,
  title = {Thirty Years of Computational Autopoiesis: {{A}} Review},
  shorttitle = {Thirty Years of Computational Autopoiesis},
  author = {McMullin, Barry},
  date = {2004},
  journaltitle = {Artificial life},
  volume = {10},
  number = {3},
  pages = {277--295},
  publisher = {{MIT Press}},
  file = {/Users/hugo/Papers/pdf/mcmullinThirtyYearsComputational2004.pdf;/Users/hugo/Zotero/storage/PIC2FLSZ/6789181.html}
}

@article{mcsheaComplexityEvolutionWhat1991,
  title = {Complexity and Evolution: {{What}} Everybody Knows},
  shorttitle = {Complexity and Evolution},
  author = {McShea, Daniel W.},
  date = {1991-07},
  journaltitle = {Biology \& Philosophy},
  volume = {6},
  number = {3},
  pages = {303--324},
  issn = {0169-3867, 1572-8404},
  doi = {10.1007/BF00132234},
  url = {http://link.springer.com/10.1007/BF00132234},
  urldate = {2019-07-01},
  abstract = {The consensus among evolutionists seems to be (and has been for at least a century) that the morphological complexity of organisms increases in evolution, although almost no empirical evidence for such a trend exists. Most studies of complexity have been theoretical, and the few empirical studies have not, with the exception of certain recent ones, been especially rigorous; reviews are presented of both the theoretical and empirical literature. The paucity of evidence raises the question of what sustains the consensus, and a number of suggestions are offered, including the possibility that certain cultural and/or perceptual biases are at work. In addition, a shift in emphasis from theoretical to empirical inquiry is recommended for the study of complexity, and guidelines for future empirical studies are proposed.},
  langid = {english},
  file = {/home/hugo/Papers/pdf/mcsheaComplexityEvolutionWhat1991.pdf}
}

@article{merigotMultiscaleApproachOptimal2011,
  title = {A {{Multiscale Approach}} to {{Optimal Transport}}},
  author = {Mérigot, Quentin},
  date = {2011-08},
  journaltitle = {Computer Graphics Forum},
  volume = {30},
  number = {5},
  pages = {1583--1592},
  issn = {01677055},
  doi = {10.1111/j.1467-8659.2011.02032.x},
  url = {http://doi.wiley.com/10.1111/j.1467-8659.2011.02032.x},
  urldate = {2018-11-29},
  abstract = {In this paper, we propose an improvement of an algorithm of Aurenhammer, Ho mann and Aronov to  nd a least square matching between a probability density and  nite set of sites with mass constraints, in the Euclidean plane. Our algorithm exploits the multiscale nature of this optimal transport problem. We iteratively simplify the target using Lloyd's algorithm, and use the solution of the simpli ed problem as a rough initial solution to the more complex one. This approach allows for fast estimation of distances between measures related to optimal transport (known as Earth-mover or Wasserstein distances). We also discuss the implementation of these algorithms, and compare the original one to its multiscale counterpart.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/merigotMultiscaleApproachOptimal22.pdf}
}

@unpublished{meritySingleHeadedAttention2019,
  title = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle = {Single {{Headed Attention RNN}}},
  author = {Merity, Stephen},
  date = {2019-11-26},
  eprint = {1911.11423},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.11423},
  urldate = {2019-11-27},
  abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto1 inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author’s lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone’s throw of a stone’s throw of state-of-the-art byte level language model results on enwik8. We also achieve state-of-the-art on WikiText-103 - or do we? This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author’s small studio apartment far too warm in the midst of a San Franciscan summer2. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts and requires minimal computation. Take that Sesame Street.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Zotero/storage/WB7TFSIA/meritySingleHeadedAttention2019.pdf}
}

@unpublished{metzGradientsAreNot2021,
  title = {Gradients Are {{Not All You Need}}},
  author = {Metz, Luke and Freeman, C. Daniel and Schoenholz, Samuel S. and Kachman, Tal},
  date = {2021-11-10},
  eprint = {2111.05803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2111.05803},
  urldate = {2021-12-13},
  abstract = {Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/9F9G5VZF/Metz et al. - 2021 - Gradients are Not All You Need.pdf}
}

@inproceedings{miconiDifferentiablePlasticityTraining2018,
  title = {Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation},
  shorttitle = {Differentiable Plasticity},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Miconi, Thomas and Stanley, Kenneth and Clune, Jeff},
  date = {2018-07-03},
  pages = {3559--3568},
  url = {http://proceedings.mlr.press/v80/miconi18a.html},
  urldate = {2020-01-28},
  abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains:...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/miconiDifferentiablePlasticityTraining22.pdf}
}

@inproceedings{miconiVirtualCreaturesModel2005,
  title = {A Virtual Creatures Model for Studies in Artificial Evolution},
  booktitle = {2005 {{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {Miconi, T. and Channon, A.},
  date = {2005-09},
  volume = {1},
  pages = {565-572 Vol.1},
  doi = {10.1109/CEC.2005.1554733},
  abstract = {We present the results of our replication of Karl Sims' work on the evolution of artificial creatures in a physically realistic 3D environment. We used standard McCulloch-Pitts neurons instead of a more complex set of ad hoc neurons, which we believe makes our model a more general tool for future experiments in artificial (co-)evolution. We provide a detailed description of our model and freely accessible source code. We describe our results both qualitatively and quantitatively, including an analysis of some evolved neural controllers. To the best of our knowledge, our work is the first replication of Sims' efforts to achieve results comparable to Sims' in efficiency and complexity, with standard neurons and realistic Newtonian physics},
  eventtitle = {2005 {{IEEE Congress}} on {{Evolutionary Computation}}},
  keywords = {artificial co-evolution,artificial creature evolution,Artificial intelligence,artificial life,biology computing,Computer science,evolution (biological),Evolutionary computation,Genetics,Intelligent robots,McCulloch-Pitts neurons,Morphology,neural controllers,neurocontrollers,Neurons,physically realistic 3D environment,Physics,Power system reliability,Testing,virtual creatures model,virtual reality},
  file = {/Users/hugo/Zotero/storage/KFB46VSM/miconiVirtualCreaturesModel2005.pdf;/Users/hugo/Zotero/storage/ESHZ5AH7/1554733.html}
}

@incollection{miikkulainenEvolvingDeepNeural2019,
  title = {Evolving Deep Neural Networks},
  booktitle = {Artificial {{Intelligence}} in the {{Age}} of {{Neural Networks}} and {{Brain Computing}}},
  author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel},
  date = {2019},
  pages = {293--312},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Zotero/storage/XQ6T8LK8/miikkulainenEvolvingDeepNeural2019.pdf;/Users/hugo/Zotero/storage/L7D8W6Q3/B9780128154809000153.html}
}

@article{mikamiOnedimensionalReactiondiffusionModel2020,
  title = {One-Dimensional Reaction-Diffusion Model for Intra- and Inter- Biofilm Oscillatory Dynamics},
  author = {Mikami, Taishi and Asally, Munehiro and Kano, Takeshi and Ishiguro, Akio},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {712--714},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00261},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00261},
  urldate = {2020-07-27},
  abstract = {Biofilm is a self-assembling microbial community that can serve as a model system for studying emergent collective dynamics of living systems. Bacillus subtilis biofilms resolve a conflict between interior and peripheral cells by electrical signaling and oscillatory colony-growth dynamics. Intriguingly, this dynamics maintain the interior-cell populations within a biofilm, which ultimately improves the survivability against antibacterial treatments as a whole. Beyond intra-biofilm coordination, two biofilms in a microfluidic device can coordinate their oscillatory phases according to the nutrient availability, through which biofilms improve their survivability by effectively utilizing limited resources. While models that can separately simulate intra- and inter- biofilm oscillatory dynamics have been proposed, recapturing these dynamics by a simple phenomenological model remains to be done. Extending from our previous work where we developed a simple reaction-diffusion model that captures the essence of the oscillatory colony growth dynamics of a single biofilm, here we show that a model similar to our previous one can recapitulate the inter- as well as intra-biofilm dynamics.}
}

@unpublished{mikolovAdvancesPreTrainingDistributed2017,
  title = {Advances in {{Pre-Training Distributed Word Representations}}},
  author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
  date = {2017-12-26},
  eprint = {1712.09405},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.09405},
  urldate = {2019-01-17},
  abstract = {Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/mikolovAdvancesPreTrainingDistributed22.pdf}
}

@inproceedings{mikolovContextDependentRecurrent2012,
  title = {Context Dependent Recurrent Neural Network Language Model},
  booktitle = {2012 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Mikolov, Tomas and Zweig, Geoffrey},
  date = {2012-12},
  pages = {234--239},
  publisher = {{IEEE}},
  location = {{Miami, FL, USA}},
  doi = {10.1109/SLT.2012.6424228},
  url = {http://ieeexplore.ieee.org/document/6424228/},
  urldate = {2019-01-17},
  abstract = {Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.},
  eventtitle = {2012 {{IEEE Spoken Language Technology Workshop}} ({{SLT}} 2012)},
  isbn = {978-1-4673-5126-3 978-1-4673-5125-6 978-1-4673-5124-9},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mikolovContextDependentRecurrent22.pdf}
}

@inproceedings{mikolovDistributedRepresentationsWords2013a,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-12-05},
  series = {{{NIPS}}'13},
  pages = {3111--3119},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}
}

@unpublished{mikolovLearningLongerMemory2015,
  title = {Learning {{Longer Memory}} in {{Recurrent Neural Networks}}},
  author = {Mikolov, Tomas and Joulin, Armand and Chopra, Sumit and Mathieu, Michael and Ranzato, Marc'Aurelio},
  date = {2015-04-16},
  eprint = {1412.7753},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1412.7753},
  urldate = {2020-09-29},
  abstract = {Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter \& Schmidhuber, 1997).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/mikolovLearningLongerMemory2015.pdf;/Users/hugo/Zotero/storage/6A97TIJN/1412.html}
}

@inproceedings{mikolovRecurrentNeuralNetwork2011,
  title = {Recurrent {{Neural Network Based Language Model}}},
  author = {Mikolov, Tomas and Karafiat, Martin and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
  date = {2011},
  pages = {4},
  abstract = {A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50\% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18\% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5\% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity.},
  eventtitle = {Interspeech 2011},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mikolovRecurrentNeuralNetwork2011.pdf}
}

@inproceedings{mikolovRoadmapMachineIntelligence2018,
  title = {A {{Roadmap Towards Machine Intelligence}}},
  booktitle = {Computational {{Linguistics}} and {{Intelligent Text Processing}}},
  author = {Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
  editor = {Gelbukh, Alexander},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {29--61},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-75477-2_2},
  abstract = {The development of intelligent machines is one of the biggest unsolved challenges in computer science. In this paper, we propose some fundamental properties these machines should have, focusing in particular on communication and learning. We discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human users. We also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment.},
  isbn = {978-3-319-75477-2},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mikolovRoadmapMachineIntelligence2018.pdf}
}

@unpublished{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  date = {2020-08-03},
  eprint = {2003.08934},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2021-08-18},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/hugo/Papers/pdf/mildenhallNeRFRepresentingScenes2020.pdf;/Users/hugo/Zotero/storage/W7I6VF47/2003.html}
}

@inproceedings{millerDesigningNeuralNetworks1989,
  title = {Designing {{Neural Networks}} Using {{Genetic Algorithms}}.},
  booktitle = {{{ICGA}}},
  author = {Miller, Geoffrey F. and Todd, Peter M. and Hegde, Shailesh U.},
  date = {1989},
  volume = {89},
  pages = {379--384},
  file = {/Users/hugo/Papers/pdf/millerDesigningNeuralNetworks12.pdf}
}

@inproceedings{millerEvolvingSelfrepairingSelfregulating2004,
  title = {Evolving a Self-Repairing, Self-Regulating, {{French}} Flag Organism},
  booktitle = {Genetic and {{Evolutionary Computation Conference}}},
  author = {Miller, Julian Francis},
  date = {2004},
  pages = {129--139},
  publisher = {{Springer}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.1049&rep=rep1&type=pdf},
  file = {/Users/hugo/Papers/pdf/millerEvolvingSelfrepairingSelfregulating22.pdf;/Users/hugo/Zotero/storage/6J9MN338/978-3-540-24854-5_12.html}
}

@unpublished{millidgePredictiveCodingApproximates2020,
  title = {Predictive {{Coding Approximates Backprop}} along {{Arbitrary Computation Graphs}}},
  author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  date = {2020-10-05},
  eprint = {2006.04182},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.04182},
  urldate = {2021-06-14},
  abstract = {Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayerperceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Zotero/storage/YVSFMHP3/Millidge et al. - 2020 - Predictive Coding Approximates Backprop along Arbi.pdf}
}

@unpublished{minaeeDeepLearningBased2020,
  title = {Deep {{Learning Based Text Classification}}: {{A Comprehensive Review}}},
  shorttitle = {Deep {{Learning Based Text Classification}}},
  author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  date = {2020-04-05},
  eprint = {2004.03705},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2004.03705},
  urldate = {2020-06-24},
  abstract = {Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/minaeeDeepLearningBased2020.pdf}
}

@article{minDistantSupervisionRelation,
  title = {Distant {{Supervision}} for {{Relation Extraction}} with an {{Incomplete Knowledge Base}}},
  author = {Min, Bonan and Grishman, Ralph and Wan, Li and Wang, Chang and Gondek, David},
  pages = {6},
  abstract = {Distant supervision, heuristically labeling a corpus using a knowledge base, has emerged as a popular choice for training relation extractors. In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/minDistantSupervisionRelation2.pdf}
}

@inproceedings{mingliangRecurrentConvolutionalNeural2015,
  title = {Recurrent Convolutional Neural Network for Object Recognition},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Ming Liang} and {Xiaolin Hu}},
  date = {2015-06},
  pages = {3367--3375},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298958},
  url = {http://ieeexplore.ieee.org/document/7298958/},
  urldate = {2020-01-10},
  abstract = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  langid = {english}
}

@book{minskyPerceptronsIntroductionComputational1972,
  title = {Perceptrons: {{An}} Introduction to Computational Geometry},
  shorttitle = {Perceptrons},
  author = {Minsky, Marvin and Papert, Seymour A.},
  date = {1972},
  edition = {2nd edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge MA}},
  doi = {10.7551/mitpress/11301.001.0001},
  url = {https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational},
  urldate = {2022-02-23},
  abstract = {The first systematic study of parallelism in computation by two pioneers in the field},
  isbn = {0-262-63022-2},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/L6AGHT8Q/PerceptronsAn-Introduction-to-Computational.html}
}

@article{mintzDistantSupervisionRelation2009,
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  date = {2009},
  journaltitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - ACL-IJCNLP '09},
  volume = {2},
  number = {2005},
  eprint = {3303671},
  eprinttype = {pmid},
  pages = {1003},
  issn = {1932432469},
  doi = {10.3115/1690219.1690287},
  url = {http://portal.acm.org/citation.cfm?doid=1690219.1690287},
  abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE- style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of enti- ties that appears in some Freebase relation, we find all sentences containing those entities in a large un- labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 re- lations at a precision of 67.6\%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.}
}

@article{mishraDonCountMe2020,
  title = {Don't Count Me out: {{On}} the Relevance of {{IP}} Addresses in the Tracking Ecosystem},
  author = {Mishra, Vikas and Laperdrix, Pierre and Vastel, Antoine and Rudametkin, Walter and Rouvoy, Romain and Lopatka, Martin},
  date = {2020},
  pages = {9},
  abstract = {Targeted online advertising has become an inextricable part of the way Web content and applications are monetized. At the beginning, online advertising consisted of simple ad-banners broadly shown to website visitors. Over time, it evolved into a complex ecosystem that tracks and collects a wealth of data to learn user habits and show targeted and personalized ads. To protect users against tracking, several countermeasures have been proposed, ranging from browser extensions that leverage filter lists, to features natively integrated into popular browsers like Firefox and Brave to combat more modern techniques like browser fingerprinting. Nevertheless, few browsers offer protections against IP address-based tracking techniques. Notably, the most popular browsers, Chrome, Firefox, Safari and Edge do not offer any.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mishraDonCountMe2020.pdf}
}

@article{mitchellComplexSystemsNetwork2006,
  title = {Complex Systems: {{Network}} Thinking},
  shorttitle = {Complex Systems},
  author = {Mitchell, Melanie},
  date = {2006-12},
  journaltitle = {Artificial Intelligence},
  volume = {170},
  number = {18},
  pages = {1194--1212},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.10.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000437020600083X},
  urldate = {2019-05-06},
  abstract = {In this article, I discuss some recent ideas in complex systems on the topic of networks, contained in or inspired by three recent complex systems books. The general science of networks is the subject of Albert-Lazlo Barabási’s Linked [A.-L. Barabási, Linked: The New Science of Networks, Perseus, New York, 2002] and Duncan Watts’ Six Degrees [D. Watts, Six Degrees: The Science of a Connected Age, Gardner’s Books, New York, 2003]. Commonalities among complex biological networks, e.g., immune systems, social insects, and cellular metabolism, and their relation to intelligence in computational systems are explored in the proceedings of a interdisciplinary conference on “Distributed Autonomous Systems” [L.A. Segel, I.R. Cohen (Eds.), Design Principles for the Immune System and Other Distributed Autonomous Systems, Oxford University Press, New York, 2001].},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mitchellComplexSystemsNetwork22.pdf}
}

@inbook{mitchellComputationCellularAutomata2005,
  title = {Computation in {{Cellular Automata}}: {{A Selected Review}}},
  shorttitle = {Computation in {{Cellular Automata}}},
  booktitle = {Non-{{Standard Computation}}},
  author = {Mitchell, Melanie},
  date = {2005-01-24},
  pages = {95--140},
  publisher = {{Wiley-VCH Verlag GmbH \& Co. KGaA}},
  location = {{Weinheim, FRG}},
  doi = {10.1002/3527602968.ch4},
  url = {http://doi.wiley.com/10.1002/3527602968.ch4},
  urldate = {2020-06-04},
  bookauthor = {Gramß, Tino and Bornholdt, Stefan and Groß, Michael and Mitchell, Melanie and Pellizzari, Thomas},
  isbn = {978-3-527-60296-4},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mitchellComputationCellularAutomata2005.pdf}
}

@article{mitchellEvolvingCellularAutomata1994,
  title = {Evolving Cellular Automata to Perform Computations: Mechanisms and Impediments},
  shorttitle = {Evolving Cellular Automata to Perform Computations},
  author = {Mitchell, Melanie and Crutchfield, James P. and Hraber, Peter T.},
  date = {1994-08-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {75},
  number = {1},
  pages = {361--391},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(94)90293-3},
  url = {https://www.sciencedirect.com/science/article/pii/0167278994902933},
  urldate = {2022-11-16},
  abstract = {We present results from experiments in which a genetic algorithm (GA) was used to evolve cellular automata (CAs) to perform a particular computational task - one-dimensional density classification. We look in detail at the evolutionary mechanisms producing the GA's behavior on this task and the impediments faced by the GA. In particular, we identify four “epochs of innovation” in which new CA strategies for solving the problem are discovered by the GA, describe how these strategies are implemented in CA rule tables, and identify the GA mechanisms underlying their discovery. The epochs are characterized by a breaking of the task's symmetries on the part of the GA. The symmetry breaking results in a short-term fitness gain but ultimately prevents the discovery of the most highly fit strategies. We discuss the extent to which symmetry breaking and other impediments are general phenomena in any GA search.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/XRYEAXPX/0167278994902933.html}
}

@inproceedings{mitchellEvolvingCellularAutomata1996,
  title = {Evolving {{Cellular Automata}} with {{Genetic Algorithms}}: {{A Review}} of {{Recent Work}}},
  booktitle = {Proceedings of the {{First International Conference}} on {{Evolutionary Computation}} and {{Its Applications}}},
  author = {Mitchell, Melanie and Road, Hyde Park and Das, Rajarshi and Box, P O},
  date = {1996},
  pages = {14},
  url = {http://csc.ucdavis.edu/~evca/Papers/evca-review.pdf},
  abstract = {We review recent work done by our group on applying genetic algorithms (GAs) to the design of cellular automata (CAs) that can perform computations requiring global coordination. A GA was used to evolve CAs for two computational tasks: density classi cation and synchronization. In both cases, the GA discovered rules that gave rise to sophisticated emergent computational strategies. These strategies can be analyzed using a \textbackslash computational mechanics" framework in which \textbackslash particles" carry information and interactions between particles e ects information processing. This framework can also be used to explain the process by which the strategies were designed by the GA. The work described here is a rst step in employing GAs to engineer useful emergent computation in decentralized multi-processor systems. It is also a rst step in understanding how an evolutionary process can produce complex systems with sophisticated collective computational abilities.},
  eventtitle = {First {{International Conference}} on {{Evolutionary Computation}} and {{Its Applications}} ({{EvCA}} '96)},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mitchellEvolvingCellularAutomata3.pdf}
}

@incollection{mitchellEvolvingCellularAutomata1997,
  title = {Evolving Cellular Automata to Perform Computations},
  booktitle = {Handbook of {{Evolutionary Computation}}},
  author = {Mitchell, Melanie and Crutchfield, James P and Das, Rajarshi},
  editor = {B�ck, Thomas and Fogel, David B and Michalewicz, Zbigniew},
  date = {1997},
  publisher = {{IOP Publishing Ltd}},
  doi = {10.1887/0750308958/b386c92},
  url = {http://stacks.iop.org/0750308958/b386c92},
  urldate = {2019-05-06},
  abstract = {We present results from an experiment similar to one performed by Packard 23], in which a genetic algorithm is used to evolve cellular automata (CA) to perform a particular computational task. Packard examined the frequency of evolved CA rules as a function of Langton's parameter 16], and interpreted the results of his experiment as giving evidence for the following two hypotheses: (1) CA rules able to perform complex computations are most likely to be found near \textbackslash critical" values, which have been claimed to correlate with a phase transition between ordered and chaotic behavioral regimes for CA; (2) When CA rules are evolved to perform a complex computation, evolution will tend to select rules with values close to the critical values. Our experiment produced very di erent results, and we suggest that the interpretation of the original results is not correct. We also review and discuss issues related to , dynamical-behavior classes, and computation in CA.},
  isbn = {978-0-7503-0895-3},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/mitchellEvolvingCellularAutomata12.pdf}
}

@unpublished{mitchellRevisitingEdgeChaos1993,
  title = {Revisiting the {{Edge}} of {{Chaos}}: {{Evolving Cellular Automata}} to {{Perform Computations}}},
  shorttitle = {Revisiting the {{Edge}} of {{Chaos}}},
  author = {Mitchell, Melanie and Hraber, Peter and Crutchfield, James P.},
  date = {1993-03-31},
  eprint = {adap-org/9303003},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/adap-org/9303003},
  urldate = {2019-05-06},
  abstract = {We present results from an experiment similar to one performed by Packard (1988), in which a genetic algorithm is used to evolve cellular automata (CA) to perform a particular computational task. Packard examined the frequency of evolved CA rules as a function of Langton's lambda parameter (Langton, 1990), and interpreted the results of his experiment as giving evidence for the following two hypotheses: (1) CA rules able to perform complex computations are most likely to be found near ``critical'' lambda values, which have been claimed to correlate with a phase transition between ordered and chaotic behavioral regimes for CA; (2) When CA rules are evolved to perform a complex computation, evolution will tend to select rules with lambda values close to the critical values. Our experiment produced very different results, and we suggest that the interpretation of the original results is not correct. We also review and discuss issues related to lambda, dynamical-behavior classes, and computation in CA. The main constructive results of our study are identifying the emergence and competition of computational strategies and analyzing the central role of symmetries in an evolutionary system. In particular, we demonstrate how symmetry breaking can impede the evolution toward higher computational capability.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {/Users/hugo/Papers/pdf/mitchellRevisitingEdgeChaos12.pdf;/Users/hugo/Zotero/storage/CYN6WB4C/9303003.html}
}

@unpublished{miwaEndtoEndRelationExtraction2016,
  title = {End-to-{{End Relation Extraction}} Using {{LSTMs}} on {{Sequences}} and {{Tree Structures}}},
  author = {Miwa, Makoto and Bansal, Mohit},
  date = {2016-01-05},
  eprint = {1601.00770},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1601.00770},
  urldate = {2018-04-24},
  abstract = {We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1\% and 5.7\% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning},
  file = {/Users/hugo/Papers/pdf/miwaEndtoEndRelationExtraction22.pdf;/Users/hugo/Zotero/storage/E93V3Q3C/1601.html}
}

@article{miyagawaHowLevyFlights2020,
  title = {How {{Lévy Flights Triggered}} by {{Presence}} of {{Defectors Affect Evolution}} of {{Cooperation}} in {{Spatial Games}}},
  author = {Miyagawa, Daiki and Ichinose, Genki and Chiba, Erika and Sayama, Hiroki},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {715--718},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00272},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00272},
  urldate = {2020-07-27},
  abstract = {Cooperation among individuals has been key to sustaining societies. However, natural selection favors defection over cooperation. Cooperation can be favored when the mobility of individuals allows cooperators to form a cluster (or group). Mobility patterns of animals sometimes follow a Lévy flight. A Lévy flight is a kind of random walk but it is composed of many small movements with a few big movements. Here, we developed an agent-based model in a square lattice where agents perform Lévy flights depending on the fraction of neighboring defectors. We focus on how the sensitivity to defectors when performing Lévy flights promotes the evolution of cooperation. Results of evolutionary simulations showed that cooperation was most promoted when the sensitivity to defectors was moderate. As the population density became larger, higher sensitivity was more beneficial for cooperation to evolve.}
}

@incollection{montavonWassersteinTrainingRestricted2016,
  title = {Wasserstein {{Training}} of {{Restricted Boltzmann Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Montavon, Grégoire and Müller, Klaus-Robert and Cuturi, Marco},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {3718--3726},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6248-wasserstein-training-of-restricted-boltzmann-machines.pdf},
  urldate = {2018-12-04},
  file = {/Users/hugo/Papers/pdf/montavonWassersteinTrainingRestricted22.pdf;/Users/hugo/Zotero/storage/U9MVEGFG/6248-wasserstein-training-of-restricted-boltzmann-machines.html}
}

@article{mooreWhenSpecialistsTransition2020,
  title = {When {{Specialists Transition}} to {{Generalists}}: {{Evolutionary Pressure}} in {{Lexicase Selection}}},
  shorttitle = {When {{Specialists Transition}} to {{Generalists}}},
  author = {Moore, Jared M. and Stanton, Adam},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {719--726},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00254},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00254},
  urldate = {2020-07-27},
  abstract = {Generalized behavior is a long standing goal for evolutionary robotics. Behaviors for a given task should be robust to perturbation and capable of operating across a variety of environments. We have previously shown that Lexicase selection evolves high-performing individuals in a semi-generalized wall crossing task–i.e., where the task is broadly the same, but there is variation between individual instances. Further work has identified effective parameter values for Lexicase selection in this domain but other factors affecting and explaining performance remain to be identified. In this paper, we expand our prior investigations, examining populations over evolutionary time exploring other factors that might lead to generalized behavior. Results show that genomic clusters do not correspond to performance, indicating that clusters of specialists do not form within the population. While early individuals gain a foothold in the selection process by specializing on a few wall heights, successful populations are ultimately pressured towards generalized behavior. Finally, we find that this transition from specialists to generalists also leads to an increase in tiebreaks, a mechanism in Lexicase, during selection providing a metric to assess the performance of individual replicates.}
}

@inproceedings{moranEffectsCooperativeCompetitive2017,
  title = {Effects of Cooperative and Competitive Coevolution on Complexity in a Linguistic Prediction Game},
  booktitle = {Proceedings of the 14th {{European Conference}} on {{Artificial Life ECAL}} 2017},
  author = {Moran, Nick and Pollack, Jordan},
  date = {2017-09},
  pages = {298--205},
  publisher = {{MIT Press}},
  location = {{Lyon, France}},
  doi = {10.7551/ecal_a_051},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_051},
  urldate = {2019-06-21},
  eventtitle = {Proceedings of the 14th {{European Conference}} on {{Artificial Life ECAL}} 2017},
  isbn = {978-0-262-34633-7},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/EJGFMSIC/moranEffectsCooperativeCompetitive2017.pdf}
}

@unpublished{moranReservoirComputingHardware2018,
  title = {Reservoir {{Computing Hardware}} with {{Cellular Automata}}},
  author = {Morán, Alejandro and Frasser, Christiam F. and Rosselló, Josep L.},
  date = {2018-06-21},
  eprint = {1806.04932},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1806.04932},
  urldate = {2020-01-10},
  abstract = {Elementary cellular automata (ECA) is a widely studied one-dimensional processing methodology where the successive iteration of the automaton may lead to the recreation of a rich pattern dynamic. Recently, cellular automata have been proposed as a feasible way to implement Reservoir Computing (RC) systems in which the automata rule is fixed and the training is performed using a linear regression. In this work we perform an exhaustive study of the performance of the different ECA rules when applied to pattern recognition of time-independent input signals using a RC scheme. Once the different ECA rules have been tested, the most accurate one (rule 90) is selected to implement a digital circuit. Rule 90 is easily reproduced using a reduced set of XOR gates and shift-registers, thus representing a high-performance alternative for RC hardware implementation in terms of processing time, circuit area, power dissipation and system accuracy. The model (both in software and its hardware implementation) has been tested using a pattern recognition task of handwritten numbers (the MNIST database) for which we obtained competitive results in terms of accuracy, speed and power dissipation. The proposed model can be considered to be a low-cost method to implement fast pattern recognition digital circuits.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/moranReservoirComputingHardware22.pdf}
}

@article{mordvintsevGrowingNeuralCellular2020,
  title = {Growing {{Neural Cellular Automata}}},
  author = {Mordvintsev, Alexander and Randazzo, Ettore and Niklasson, Eyvind and Levin, Michael},
  date = {2020-02-11},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {5},
  number = {2},
  pages = {e23},
  issn = {2476-0757},
  doi = {10.23915/distill.00023},
  url = {https://distill.pub/2020/growing-ca},
  urldate = {2020-02-12},
  abstract = {Differentiable Self-Organisation: A Cellular Automata model of Morphogenesis.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/DQXWVUX2/growing-ca.html}
}

@incollection{motaSophisticationRandomnessDeficiency2013,
  title = {Sophistication as {{Randomness Deficiency}}},
  booktitle = {Descriptional {{Complexity}} of {{Formal Systems}}},
  author = {Mota, Francisco and Aaronson, Scott and Antunes, Luís and Souto, André},
  editor = {Jurgensen, Helmut and Reis, Rogério},
  date = {2013},
  volume = {8031},
  pages = {172--181},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39310-5_17},
  url = {http://link.springer.com/10.1007/978-3-642-39310-5_17},
  urldate = {2019-04-26},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-642-39309-9 978-3-642-39310-5},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/motaSophisticationRandomnessDeficiency22.pdf}
}

@article{mouretEncouragingBehavioralDiversity2012,
  title = {Encouraging {{Behavioral Diversity}} in {{Evolutionary Robotics}}: An {{Empirical Study}}},
  shorttitle = {Encouraging {{Behavioral Diversity}} in {{Evolutionary Robotics}}},
  author = {Mouret, Jean-Baptiste and Doncieux, Stéphane},
  date = {2012},
  journaltitle = {Evolutionary Computation},
  volume = {20},
  number = {1},
  pages = {91--133},
  publisher = {{Massachusetts Institute of Technology Press (MIT Press)}},
  doi = {10.1162/EVCO_a_00048},
  url = {https://hal.archives-ouvertes.fr/hal-00687609},
  urldate = {2022-11-15},
  abstract = {Evolutionary robotics (ER) aims at automatically designing robots or controllers of robots without having to describe their inner workings. To reach this goal, ER researchers primarily employ phenotypes that can lead to an infinite number of robot behaviors and fitness functions that only reward the achievement of the task--and not how to achieve it. These choices make ER particularly prone to premature convergence. To tackle this problem, several papers recently proposed to explicitly encourage the diversity of the robot behaviors, rather than the diversity of the genotypes as in classic evolutionary optimization. Such an approach avoids the need to compute distances between structures and the pitfalls of the noninjectivity of the phenotype/behavior relation; however, it also introduces new questions: how to compare behavior? should this comparison be task specific? and what is the best way to encourage diversity in this context? In this paper, we review the main published approaches to behavioral diversity and benchmark them in a common framework. We compare each approach on three different tasks and two different genotypes. The results show that fostering behavioral diversity substantially improves the evolutionary process in the investigated experiments, regardless of genotype or task. Among the benchmarked approaches, multi-objective methods were the most efficient and the generic, Hamming-based, behavioral distance was at least as efficient as task specific behavioral metrics.},
  keywords = {diversity,evolutionary robotics,multi-objective evolutionary algorithms,neural networks,selective pressure},
  file = {/Users/hugo/Papers/pdf/mouretEncouragingBehavioralDiversity2012.pdf}
}

@misc{mouretIlluminatingSearchSpaces2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  date = {2015-04-19},
  number = {arXiv:1504.04909},
  eprint = {1504.04909},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1504.04909},
  urldate = {2022-11-10},
  abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  file = {/Users/hugo/Papers/pdf/mouretIlluminatingSearchSpaces22.pdf}
}

@book{muller-schloerOrganicComputingParadigm2011,
  title = {Organic Computing: A Paradigm Shift for Complex Systems},
  shorttitle = {Organic Computing},
  editor = {Müller-Schloer, Christian},
  date = {2011},
  series = {Autonomic Systems},
  publisher = {{Birkhäuser}},
  location = {{Basel}},
  isbn = {978-3-0348-0129-4},
  langid = {english},
  pagetotal = {627},
  file = {/Users/hugo/Papers/pdf/muller-schloerOrganicComputingParadigm2011.pdf}
}

@unpublished{mullerChallengesHighdimensionalReinforcement2018,
  title = {Challenges in {{High-dimensional Reinforcement Learning}} with {{Evolution Strategies}}},
  author = {Müller, Nils and Glasmachers, Tobias},
  date = {2018-07-01},
  eprint = {1806.01224},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.01224},
  urldate = {2021-06-14},
  abstract = {Evolution Strategies (ESs) have recently become popular for training deep neural networks, in particular on reinforcement learning tasks, a special form of controller design. Compared to classic problems in continuous direct search, deep networks pose extremely high-dimensional optimization problems, with many thousands or even millions of variables. In addition, many control problems give rise to a stochastic fitness function. Considering the relevance of the application, we study the suitability of evolution strategies for highdimensional, stochastic problems. Our results give insights into which algorithmic mechanisms of modern ES are of value for the class of problems at hand, and they reveal principled limitations of the approach. They are in line with our theoretical understanding of ESs. We show that combining ESs that offer reduced internal algorithm cost with uncertainty handling techniques yields promising methods for this class of problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/mullerChallengesHighdimensionalReinforcement2018.pdf}
}

@article{munakataChaosComputingImplementation2002,
  title = {Chaos Computing: Implementation of Fundamental Logical Gates by Chaotic Elements},
  shorttitle = {Chaos Computing},
  author = {Munakata, T. and Sinha, S. and Ditto, W.L.},
  date = {2002-11},
  journaltitle = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume = {49},
  number = {11},
  pages = {1629--1633},
  issn = {1558-1268},
  doi = {10.1109/TCSI.2002.804551},
  abstract = {Basic principles of implementing the most fundamental computing functions by chaotic elements are described. They provide a theoretical foundation of computer architecture based on a totally new principle other than silicon chips. The fundamental functions are: the logical AND, OR, NOT, XOR, and NAND operations (gates) and bit-by-bit arithmetic operations. Each of the logical operations is realized by employing a single chaotic element. Computer memory can be constructed by combining logical gates. With these fundamental ingredients in hand, it is conceivable to build a simple, fast, yet cost effective, general-purpose computing device. Chaos computing may also lead to dynamic architecture, where the hardware design itself evolves during the course of computation.. The basic ideas are explained by employing a one-dimensional model, specifically the logistic map.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems I}}: {{Fundamental Theory}} and {{Applications}}},
  keywords = {Arithmetic,arithmetic operation,chaos,Chaos,chaos computing,chaotic element,Circuits,computer architecture,Computer architecture,computer memory,Costs,DNA,dynamic architecture,general purpose computers,general-purpose computing device,Hardware,hardware design,logic gates,logical gate,logical operation,logistic map,Logistics,one-dimensional model,RNA,Silicon},
  file = {/Users/hugo/Papers/pdf/munakataChaosComputingImplementation2002.pdf;/Users/hugo/Zotero/storage/6G9ICWH8/1046831.html}
}

@online{nagGenerativeAdversarialNetworks2017,
  title = {Generative {{Adversarial Networks}} ({{GANs}}) in 50 Lines of Code ({{PyTorch}})},
  author = {Nag, Dev},
  date = {2017-02-11T01:18:21},
  url = {https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f},
  urldate = {2018-11-27},
  abstract = {tl;dr: GANs are simpler to set up than you think},
  organization = {{Medium}},
  file = {/Users/hugo/Zotero/storage/VYCDI4UM/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f.html}
}

@inproceedings{nagpalProgrammablePatternFormationScaleIndependence2008,
  title = {Programmable {{Pattern-Formation}} and {{Scale-Independence}}},
  booktitle = {Unifying {{Themes}} in {{Complex Systems IV}}},
  author = {Nagpal, Radhika},
  editor = {Minai, Ali A. and Bar-Yam, Yaneer},
  date = {2008},
  pages = {275--282},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-73849-7_31},
  abstract = {This paper presents a programming language for pattern-formation on a surface of locally-interacting, identically-programmed agents, by combining local organization primitives from developmental biology with combination rules from geometry. The approach is significantly different from current approaches to the design of self-organizing systems: the desired global shape is specified using an abstract geometry-based language, and the agent program is directly compiled from the global specification. Using this approach any 2D Euclidean construction can be formed from local-interactions of the agents. The resulting process is extremely reliable in the face of random agent distributions and varying agent numbers. In addition the process is scale-independent, which implies that the pattern scales as the number of agents increases, with no modification of the agent program. The pattern also scales asymmetrically to produce related patterns, such as D’Arcy Thompson’s famous transformations.},
  isbn = {978-3-540-73849-7},
  langid = {english},
  keywords = {Agent Program,Cellular Automaton,Combination Rule,Global Shape,Local Rule}
}

@thesis{nagpalProgrammableSelfassemblyConstructing2001,
  type = {phdthesis},
  title = {Programmable Self-Assembly: Constructing Global Shape Using Biologically-Inspired Local Interactions and Origami Mathematics},
  shorttitle = {Programmable Self-Assembly},
  author = {Nagpal, Radhika},
  date = {2001},
  institution = {{Massachusetts Institute of Technology}},
  location = {{USA}},
  pagetotal = {1},
  annotation = {AAI0803332}
}

@inproceedings{nagpalProgrammableSelfassemblyUsing2002,
  title = {Programmable Self-Assembly Using Biologically-Inspired Multiagent Control},
  booktitle = {Proceedings of the First International Joint Conference on {{Autonomous}} Agents and Multiagent Systems: Part 1},
  author = {Nagpal, Radhika},
  date = {2002-07-15},
  series = {{{AAMAS}} '02},
  pages = {418--425},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/544741.544839},
  url = {https://doi.org/10.1145/544741.544839},
  urldate = {2022-11-08},
  abstract = {This paper presents a programming language that species a robust process for shape formation on a sheet of identically programed agents, by combining local organization primitives from epithelial cell orphogenesis and Drosophila cell differentiation with combination rules from geometry. This work represents a significantly different approach to the design of self-organizing systems: the desired global shape is specified using an abstract geometry-based language, and the agent program is directly compiled from the global specification. The resulting self-assembly process is extremely reliable in the face of random agent distributions, random agent death and varying agent numbers, without relying on global coordinates or centralized control.},
  isbn = {978-1-58113-480-3},
  keywords = {amorphous computing,collective behavior,morphogenesis,paper-folding,pattern formation,smart matter}
}

@unpublished{najarroMetaLearningHebbianPlasticity2020,
  title = {Meta-{{Learning}} through {{Hebbian Plasticity}} in {{Random Networks}}},
  author = {Najarro, Elias and Risi, Sebastian},
  date = {2020-09-26},
  eprint = {2007.02686},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.02686},
  urldate = {2020-10-01},
  abstract = {Lifelong learning and adaptability are two defining aspects of biological agents. Modern reinforcement learning (RL) approaches have shown significant progress in solving complex tasks, however once training is concluded, the found solutions are typically static and incapable of adapting to new information or perturbations. While it is still not completely understood how biological brains learn and adapt so efficiently from experience, it is believed that synaptic plasticity plays a prominent role in this process. Inspired by this biological mechanism, we propose a search method that, instead of optimizing the weight parameters of neural networks directly, only searches for synapse-specific Hebbian learning rules that allow the network to continuously self-organize its weights during the lifetime of the agent. We demonstrate our approach on several reinforcement learning tasks with different sensory modalities and more than 450K trainable plasticity parameters. We find that starting from completely random weights, the discovered Hebbian rules enable an agent to navigate a dynamical 2D-pixel environment; likewise they allow a simulated 3D quadrupedal robot to learn how to walk while adapting to morphological damage not seen during training and in the absence of any explicit reward or error signal in less than 100 timesteps. Code is available at https://github.com/enajx/HebbianMetaLearning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/najarroMetaLearningHebbianPlasticity2020.pdf}
}

@incollection{narbelQualitativeQuantitativeCellular2006,
  title = {Qualitative and {{Quantitative Cellular Automata}} from {{Differential Equations}}},
  booktitle = {Cellular {{Automata}}},
  author = {Narbel, Philippe},
  editor = {El Yacoubi, Samira and Chopard, Bastien and Bandini, Stefania},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {4173},
  pages = {112--121},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11861201_16},
  url = {http://link.springer.com/10.1007/11861201_16},
  urldate = {2021-12-13},
  abstract = {We give a synthetic and formalized account of relationships between cellular automata (CA) and differential equations (DE): Numerical schemes and phase portraits analysis (via cell-to-cell mappings) can be translated into CA, and compositions of differential operators and phase portraits induce CA compositions. Based on DE, CA can be tuned according to discretization parameters so that faithful CA sequences can be built describing qualitative as well as quantitative solutions.},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-540-40929-8 978-3-540-40932-8},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/HTD66SF9/Narbel - 2006 - Qualitative and Quantitative Cellular Automata fro.pdf}
}

@article{natschlagerLiquidComputerNovel2002,
  title = {The" Liquid Computer": {{A}} Novel Strategy for Real-Time Computing on Time Series},
  shorttitle = {The" Liquid Computer"},
  author = {Natschläger, Thomas and Maass, Wolfgang and Markram, Henry},
  date = {2002},
  journaltitle = {Special issue on Foundations of Information Processing of TELEMATIK},
  volume = {8},
  pages = {39--43},
  issue = {ARTICLE},
  file = {/Users/hugo/Zotero/storage/BVQ9BAUI/117806.html}
}

@article{nayakSurveyTwoDimensional,
  title = {A {{Survey}} on {{Two Dimensional Cellular Automata}} and {{Its Application}} in {{Image Processing}}},
  author = {Nayak, Deepak Ranjan and Patra, Prashanta Kumar and Mahapatra, Amitav},
  pages = {10},
  abstract = {Parallel algorithms for solving any image processing task is a highly demanded approach in the modern world. Cellular Automata (CA) are the most common and simple models of parallel computation. So, CA has been successfully used in the domain of image processing for the last couple of years. This paper provides a survey of available literatures of some methodologies employed by different researchers to utilize the cellular automata for solving some important problems of image processing. The survey includes some important image processing tasks such as rotation, zooming, translation, segmentation, edge detection, compression and noise reduction of images. Finally, the experimental results of some methodologies are presented.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/I4VYRPVH/nayakSurveyTwoDimensional.pdf}
}

@unpublished{nealModernTakeBiasVariance2019,
  title = {A {{Modern Take}} on the {{Bias-Variance Tradeoff}} in {{Neural Networks}}},
  author = {Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  date = {2019-12-18},
  eprint = {1810.08591},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.08591},
  urldate = {2020-02-24},
  abstract = {The bias-variance tradeoff tells us that as model complexity increases, bias falls and variances increases, leading to a U-shaped test error curve. However, recent empirical results with over-parameterized neural networks are marked by a striking absence of the classic U-shaped test error curve: test error keeps decreasing in wider networks. This suggests that there might not be a bias-variance tradeoff in neural networks with respect to network width, unlike was originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky evidence used to support this claim in neural networks, we measure bias and variance in the modern setting. We find that both bias and variance can decrease as the number of parameters grows. To better understand this, we introduce a new decomposition of the variance to disentangle the effects of optimization and data sampling. We also provide theoretical analysis in a simplified setting that is consistent with our empirical findings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/nealModernTakeBiasVariance2019.pdf}
}

@inproceedings{nehanivEvolutionAsynchronousCellular2003,
  title = {Evolution in Asynchronous Cellular Automata},
  booktitle = {Proceedings of the Eighth International Conference on {{Artificial}} Life},
  author = {Nehaniv, Chrystopher L.},
  date = {2003},
  pages = {65--73},
  file = {/Users/hugo/Zotero/storage/UAJAVYMS/nehanivEvolutionAsynchronousCellular2003.pdf;/Users/hugo/Zotero/storage/CJHCT4H8/books.html}
}

@inproceedings{nehanivSelfreproductionAsynchronousCellular2002,
  title = {Self-Reproduction in Asynchronous Cellular Automata},
  booktitle = {Proceedings 2002 {{NASA}}/{{DoD Conference}} on {{Evolvable Hardware}}},
  author = {Nehaniv, C.L.},
  date = {2002-07},
  pages = {201--209},
  doi = {10.1109/EH.2002.1029886},
  abstract = {Building on the work of Von Neumann, Burks, Codd, and Langton, among others, we introduce the first examples of asynchronous self-reproduction in cellular automata. Reliance on a global synchronous update signal has been a limitation of all solutions since the problem of achieving self-production in cellular automata was first attacked by Von Neumann half a century ago. Our results obviate the need for this restriction. We introduce a simple constructive mechanism to transform any cellular automata network with synchronous update into one with the same behavior but whose cells may be updated randomly and asynchronously. This is achieved by introduction of a synchronization substratum which locally keeps track of the passage of time in a local neighborhood in a manner that keeps all cells locally in-step. The generality of this mechanism is guaranteed by a general mathematical theorem (due to the author) that allows any synchronous cellular automata configuration and rule to be realized asynchronously in such a way the the behavior of the original synchronous cellular automata can be recovered from that of the corresponding asynchronous cellular automaton. Thus all important results on self-reproduction, universal computation, and universal construction, and evolution in populations of self-reproducing configurations in cellular automata that have been obtained in the past carry over to the asynchronous domain.},
  eventtitle = {Proceedings 2002 {{NASA}}/{{DoD Conference}} on {{Evolvable Hardware}}},
  keywords = {Adaptive systems,artificial life,asynchronous cellular automata,asynchronous self reproduction,Automata,Biological materials,cellular automata,DNA,Environmental economics,Evolution (biology),Genetics,global synchronous update signal,Hardware,local neighborhood,mathematical theorem,NASA,self-reproduction,World Wide Web},
  file = {/Users/hugo/Zotero/storage/DFZJ54BS/1029886.html}
}

@article{nemirovskiRobustStochasticApproximation2009,
  title = {Robust {{Stochastic Approximation Approach}} to {{Stochastic Programming}}},
  author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
  date = {2009-01},
  journaltitle = {SIAM Journal on Optimization},
  volume = {19},
  number = {4},
  pages = {1574--1609},
  issn = {1052-6234, 1095-7189},
  doi = {10.1137/070704277},
  url = {http://epubs.siam.org/doi/10.1137/070704277},
  urldate = {2018-12-20},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/nemirovskiRobustStochasticApproximation22.pdf}
}

@incollection{neumannProbabilisticLogicsSynthesis1956,
  title = {Probabilistic {{Logics}} and the {{Synthesis}} of {{Reliable Organisms From Unreliable Components}}},
  booktitle = {Automata {{Studies}}. ({{AM-34}})},
  author = {von Neumann, J.},
  editor = {Shannon, C. E. and McCarthy, J.},
  date = {1956-12-31},
  pages = {43--98},
  publisher = {{Princeton University Press}},
  doi = {10.1515/9781400882618-003},
  url = {https://www.degruyter.com/document/doi/10.1515/9781400882618-003/html},
  urldate = {2022-11-10},
  isbn = {978-1-4008-8261-8}
}

@inproceedings{ngSpectralClusteringAnalysis2001,
  title = {On Spectral Clustering: Analysis and an Algorithm},
  shorttitle = {On Spectral Clustering},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Neural Information Processing Systems}}: {{Natural}} and {{Synthetic}}},
  author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
  date = {2001-01-03},
  series = {{{NIPS}}'01},
  pages = {849--856},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {Despite many empirical successes of spectral clustering methods— algorithms that cluster points using eigenvectors of matrices derived from the data—there are several unresolved issues. First. there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.}
}

@inproceedings{nguyenGenerativeEventSchema2015,
  title = {Generative {{Event Schema Induction}} with {{Entity Disambiguation}}},
  author = {Nguyen, Kiem-Hieu and Tannier, Xavier and Ferret, Olivier and Besançon, Romaric},
  date = {2015},
  pages = {188--197},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/P15-1019},
  url = {http://aclweb.org/anthology/P15-1019},
  urldate = {2018-06-19},
  abstract = {This paper presents a generative model to event schema induction. Previous methods in the literature only use head words to represent entities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/nguyenGenerativeEventSchema22.pdf}
}

@article{nguyenVariationalContinualLearning2017,
  title = {Variational {{Continual Learning}}},
  author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
  date = {2017},
  journaltitle = {CoRR},
  volume = {abs/1710.10628},
  eprint = {1710.10628},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1710.10628},
  urldate = {2022-03-04},
  archiveprefix = {arXiv}
}

@article{nicheleDeepLearningCellular2017,
  title = {Deep Learning with Cellular Automaton-Based Reservoir Computing},
  author = {Nichele, Stefano and Molund, Andreas},
  date = {2017},
  journaltitle = {0891-2513},
  publisher = {{Complex Systems Publications Inc}},
  issn = {0891-2513},
  doi = {http://dx.doi.org/10.25088/ComplexSystems.26.4.319},
  url = {https://oda-hioa.archive.knowledgearc.net/handle/10642/6164},
  urldate = {2020-06-23},
  abstract = {Recurrent neural networks (RNNs) have been a prominent concept wiithin artificial intelligence. They are inspired by biological neural net works (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the   more generic artificial neural networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech   recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, echo state Networks and liquid state machines have been proposed as   possible RNN alternatives, under the name of reservoir computing (RC). Reservoir computers are far easier to train. In this paper, cellular automata (CAs) are used as a reservoir and are tested on the five-bit memory task (a well known benchmark   within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata   and a recurrent architecture for handling the sequential aspects. Furthermore, a layered (deep) reservoir architecture   is proposed. Performances are compared to earlier work, in addition to the single-layer version. Results show that the single   cellular automaton (CA) reservoir system yields similar results to state-of-the-art work. The system comprised of two layered   reservoirs does show a noticeable improvement compared to a single CA reservoir. This work lays the foundation for   implementations of deep learning with CA-based reservoir systems.},
  langid = {english},
  annotation = {Accepted: 2018-05-09T07:58:01Z},
  file = {/Users/hugo/Papers/pdf/nicheleDeepLearningCellular2017.pdf;/Users/hugo/Zotero/storage/UMTI75FY/6164.html}
}

@unpublished{nicheleDeepReservoirComputing2017,
  title = {Deep {{Reservoir Computing Using Cellular Automata}}},
  author = {Nichele, Stefano and Molund, Andreas},
  date = {2017-03-08},
  eprint = {1703.02806},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.02806},
  urldate = {2020-06-23},
  abstract = {Recurrent Neural Networks (RNNs) have been a prominent concept within artificial intelligence. They are inspired by Biological Neural Networks (BNNs) and provide an intuitive and abstract representation of how BNNs work. Derived from the more generic Artificial Neural Networks (ANNs), the recurrent ones are meant to be used for temporal tasks, such as speech recognition, because they are capable of memorizing historic input. However, such networks are very time consuming to train as a result of their inherent nature. Recently, Echo State Networks and Liquid State Machines have been proposed as possible RNN alternatives, under the name of Reservoir Computing (RC). RCs are far more easy to train. In this paper, Cellular Automata are used as reservoir, and are tested on the 5-bit memory task (a well known benchmark within the RC community). The work herein provides a method of mapping binary inputs from the task onto the automata, and a recurrent architecture for handling the sequential aspects of it. Furthermore, a layered (deep) reservoir architecture is proposed. Performances are compared towards earlier work, in addition to its single-layer version. Results show that the single CA reservoir system yields similar results to state-of-the-art work. The system comprised of two layered reservoirs do show a noticeable improvement compared to a single CA reservoir. This indicates potential for further research and provides valuable insight on how to design CA reservoir systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/nicheleDeepReservoirComputing2017.pdf;/Users/hugo/Zotero/storage/SQT8XITZ/1703.html}
}

@inproceedings{nicheleMeasuringPhenotypicStructural2014,
  title = {Measuring {{Phenotypic Structural Complexity}} of {{Artificial Cellular Organisms}}},
  booktitle = {Innovations in {{Bio-inspired Computing}} and {{Applications}}},
  author = {Nichele, Stefano and Tufte, Gunnar},
  editor = {Abraham, Ajith and Krömer, Pavel and Snášel, Václav},
  date = {2014},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {23--35},
  publisher = {{Springer International Publishing}},
  abstract = {Artificial multi-cellular organisms develop from a single zygote to different structures and shapes, some simple, some complex. Such phenotypic structural complexity is the result of morphogenesis, where cells grow and differentiate according to the information encoded in the genome. In this paper we investigate the structural complexity of artificial cellular organisms at phenotypic level, in order to understand if genome information could be used to predict the emergent structural complexity. Our measure of structural complexity is based on the theory of Kolmogorov complexity and approximations. We relate the Lambda parameter, with its ability to detect different behavioral regimes, to the calculated structural complexity. It is shown that the easily computable Lempel-Ziv complexity approximation has a good ability to discriminate emergent structural complexity, thus providing a measurement that can be related to a genome parameter for estimation of the developed organism’s phenotypic complexity. The experimental model used herein is based on 1D, 2D and 3D Cellular Automata.},
  isbn = {978-3-319-01781-5},
  langid = {english},
  keywords = {CAs,Developmental Systems,Emergence,Structural Complexity},
  file = {/Users/hugo/Papers/pdf/nicheleMeasuringPhenotypicStructural22.pdf}
}

@unpublished{nicheleReservoirComputingUsing2017,
  title = {Reservoir {{Computing Using Non-Uniform Binary Cellular Automata}}},
  author = {Nichele, Stefano and Gundersen, Magnus S.},
  date = {2017-02-13},
  eprint = {1702.03812},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1702.03812},
  urldate = {2020-06-23},
  abstract = {The Reservoir Computing (RC) paradigm utilizes a dynamical system, i.e., a reservoir, and a linear classifier, i.e., a read-out layer, to process data from sequential classification tasks. In this paper the usage of Cellular Automata (CA) as a reservoir is investigated. The use of CA in RC has been showing promising results. In this paper, selected state-of-the-art experiments are reproduced. It is shown that some CA-rules perform better than others, and the reservoir performance is improved by increasing the size of the CA reservoir itself. In addition, the usage of parallel loosely coupled CA-reservoirs, where each reservoir has a different CA-rule, is investigated. The experiments performed on quasi-uniform CA reservoir provide valuable insights in CA reservoir design. The results herein show that some rules do not work well together, while other combinations work remarkably well. This suggests that non-uniform CA could represent a powerful tool for novel CA reservoir implementations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Emerging Technologies},
  file = {/Users/hugo/Papers/pdf/nicheleReservoirComputingUsing2017.pdf;/Users/hugo/Zotero/storage/D96INVWY/1702.html}
}

@unpublished{nicholFirstOrderMetaLearningAlgorithms2018,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  date = {2018-10-22},
  eprint = {1803.02999},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1803.02999},
  urldate = {2021-08-25},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/nicholFirstOrderMetaLearningAlgorithms2018.pdf;/Users/hugo/Zotero/storage/H8VUTXE3/1803.html}
}

@misc{nicholGLIDEPhotorealisticImage2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  date = {2022-03-08},
  number = {arXiv:2112.10741},
  eprint = {2112.10741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10741},
  url = {http://arxiv.org/abs/2112.10741},
  urldate = {2022-07-26},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/nicholGLIDEPhotorealisticImage2022.pdf;/Users/hugo/Zotero/storage/SXQX9NWW/2112.html}
}

@book{nickelReviewRelationalMachine2016,
  title = {A Review of Relational Machine Learning for Knowledge Graphs},
  author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
  date = {2016-01},
  volume = {104},
  number = {1},
  eprint = {18267787},
  eprinttype = {pmid},
  doi = {10.1109/JPROC.2015.2483592},
  url = {http://arxiv.org/abs/1503.00759 http://dx.doi.org/10.1109/JPROC.2015.2483592 http://ieeexplore.ieee.org/document/7358050/ http://www.ncbi.nlm.nih.gov/pubmed/18267787},
  abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
  isbn = {1-08-980130-0},
  pagetotal = {11–33},
  keywords = {Graph-based models,knowledge extraction,knowledge graphs,latent feature models,statistical relational learning},
  file = {/Users/hugo/Papers/pdf/nickelReviewRelationalMachine22.pdf}
}

@article{nickelThreeWayModelCollective2011,
  title = {A {{Three-Way Model}} for {{Collective Learning}} on {{Multi-Relational Data}}},
  author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
  date = {2011},
  journaltitle = {Proceedings of the 28th International Conference on Machine Learning},
  pages = {8},
  abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/nickelThreeWayModelCollective22.pdf;/Users/hugo/Papers/pdf/nickelThreeWayModelCollective3.pdf}
}

@article{niklassonSelfOrganisingTextures2021,
  title = {Self-{{Organising Textures}}},
  author = {Niklasson, Eyvind and Mordvintsev, Alexander and Randazzo, Ettore and Levin, Michael},
  date = {2021-02-11},
  journaltitle = {Distill},
  volume = {6},
  number = {2},
  pages = {e00027--003},
  doi = {10.23915/distill.00027.003},
  url = {https://distill.pub/selforg/2021/textures/},
  file = {/Users/hugo/Zotero/storage/LFQ47A8F/textures.html}
}

@article{nishioFaultTolerantCellular1975,
  title = {Fault Tolerant Cellular Spaces},
  author = {Nishio, Hidenosuke and Kobuchi, Youichi},
  date = {1975-10-01},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  volume = {11},
  number = {2},
  pages = {150--170},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(75)80065-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000075800651},
  urldate = {2022-11-10},
  abstract = {This paper treats the problem of designing a fault tolerant cellular space which simulates an arbitrary given cellular space in real time. A cellular space is called fault tolerant if it behaves normally even when its component cells misoperate. First such notions as simulation, misoperation, and K-separated misoperation are defined. Then a new multidimensional coding of configurations is introduced and explained using as typical example the two-dimensional space. The first main result is Theorem 1, which states that the introduced coding method is useful for correcting errors occurring at most once in every K=5×5 rectangle. The general theory is given in Section 6, where the second main result is given in the form of Theorem 8. It gives a necessary and sufficient condition for testing whether or not a given coding is adequate for error correction.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/nishioFaultTolerantCellular1975.pdf;/Users/hugo/Zotero/storage/NYBEN75W/S0022000075800651.html}
}

@article{niuElementaryLargescaleKnowledgebase2012,
  title = {Elementary: {{Large-scale Knowledge-base Construction}} via {{Machine Learning}} and {{Statistical Inference}}},
  author = {Niu, Feng and Zhang, Ce and Ré, Christopher and Shavlik, Jude},
  date = {2012},
  journaltitle = {International Journal on Semantic Web and Information Systems (IJSWIS)},
  volume = {8},
  number = {3},
  pages = {42---73},
  issn = {15526283},
  doi = {10.4018/jswis.2012070103},
  abstract = {Researchers have approached knowledge-base construction (KBC) with a wide range of data resources and techniques. We present Elementary, a prototype KBC system that is able to combine diverse resources and different KBC techniques via machine learning and statistical inference to construct knowledge bases. Using Elementary, we have implemented a solution to the TAC-KBP challenge with quality comparable to the state of the art, as well as an end-to-end online demonstration that automatically and continuously enriches Wikipedia with structured data by reading millions of webpages on a daily basis. We describe several challenges and our solutions in designing, implementing, and deploying Elementary. In particular, we first describe the conceptual framework and architecture of Elementary, and then discuss how we address scalability challenges to enable Web-scale deployment. First, to take advantage of diverse data resources and proven techniques, Elementary employs Markov logic, a succinct yet expressive language to specify probabilistic graphical models. Elementary accepts both domain-knowledge rules and classical machine-learning models such as conditional random fields, thereby integrating different data resources and KBC techniques in a principled manner. Second, to support large-scale KBC with terabytes of data and millions of entities, Elementary leverages high-throughput parallel computing infrastructure such as Hadoop, Condor, and parallel databases. Furthermore, to scale sophisticated statistical inference, Elementary employs a novel decomposition-based approach to Markov logic inference that solves routine subtasks such as classification and coreference with specialized algorithms. We empirically show that this decomposition-based inference approach achieves higher performance than prior inference approaches. To validate the effectiveness of Elementary's approach to KBC, we experimentally show that its ability to incorporate diverse signals has positive impacts on KBC quality.},
  keywords = {information extraction,Knowledge-base construction,machine learning,natural language understanding,statistical inference,systems},
  file = {/Users/hugo/Papers/pdf/niuElementaryLargescaleKnowledgebase22.pdf}
}

@article{niuTuffyScalingStatistical2011,
  title = {Tuffy: {{Scaling}} up {{Statistical Inference}} in {{Markov Logic Networks}} Using an {{RDBMS}}},
  author = {Niu, Feng and Ré, Christopher and Doan, AnHai and Shavlik, Jude},
  date = {2011-04},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {4},
  number = {6},
  pages = {373--384},
  issn = {2150-8097},
  doi = {10.14778/1978665.1978669},
  url = {http://dl.acm.org/citation.cfm?id=1978665.1978669 http://arxiv.org/abs/1104.3216},
  abstract = {Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.}
}

@article{nobiliJohnNeumannAutomata1994,
  title = {John von {{Neumann}}’s {{Automata Revisited}}.},
  author = {Nobili, Renato and Pesavento, Umberto},
  date = {1994-11-01},
  pages = {27},
  abstract = {This paper describes the first world-wide time implementation of the celebrated automata of John von Neumann. Logical aspects concerning the structure and the behavior of simple and complex cellular automata are discussed. The complete description of a universal constructor, i.e., an automaton which, once supplied with a suitable code formed by a cell pencil, is capable of generating any sort of automaton, is also presented for the first time. This generative capability is based on the local injection of a cell-excitation stream into the planar cell lattice. The realization of a universal constructor substantially simpler than those proposed by other authors was possible thanks to the use of a sophisticated graphic and logical environment for the composition of cell assemblies and the observation of their evolution. This research is focused on how cellular automata can be created and used to implement parallel information processing by planar cell lattices. An appealing feature of our approach is the perspective of producing silicon cell automata capable of performing any sort of parallel computation by the temporary generation of suitable cell state configurations. This appears a necessary condition for the effective implementation of general purpose parallel computers as, in general, good performance in parallel processing depends rather critically on architectural optimality relative the task to be performed.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/NWN7ICVD/nobiliJohnNeumannAutomata1994.pdf}
}

@unpublished{noklandDirectFeedbackAlignment2016,
  title = {Direct {{Feedback Alignment Provides Learning}} in {{Deep Neural Networks}}},
  author = {Nøkland, Arild},
  date = {2016-12-21},
  eprint = {1609.01596},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1609.01596},
  urldate = {2020-06-28},
  abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45\% error on the permutation invariant MNIST task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/noklandDirectFeedbackAlignment2016.pdf;/Users/hugo/Zotero/storage/QAKSU2NZ/1609.html}
}

@online{normandTransparenceCinqChoses2018,
  title = {Transparence : cinq choses à savoir sur les lobbies en France},
  shorttitle = {Transparence},
  author = {Normand, Grégoire},
  date = {2018-01-12},
  url = {https://www.latribune.fr/economie/france/transparence-cinq-choses-a-savoir-sur-les-lobbies-en-france-764327.html},
  urldate = {2018-06-18},
  abstract = {Depuis le premier janvier dernier, la justice peut prononcer des sanctions à l'encontre des groupes de pression qui ne s'inscrivent pas au registre de l'administration.},
  langid = {french},
  organization = {{La Tribune}},
  file = {/Users/hugo/Zotero/storage/8PKMSR8G/transparence-cinq-choses-a-savoir-sur-les-lobbies-en-france-764327.html}
}

@article{ofriaAvidaSoftwarePlatform2004,
  title = {Avida: A Software Platform for Research in Computational Evolutionary Biology},
  shorttitle = {Avida},
  author = {Ofria, Charles and Wilke, Claus O.},
  date = {2004},
  journaltitle = {Artificial Life},
  shortjournal = {Artif. Life},
  volume = {10},
  number = {2},
  eprint = {15107231},
  eprinttype = {pmid},
  pages = {191--229},
  issn = {1064-5462},
  doi = {10.1162/106454604773563612},
  abstract = {Avida is a software platform for experiments with self-replicating and evolving computer programs. It provides detailed control over experimental settings and protocols, a large array of measurement tools, and sophisticated methods to analyze and post-process experimental data. We explain the general principles on which Avida is built, as well as its main components and their interactions. We also explain how experiments are set up, carried out, and analyzed.},
  langid = {english},
  keywords = {Biological Evolution,Computational Biology,Research Design,Software},
  file = {/Users/hugo/Zotero/storage/ZVMPPRL8/ofriaAvidaSoftwarePlatform2004.pdf}
}

@unpublished{oliverRealisticEvaluationDeep2018,
  title = {Realistic {{Evaluation}} of {{Deep Semi-Supervised Learning Algorithms}}},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  date = {2018-04-24},
  eprint = {1804.09170},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.09170},
  urldate = {2018-05-02},
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/oliverRealisticEvaluationDeep22.pdf;/Users/hugo/Zotero/storage/AUYSFQF2/1804.html}
}

@article{omohundroModellingCellularAutomata1984,
  title = {Modelling Cellular Automata with Partial Differential Equations},
  author = {Omohundro, Stephen},
  date = {1984-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {10},
  number = {1-2},
  pages = {128--134},
  issn = {01672789},
  doi = {10.1016/0167-2789(84)90255-0},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0167278984902550},
  urldate = {2021-12-13},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/KWGR3ETY/Omohundro - 1984 - Modelling cellular automata with partial different.pdf}
}

@unpublished{ororbiaiiLearningSimplerLanguage2017,
  title = {Learning {{Simpler Language Models}} with the {{Differential State Framework}}},
  author = {Ororbia II, Alexander G. and Mikolov, Tomas and Reitter, David},
  date = {2017-07-16},
  eprint = {1703.08864},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.08864},
  urldate = {2020-09-29},
  abstract = {Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks such as language modeling. Existing architectures that address the issue are often complex and costly to train. The Differential State Framework (DSF) is a simple and high-performing design that unifies previously introduced gated neural models. DSF models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical, simple recurrent network. Within the DSF framework, a new architecture is presented, the Delta-RNN. In language modeling at the word and character levels, the Delta-RNN outperforms popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU), and, when regularized, performs comparably to several state-of-the-art baselines. At the subword level, the Delta-RNN's performance is comparable to that of complex gated architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/ororbiaiiLearningSimplerLanguage2017.pdf;/Users/hugo/Zotero/storage/ER993V9H/1703.html}
}

@article{ostrovskyCellularAutomataPolymer2001,
  title = {Cellular Automata for Polymer Simulation with Application to Polymer Melts and Polymer Collapse Including Implications for Protein Folding},
  author = {Ostrovsky, B. and Crooks, G. and Smith, M. A. and Bar-Yam, Y.},
  date = {2001-04-01},
  journaltitle = {Parallel Computing},
  shortjournal = {Parallel Computing},
  series = {Cellular Automata: {{From}} Modeling to Applications},
  volume = {27},
  number = {5},
  pages = {613--641},
  issn = {0167-8191},
  doi = {10.1016/S0167-8191(00)00081-8},
  url = {https://www.sciencedirect.com/science/article/pii/S0167819100000818},
  urldate = {2022-11-08},
  abstract = {Cellular automata can be designed that allow the simulation of a large variety of polymer problems including isolated polymers in dilute solution, polymers in high density melts and polymers embedded in media. The two-space algorithm is a particularly efficient algorithm for polymer simulation that is easy to implement and generalize on both conventional serial hardward and Cellular Automaton (CA) Machines. We describe the implementation of this algorithm and two applications: two dimensions (2-D) melts and polymer collapse. Simulations of high density melts in 2-D show that contrary to expectations polymers do not segregate at high density, there is significant interpenetration as there is in 3-D. Polymer collapse is studied in the regime far from equilibrium. Collapse is found to be dominated by migration of the chain ends. The kinetic process of collapse can systematically and reproducibly restrict the possible conformations that are explored during protein folding. This suggests that the kinetics of collapse may help lead to the desired folded conformation of proteins.},
  langid = {english},
  keywords = {Cellular automata,Domain decomposition,Polymer collapse,Polymer melts,Protein folding},
  file = {/Users/hugo/Zotero/storage/R6ZK8UYH/S0167819100000818.html}
}

@article{osullivanGraphCellularAutomataGeneralised2001,
  title = {Graph-{{Cellular Automata}}: {{A Generalised Discrete Urban}} and {{Regional Model}}},
  shorttitle = {Graph-{{Cellular Automata}}},
  author = {O'Sullivan, David},
  date = {2001-10},
  journaltitle = {Environment and Planning B: Planning and Design},
  shortjournal = {Environ Plann B Plann Des},
  volume = {28},
  number = {5},
  pages = {687--705},
  issn = {0265-8135, 1472-3417},
  doi = {10.1068/b2707},
  url = {http://journals.sagepub.com/doi/10.1068/b2707},
  urldate = {2021-06-03},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/osullivanGraphCellularAutomataGeneralised2001.pdf}
}

@article{ottaviSimulationIsingModel1989,
  title = {Simulation of the {{Ising Model}} by {{Cellular Automata}}},
  author = {Ottavi, H. and Parodi, O.},
  date = {1989-04},
  journaltitle = {Europhysics Letters (EPL)},
  shortjournal = {EPL},
  volume = {8},
  number = {8},
  pages = {741--746},
  publisher = {{IOP Publishing}},
  issn = {0295-5075},
  doi = {10.1209/0295-5075/8/8/006},
  url = {https://doi.org/10.1209/0295-5075/8/8/006},
  urldate = {2022-05-02},
  abstract = {A microcanonical algorithm based upon a deterministic cellular automaton had been presented by Creutz for simulating the Ising system. This model is tested on a SIMD parallel computer working as a cellular automaton and found to fail at low temperature (T ⩽ 0.91Tc). Two improved versions, a random one and a deterministic one, are presented and found to work at low temperature. The deterministic version is successfully tested against the Metropolis algorithm and gives a rather good estimation for the critical exponent β in a 48 × 48 square lattice.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/ottaviSimulationIsingModel1989.pdf}
}

@unpublished{otteGenerativeAdversarialNeural2021,
  title = {Generative {{Adversarial Neural Cellular Automata}}},
  author = {Otte, Maximilian and Delfosse, Quentin and Czech, Johannes and Kersting, Kristian},
  date = {2021-07-19},
  eprint = {2108.04328},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.04328},
  urldate = {2021-08-11},
  abstract = {Motivated by the interaction between cells, the recently introduced concept of Neural Cellular Automata shows promising results in a variety of tasks. So far, this concept was mostly used to generate images for a single scenario. As each scenario requires a new model, this type of generation seems contradictory to the adaptability of cells in nature. To address this contradiction, we introduce a concept using different initial environments as input while using a single Neural Cellular Automata to produce several outputs. Additionally, we introduce GANCA, a novel algorithm that combines Neural Cellular Automata with Generative Adversarial Networks, allowing for more generalization through adversarial training. The experiments show that a single model is capable of learning several images when presented with different inputs, and that the adversarially trained model improves drastically on out-of-distribution data compared to a supervised trained model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,I.2.m},
  file = {/Users/hugo/Papers/pdf/otteGenerativeAdversarialNeural2021.pdf}
}

@article{oudeyerWhatIntrinsicMotivation2007,
  title = {What Is {{Intrinsic Motivation}}? {{A Typology}} of {{Computational Approaches}}},
  shorttitle = {What Is {{Intrinsic Motivation}}?},
  author = {Oudeyer, Pierre-Yves and Kaplan, Frederic},
  date = {2007-11-02},
  journaltitle = {Frontiers in Neurorobotics},
  shortjournal = {Front Neurorobotics},
  volume = {1},
  eprint = {18958277},
  eprinttype = {pmid},
  pages = {6},
  issn = {1662-5218},
  doi = {10.3389/neuro.12.006.2007},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2533589/},
  urldate = {2021-11-29},
  abstract = {Intrinsic motivation, centrally involved in spontaneous exploration and curiosity, is a crucial concept in developmental psychology. It has been argued to be a crucial mechanism for open-ended cognitive development in humans, and as such has gathered a growing interest from developmental roboticists in the recent years. The goal of this paper is threefold. First, it provides a synthesis of the different approaches of intrinsic motivation in psychology. Second, by interpreting these approaches in a computational reinforcement learning framework, we argue that they are not operational and even sometimes inconsistent. Third, we set the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches. This typology is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We argue that this kind of computational typology might be useful for opening new avenues for research both in psychology and developmental robotics.},
  pmcid = {PMC2533589},
  file = {/Users/hugo/Papers/pdf/oudeyerWhatIntrinsicMotivation2007.pdf}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02155},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2022-07-26},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/ouyangTrainingLanguageModels2022.pdf;/Users/hugo/Zotero/storage/AB5D63DU/2203.html}
}

@article{packardAdaptationEdgeChaos1988,
  title = {Adaptation toward the Edge of Chaos},
  author = {Packard, Norman H.},
  date = {1988},
  journaltitle = {Dynamic patterns in complex systems},
  volume = {212},
  pages = {293--301},
  publisher = {{World Scientific}}
}

@article{packardOverviewOpenEndedEvolution2019,
  title = {An {{Overview}} of {{Open-Ended Evolution}}: {{Editorial Introduction}} to the {{Open-Ended Evolution II Special Issue}}},
  shorttitle = {An {{Overview}} of {{Open-Ended Evolution}}},
  author = {Packard, Norman and Bedau, Mark A. and Channon, Alastair and Ikegami, Takashi and Rasmussen, Steen and Stanley, Kenneth O. and Taylor, Tim},
  date = {2019-05-01},
  journaltitle = {Artificial Life},
  shortjournal = {Artificial Life},
  volume = {25},
  number = {2},
  pages = {93--103},
  issn = {1064-5462},
  doi = {10.1162/artl_a_00291},
  url = {https://doi.org/10.1162/artl_a_00291},
  urldate = {2022-11-08},
  abstract = {Nature's spectacular inventiveness, reflected in the enormous diversity of form and function displayed by the biosphere, is a feature of life that distinguishes living most strongly from nonliving. It is, therefore, not surprising that this aspect of life should become a central focus of artificial life. We have known since Darwin that the diversity is produced dynamically, through the process of evolution; this has led life's creative productivity to be called Open-Ended Evolution (OEE) in the field. This article introduces the second of two special issues on current research in OEE and provides an overview of the contents of both special issues. Most of the work was presented at a workshop on open-ended evolution that was held as a part of the 2018 Conference on Artificial Life in Tokyo, and much of it had antecedents in two previous workshops on open-ended evolution at artificial life conferences in Cancun and York. We present a simplified categorization of OEE and summarize progress in the field as represented by the articles in this special issue.},
  file = {/Users/hugo/Papers/pdf/packardOverviewOpenEndedEvolution2019.pdf;/Users/hugo/Zotero/storage/R8RMTBRQ/An-Overview-of-Open-Ended-Evolution-Editorial.html}
}

@article{packardTwodimensionalCellularAutomata1985,
  title = {Two-Dimensional Cellular Automata},
  author = {Packard, Norman H and Wolfram, Stephen},
  date = {1985},
  journaltitle = {Journal of Statistical Physics},
  volume = {38},
  number = {5/6},
  pages = {46},
  langid = {english}
}

@inproceedings{pangSeeingStarsExploiting2005,
  title = {Seeing {{Stars}}: {{Exploiting Class Relationships}} for {{Sentiment Categorization}} with {{Respect}} to {{Rating Scales}}},
  shorttitle = {Seeing {{Stars}}},
  booktitle = {Proceedings of the 43rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}}’05)},
  author = {Pang, Bo and Lee, Lillian},
  date = {2005},
  pages = {115--124},
  file = {/Users/hugo/Papers/pdf/pangSeeingStarsExploiting2005.pdf}
}

@article{panStudyFailureScale2009,
  title = {Study of Failure and Scale Effects in Rocks under Uniaxial Compression Using {{3D}} Cellular Automata},
  author = {Pan, Peng-Zhi and Feng, Xia-Ting and Hudson, John A.},
  date = {2009},
  journaltitle = {International Journal of Rock Mechanics and Mining Sciences},
  volume = {46},
  number = {4},
  pages = {674--685},
  publisher = {{Elsevier}},
  file = {/Users/hugo/Zotero/storage/E834HPD5/S1365160908001743.html}
}

@inproceedings{papineniBleuMethodAutomatic2002,
  title = {Bleu: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {Bleu},
  booktitle = {Proceedings of the 40th Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  date = {2002},
  pages = {311--318},
  file = {/Users/hugo/Papers/pdf/papineniBleuMethodAutomatic2002.pdf}
}

@article{parisiContinualLifelongLearning2019,
  title = {Continual {{Lifelong Learning}} with {{Neural Networks}}: {{A Review}}},
  shorttitle = {Continual {{Lifelong Learning}} with {{Neural Networks}}},
  author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  date = {2019-05},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {113},
  eprint = {1802.07569},
  eprinttype = {arxiv},
  pages = {54--71},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.01.012.},
  url = {http://arxiv.org/abs/1802.07569},
  urldate = {2020-05-06},
  abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/parisiContinualLifelongLearning2019.pdf}
}

@article{pashineDirectedAgingMemory2019,
  title = {Directed Aging, Memory, and Nature’s Greed},
  author = {Pashine, Nidhi and Hexner, Daniel and Liu, Andrea J. and Nagel, Sidney R.},
  date = {2019-12-20},
  journaltitle = {Science Advances},
  volume = {5},
  number = {12},
  pages = {eaax4215},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.aax4215},
  url = {https://www.science.org/doi/10.1126/sciadv.aax4215},
  urldate = {2022-03-15},
  file = {/Users/hugo/Papers/pdf/pashineDirectedAgingMemory.pdf}
}

@inproceedings{pathakCuriosityDrivenExplorationSelfSupervised2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self-Supervised Prediction}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  date = {2017-07},
  pages = {488--489},
  publisher = {{IEEE}},
  location = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.70},
  url = {http://ieeexplore.ieee.org/document/8014804/},
  urldate = {2020-11-26},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-5386-0733-6},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/YPACNY9B/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf}
}

@inproceedings{pathakLearningControlSelfAssembling2019,
  title = {Learning to {{Control Self-Assembling Morphologies}}: {{A Study}} of {{Generalization}} via {{Modularity}}},
  booktitle = {{{NeurIPS}}},
  author = {Pathak, Deepak and Lu, Chris and Darrell, Trevor and Isola, Phillip and Efros, Alexei A},
  date = {2019},
  pages = {13},
  abstract = {Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action and there is another limb nearby, the latter is magnetically connected to the ‘parent’ limb’s motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these dynamic and modular agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the structure of the agent, compared to static and monolithic baselines. Project video and code are available at https://pathak22.github.io/modular-assemblies/.},
  eventtitle = {{{NeurIPS}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/pathakLearningControlSelfAssembling22.pdf}
}

@article{patteeEvolvedOpenEndednessNot2019,
  title = {Evolved {{Open-Endedness}}, {{Not Open-Ended Evolution}}},
  author = {Pattee, Howard H. and Sayama, Hiroki},
  date = {2019-04},
  journaltitle = {Artificial Life},
  volume = {25},
  number = {1},
  pages = {4--8},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/artl_a_00276},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/artl_a_00276},
  urldate = {2020-02-24},
  abstract = {Open-endedness is often considered a prerequisite property of the whole evolutionary system and its dynamical behaviors. In the actual history of evolution on Earth, however, there are many examples showing that open-endedness is rather a consequence of evolution. We suggest that this view, which we call “evolved open-endedness” (EOE), be incorporated more in the research of open-ended evolution. This view should allow for systematic investigation of more nuanced, more concrete research questions about open-endedness and its relationship with adaptation and sustainability.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/patteeEvolvedOpenEndednessNot2019.pdf}
}

@misc{peeblesLearningLearnGenerative2022,
  title = {Learning to {{Learn}} with {{Generative Models}} of {{Neural Network Checkpoints}}},
  author = {Peebles, William and Radosavovic, Ilija and Brooks, Tim and Efros, Alexei A. and Malik, Jitendra},
  date = {2022-09-26},
  number = {arXiv:2209.12892},
  eprint = {2209.12892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.12892},
  urldate = {2022-09-29},
  abstract = {We explore a data-driven approach for learning to optimize neural networks. We construct a dataset of neural network checkpoints and train a generative model on the parameters. In particular, our model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric. At test time, it can optimize neural networks with unseen parameters for downstream tasks in just one update. We find that our approach successfully generates parameters for a wide range of loss prompts. Moreover, it can sample multimodal parameter solutions and has favorable scaling properties. We apply our method to different neural network architectures and tasks in supervised and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/peeblesLearningLearnGenerative2022.pdf}
}

@inproceedings{peleFastRobustEarth2009,
  title = {Fast and Robust {{Earth Mover}}'s {{Distances}}},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Pele, O. and Werman, M.},
  date = {2009-09},
  pages = {460--467},
  doi = {10.1109/ICCV.2009.5459199},
  abstract = {We present a new algorithm for a robust family of Earth Mover's Distances - EMDs with thresholded ground distances. The algorithm transforms the flow-network of the EMD so that the number of edges is reduced by an order of magnitude. As a result, we compute the EMD by an order of magnitude faster than the original algorithm, which makes it possible to compute the EMD on large histograms and databases. In addition, we show that EMDs with thresholded ground distances have many desirable properties. First, they correspond to the way humans perceive distances. Second, they are robust to outlier noise and quantization effects. Third, they are metrics. Finally, experimental results on image retrieval show that thresholding the ground distance of the EMD improves both accuracy and speed.},
  eventtitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  keywords = {Computer vision,Costs,Earth,edge detection,flow-network,Histograms,Humans,Image databases,Image edge detection,Image retrieval,Outlier noise,Quantization,Quantization effects,Robust earth mover's distances,Robustness,Thresholded Ground Distances},
  file = {/Users/hugo/Papers/pdf/peleFastRobustEarth22.pdf;/Users/hugo/Zotero/storage/XHQK7M4B/5459199.html}
}

@inproceedings{pentinaLifelongLearningNoni2015,
  title = {Lifelong {{Learning}} with {{Non-i}}.i.d. {{Tasks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pentina, Anastasia and Lampert, Christoph H},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2015/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html},
  urldate = {2022-08-24},
  abstract = {In this work we aim at extending theoretical foundations of lifelong learning. Previous work analyzing this scenario is based on the assumption that the tasks are sampled i.i.d. from a task environment or limited to strongly constrained data distributions. Instead we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d. sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time. In the first case we prove a PAC-Bayesian theorem, which can be seen as a direct generalization of the analogous previous result for the i.i.d. case. For the second scenario we propose to learn an inductive bias in form of a transfer procedure. We present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.},
  file = {/Users/hugo/Papers/pdf/pentinaLifelongLearningNoni2015.pdf}
}

@inproceedings{pentinaLifelongLearningWeighted2016,
  title = {Lifelong {{Learning}} with {{Weighted Majority Votes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pentina, Anastasia and Urner, Ruth},
  date = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2016/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html},
  urldate = {2022-08-24},
  abstract = {Better understanding of the potential benefits of information transfer and representation learning is an important step towards the goal of building intelligent systems that are able to persist in the world and learn over time. In this work, we consider a setting where the learner encounters a stream of tasks but is able to retain only limited information from each encountered task, such as a learned predictor. In contrast to most previous works analyzing this scenario, we do not make any distributional assumptions on the task generating process. Instead, we formulate a complexity measure that captures the diversity of the observed tasks. We provide a lifelong learning algorithm with error guarantees for every observed task (rather than on average). We show sample complexity reductions in comparison to solving every task in isolation in terms of our task complexity measure. Further, our algorithmic framework can naturally be viewed as learning a representation from encountered tasks with a neural network.},
  file = {/Users/hugo/Papers/pdf/pentinaLifelongLearningWeighted2016.pdf}
}

@inproceedings{pentinaMultitaskLifelongLearning2015a,
  title = {Multi-Task and {{Lifelong Learning}} of {{Kernels}}},
  booktitle = {Algorithmic {{Learning Theory}}},
  author = {Pentina, Anastasia and Ben-David, Shai},
  editor = {Chaudhuri, Kamalika and GENTILE, CLAUDIO and Zilles, Sandra},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {194--208},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-24486-0_13},
  abstract = {We consider a problem of learning kernels for use in SVM classification in the multi-task and lifelong scenarios and provide generalization bounds on the error of a large margin classifier. Our results show that, under mild conditions on the family of kernels used for learning, solving several related tasks simultaneously is beneficial over single task learning. In particular, as the number of observed tasks grows, assuming that in the considered family of kernels there exists one that yields low approximation error on all tasks, the overhead associated with learning such a kernel vanishes and the complexity converges to that of learning when this good kernel is given to the learner.},
  isbn = {978-3-319-24486-0},
  langid = {english},
  keywords = {Kernel learning,Lifelong learning,Multi-task learning},
  file = {/Users/hugo/Papers/pdf/pentinaMultitaskLifelongLearning2015a.pdf}
}

@article{peretoControversiesOriginLife2005,
  title = {Controversies on the Origin of Life},
  author = {Peretó, Juli},
  date = {2005-03},
  journaltitle = {International Microbiology},
  volume = {8},
  number = {1},
  pages = {23--31},
  publisher = {{Sociedad Española de Microbiología}},
  issn = {1139-6709},
  url = {https://scielo.isciii.es/scielo.php?script=sci_abstract&pid=S1139-67092005000100004&lng=es&nrm=iso&tlng=en},
  urldate = {2021-05-10},
  file = {/Users/hugo/Papers/pdf/peretoControversiasSobreOrigen2005.pdf;/Users/hugo/Zotero/storage/WU2ADVVD/scielo.html}
}

@article{pesaventoImplementationNeumannSelfReproducing1995,
  title = {An {{Implementation}} of von {{Neumann}}'s {{Self-Reproducing Machine}}},
  author = {Pesavento, Umberto},
  date = {1995-07-01},
  journaltitle = {Artificial Life},
  shortjournal = {Artificial Life},
  volume = {2},
  number = {4},
  pages = {337--354},
  issn = {1064-5462},
  doi = {10.1162/artl.1995.2.4.337},
  url = {https://doi.org/10.1162/artl.1995.2.4.337},
  urldate = {2022-11-08},
  abstract = {This article describes in detail an implementation of John von Neumann's self-reproducing machine. Self-reproduction is achieved as a special case of construction by a universal constructor. The theoretical proof of the existence of such machines was given by John von Neumann in the early 1950s [6], but was first implemented in 1994, by the author in collaboration with R. Nobili. Our implementation relies on an extension of the state-transition rule of von Neumann's original cellular automaton. This extension was introduced to simplify the design of the constructor. The main operations in our constructor can be mapped into operations of von Neumann's machine.},
  file = {/Users/hugo/Zotero/storage/BSBTCBUP/An-Implementation-of-von-Neumann-s-Self.html}
}

@unpublished{petersPatterndefeatingQuicksort2021,
  title = {Pattern-Defeating {{Quicksort}}},
  author = {Peters, Orson R. L.},
  date = {2021-06-09},
  eprint = {2106.05123},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.05123},
  urldate = {2022-04-21},
  abstract = {A new solution for the Dutch national flag problem is proposed, requiring no three-way comparisons, which gives quicksort a proper worst-case runtime of O(nk) for inputs with k distinct elements. This is used together with other known and novel techniques to construct a hybrid sort that is never significantly slower than regular quicksort while speeding up drastically for many input distributions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/hugo/Papers/pdf/petersPatterndefeatingQuicksort2021.pdf}
}

@unpublished{peyreComputationalOptimalTransport2018,
  title = {Computational {{Optimal Transport}}},
  author = {Peyré, Gabriel and Cuturi, Marco},
  date = {2018-03-01},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1803.00567},
  urldate = {2018-11-22},
  abstract = {Optimal Transport (OT) is a mathematical gem at the interface between probability, analysis and optimization. The goal of that theory is to define geometric tools that are useful to compare probability distributions. Earlier contributions originated from Monge's work in the 18th century, to be later rediscovered under a different formalism by Tolstoi in the 1920's, Kantorovich, Hitchcock and Koopmans in the 1940's. The problem was solved numerically by Dantzig in 1949 and others in the 1950's within the framework of linear programming, paving the way for major industrial applications in the second half of the 20th century. OT was later rediscovered under a different light by analysts in the 90's, following important work by Brenier and others, as well as in the computer vision/graphics fields under the name of earth mover's distances. Recent years have witnessed yet another revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression,classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/peyreComputationalOptimalTransport22.pdf;/Users/hugo/Zotero/storage/RKQ8UCE7/1803.html}
}

@unpublished{phamEfficientNeuralArchitecture2018,
  title = {Efficient {{Neural Architecture Search}} via {{Parameter Sharing}}},
  author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  date = {2018-02-11},
  eprint = {1802.03268},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.03268},
  urldate = {2019-12-12},
  abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/EPMQ893S/phamEfficientNeuralArchitecture2018.pdf;/Users/hugo/Zotero/storage/Y7TQNFXV/1802.html}
}

@inproceedings{pinheiroRecurrentConvolutionalNeural2014,
  title = {Recurrent {{Convolutional Neural Networks}} for {{Scene Labeling}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Pinheiro, Pedro and Collobert, Ronan},
  date = {2014-01-27},
  pages = {82--90},
  url = {http://proceedings.mlr.press/v32/pinheiro14.html},
  urldate = {2020-01-10},
  abstract = {The goal of the scene labeling task is to assign a class label to each pixel in an image.  To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long r...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/pinheiroRecurrentConvolutionalNeural22.pdf}
}

@article{pintoRobustAdversarialReinforcement,
  title = {Robust {{Adversarial Reinforcement Learning}}},
  author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  pages = {10},
  abstract = {Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H∞ control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced – that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/pintoRobustAdversarialReinforcement2.pdf}
}

@article{pittmanExperimentalControlledNOTLogic2003,
  title = {Experimental Controlled-{{NOT}} Logic Gate for Single Photons in the Coincidence Basis},
  author = {Pittman, T. B. and Fitch, M. J. and Jacobs, B. C and Franson, J. D.},
  date = {2003-09-26},
  journaltitle = {Physical Review A},
  shortjournal = {Phys. Rev. A},
  volume = {68},
  number = {3},
  pages = {032316},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevA.68.032316},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.68.032316},
  urldate = {2022-03-15},
  abstract = {We report a proof-of-principle demonstration of a probabilistic controlled-NOT gate for single photons. Single-photon control and target qubits were mixed with a single ancilla photon in a device constructed using only linear optical elements. The successful operation of the controlled-NOT gate relied on post-selected three-photon interference effects, which required the detection of the photons in the output modes.},
  file = {/Users/hugo/Papers/pdf/pittmanExperimentalControlledNOTLogic2003.pdf;/Users/hugo/Zotero/storage/L4CEMR9I/PhysRevA.68.html}
}

@unpublished{pivatoSpectralDomainBoundaries2007,
  title = {Spectral Domain Boundaries in Cellular Automata},
  author = {Pivato, Marcus},
  date = {2007-02-22},
  eprint = {math/0507091},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/math/0507091},
  urldate = {2020-06-16},
  abstract = {Let L:=Z\^D be a D-dimensional lattice. Let A\^L be the Cantor space of L-indexed configurations in a finite alphabet A, with the natural L-action by shifts. A `cellular automaton' is a continuous, shift-commuting self-map F:A\^L--{$>$}A\^L. An `F-invariant subshift' is a closed, F-invariant and shift-invariant subset X of A\^L. Suppose x is an element of A\^L that is X-admissible everywhere except for some small region of L which we call a `defect'. Such defects are analogous to `domain boundaries' in a crystalline solid. It has been empirically observed that these defects persist under iteration of F, and often propagate like `particles' which coalesce or annihilate on contact. We use spectral theory to explain the persistence of some defects under F, and partly explain the outcomes of their collisions.},
  archiveprefix = {arXiv},
  keywords = {37B15 (primary); 68Q80 (secondary)),Mathematics - Dynamical Systems},
  file = {/Users/hugo/Papers/pdf/pivatoSpectralDomainBoundaries2007.pdf;/Users/hugo/Zotero/storage/I99TDGCG/0507091.html}
}

@article{plattProbabilisticOutputsSupport1999,
  title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
  author = {Platt, J},
  date = {1999},
  journaltitle = {Advances in large margin classifiers},
  volume = {10},
  number = {3},
  eprint = {1000183100},
  eprinttype = {pmid},
  pages = {61--74},
  issn = {0262194481},
  doi = {10.1.1.41.1639},
  abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
  file = {/Users/hugo/Papers/pdf/plattProbabilisticOutputsSupport1999.pdf}
}

@unpublished{plautPrincipalSubspacesPrincipal2018,
  title = {From {{Principal Subspaces}} to {{Principal Components}} with {{Linear Autoencoders}}},
  author = {Plaut, Elad},
  date = {2018-12-28},
  eprint = {1804.10253},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.10253},
  urldate = {2020-02-25},
  abstract = {The autoencoder is an effective unsupervised learning model which is widely used in deep learning. It is well known that an autoencoder with a single fully-connected hidden layer, a linear activation function and a squared error cost function trains weights that span the same subspace as the one spanned by the principal component loading vectors, but that they are not identical to the loading vectors. In this paper, we show how to recover the loading vectors from the autoencoder weights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/plautPrincipalSubspacesPrincipal2018.pdf;/Users/hugo/Zotero/storage/4VUPFEWF/1804.html}
}

@book{polimeniJevonsParadoxMyth2015,
  title = {The {{Jevons}}' Paradox and the Myth of Resource Efficiency Improvements},
  author = {Polimeni, John M and Mayumi, Kozo and Giampietro, M and Alcott, Blake},
  date = {2015},
  isbn = {978-1-138-86695-9},
  langid = {english},
  annotation = {OCLC: 909325312},
  file = {/Users/hugo/Papers/pdf/polimeniJevonsParadoxMyth2015.pdf}
}

@unpublished{poluGenerativeLanguageModeling2020,
  title = {Generative {{Language Modeling}} for {{Automated Theorem Proving}}},
  author = {Polu, Stanislas and Sutskever, Ilya},
  date = {2020-09-07},
  eprint = {2009.03393},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.03393},
  urldate = {2021-06-14},
  abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans – the generation of original mathematical terms – might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep learning based system has contributed proofs that were adopted by a formal mathematics community.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/MLMV3EYG/Polu and Sutskever - 2020 - Generative Language Modeling for Automated Theorem.pdf}
}

@article{polyakAccelerationStochasticApproximation1992,
  title = {Acceleration of {{Stochastic Approximation}} by {{Averaging}}},
  author = {Polyak, B. T. and Juditsky, A. B.},
  date = {1992-07},
  journaltitle = {SIAM Journal on Control and Optimization},
  volume = {30},
  number = {4},
  pages = {838--855},
  issn = {0363-0129, 1095-7138},
  doi = {10.1137/0330046},
  url = {http://epubs.siam.org/doi/10.1137/0330046},
  urldate = {2018-12-20},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/polyakAccelerationStochasticApproximation12.pdf}
}

@article{potoyanRecentSuccessesCoarsegrained2013,
  title = {Recent Successes in Coarse-Grained Modeling of {{DNA}}: {{Coarse-grained}} Modeling of {{DNA}}},
  shorttitle = {Recent Successes in Coarse-Grained Modeling of {{DNA}}},
  author = {Potoyan, Davit A. and Savelyev, Alexey and Papoian, Garegin A.},
  date = {2013-01},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume = {3},
  number = {1},
  pages = {69--83},
  issn = {17590876},
  doi = {10.1002/wcms.1114},
  url = {http://doi.wiley.com/10.1002/wcms.1114},
  urldate = {2020-03-06},
  langid = {english}
}

@article{pouwEconomicAnalysisInternational2022,
  title = {Economic Analysis of International Environmental Agreements: Lessons Learnt 2000–2020},
  shorttitle = {Economic Analysis of International Environmental Agreements},
  author = {Pouw, Nicky R. M. and Weikard, Hans-Peter and Howarth, Richard B.},
  date = {2022-04-15},
  journaltitle = {International Environmental Agreements: Politics, Law and Economics},
  shortjournal = {Int Environ Agreements},
  issn = {1573-1553},
  doi = {10.1007/s10784-022-09576-5},
  url = {https://doi.org/10.1007/s10784-022-09576-5},
  urldate = {2022-04-26},
  abstract = {On the occasion of the 20th anniversary of International Environmental Agreements: Politics, Law \& Economics, we conduct an extensive review of papers published in this journal that address the economic dimensions of international environmental agreements (IEAs). We focus particularly on the lessons learnt from this body of literature and the implications for the assessment and design of IEAs in relation to goals such as efficiency, effectiveness, and equity. Our key conclusions run as follows. First, at the international level, universal coalitions are more cost-efficient and effective than fragmented regimes, but more difficult to negotiate and less stable. Second, in developing countries, there is need for substantial external funding to cover the short-run costs of environmental compliance. Third, market-based solutions have been increasingly applied in international agreements but with mixed results. For example, cap-and-trade systems have the potential to achieve greenhouse gas emissions reductions and least economic cost. But in the provisioning of water services, private sector solutions often result in outcomes that are unaffordable for low-income groups or nonviable for businesses, suggesting well-designed public–private partnerships. At the international level, Green Bond markets can attract investors for climate and environmental projects, but implementation failures tend to weaken outcomes. Finally, in practical politics, economically optimal designs are rarely achieved. Future applied economic research should therefore critically investigate institutions and the scope for their reform. Gains in knowledge are expected to come from economic analyses taking a broader perspective on “the economy”, taking institutions and social and ecological relations into account from the start.},
  langid = {english},
  keywords = {Economics of international environmental agreements,Effectiveness,Efficiency,Equity,Lessons learnt,Market mechanisms},
  file = {/Users/hugo/Papers/pdf/pouwEconomicAnalysisInternational2022.pdf}
}

@article{prellBackstopTechnologyGrowth1996,
  title = {Backstop {{Technology}} and {{Growth}}: {{Doomsday}} or {{Steady State}}?},
  shorttitle = {Backstop {{Technology}} and {{Growth}}},
  author = {Prell, Mark A.},
  date = {1996-03},
  journaltitle = {Journal of Environmental Economics and Management},
  shortjournal = {Journal of Environmental Economics and Management},
  volume = {30},
  number = {2},
  pages = {254--264},
  issn = {00950696},
  doi = {10.1006/jeem.1996.0017},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0095069696900170},
  urldate = {2020-04-07},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/prellBackstopTechnologyGrowth1996.pdf}
}

@article{pughQualityDiversityNew2016,
  title = {Quality {{Diversity}}: {{A New Frontier}} for {{Evolutionary Computation}}},
  shorttitle = {Quality {{Diversity}}},
  author = {Pugh, Justin K. and Soros, Lisa B. and Stanley, Kenneth O.},
  date = {2016},
  journaltitle = {Frontiers in Robotics and AI},
  shortjournal = {Front. Robot. AI},
  volume = {3},
  publisher = {{Frontiers}},
  issn = {2296-9144},
  doi = {10.3389/frobt.2016.00040},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full},
  urldate = {2020-07-27},
  abstract = {While evolutionary computation and evolutionary robotics take inspiration from nature, they have long focused mainly on problems of performance optimization. Yet evolution in nature can be interpreted as more nuanced than a process of simple optimization. In particular, natural evolution is a divergent search that optimizes locally within each niche as it simultaneously diversifies. This tendency to discover both quality and diversity at the same time differs from many of the conventional algorithms of machine learning, and also thereby suggests a different foundation for inferring the approach of greatest potential for evolutionary algorithms. In fact, several recent evolutionary algorithms called quality diversity (QD) algorithms(e.g. novelty search with local competition and MAP-Elites) have drawn inspiration from this more nuanced view, aiming to fill a space of possibilities with the best possible example of each type of achievable behavior. The result is a new class of algorithms that return an archive of diverse, high-quality behaviors in a single run. The aim in this paper is to study the application of QD algorithms in challenging environments (in particular complex mazes) to establish their best practices for ambitious domains in the future. In addition to providing insight into cases when QD succeeds and fails, a new approach is investigated that hybridizes multiple views of behaviors (called behavior characterizations) in the same run, which succeeds in overcoming some of the challenges associated with searching for QD with respect to a behavior characterization that is not necessarily sufficient for generating both quality and diversity at the same time.},
  langid = {english},
  keywords = {behavior characterization,Behavioral diversity,Evolutionary computation,neuroevolution,non-objective search,novelty search,quality diversity},
  file = {/Users/hugo/Papers/pdf/pughQualityDiversityNew2016.pdf}
}

@incollection{pujaraKnowledgeGraphIdentification2013,
  title = {Knowledge {{Graph Identification}}},
  booktitle = {The {{Semantic Web}} – {{ISWC}} 2013},
  author = {Pujara, Jay and Miao, Hui and Getoor, Lise and Cohen, William},
  date = {2013},
  volume = {8218},
  pages = {542--557},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41335-3_34},
  url = {http://link.springer.com/10.1007/978-3-642-41335-3_34},
  urldate = {2018-04-24},
  abstract = {Large-scale information processing systems are able to extract massive collections of interrelated facts, but unfortunately transforming these candidate facts into useful knowledge is a formidable challenge. In this paper, we show how uncertain extractions about entities and their relations can be transformed into a knowledge graph. The extractions form an extraction graph and we refer to the task of removing noise, inferring missing information, and determining which candidate facts should be included into a knowledge graph as knowledge graph identification. In order to perform this task, we must reason jointly about candidate facts and their associated extraction confidences, identify coreferent entities, and incorporate ontological constraints. Our proposed approach uses probabilistic soft logic (PSL), a recently introduced probabilistic modeling framework which easily scales to millions of facts. We demonstrate the power of our method on a synthetic Linked Data corpus derived from the MusicBrainz music community and a real-world set of extractions from the NELL project containing over 1M extractions and 70K ontological relations. We show that compared to existing methods, our approach is able to achieve improved AUC and F1 with significantly lower running time.},
  isbn = {978-3-642-41334-6 978-3-642-41335-3},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/pujaraKnowledgeGraphIdentification22.pdf}
}

@misc{radfordImprovingLanguageUnderstanding2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018},
  publisher = {{OpenAI}},
  file = {/Users/hugo/Papers/pdf/radfordImprovingLanguageUnderstanding2018.pdf}
}

@article{radfordLanguageModelsAre2019,
  ids = {radfordLanguageModelsAre},
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  journaltitle = {OpenAI Blog},
  volume = {1},
  number = {8},
  pages = {9},
  file = {/Users/hugo/Papers/pdf/radfordLanguageModelsAre2019.pdf}
}

@misc{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021-02-26},
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2022-07-22},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/radfordLearningTransferableVisual2021.pdf;/Users/hugo/Zotero/storage/QKCVBBVG/2103.html}
}

@misc{raeScalingLanguageModels2022,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d' Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  options = {useprefix=true},
  date = {2022-01-21},
  number = {arXiv:2112.11446},
  eprint = {2112.11446},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.11446},
  url = {http://arxiv.org/abs/2112.11446},
  urldate = {2022-07-22},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/raeScalingLanguageModels2022.pdf;/Users/hugo/Zotero/storage/Z5IKXCV3/2112.html}
}

@article{rafayelyanLargescaleOpticalReservoir2020,
  title = {Large-Scale Optical Reservoir Computing for Spatiotemporal Chaotic Systems Prediction},
  author = {Rafayelyan, Mushegh and Dong, Jonathan and Tan, Yongqi and Krzakala, Florent and Gigan, Sylvain},
  date = {2020},
  journaltitle = {Physical Review X},
  volume = {10},
  number = {4},
  pages = {041037},
  publisher = {{APS}},
  file = {/Users/hugo/Papers/pdf/rafayelyanLargescaleOpticalReservoir2020.pdf;/Users/hugo/Zotero/storage/A6G4TC86/PhysRevX.10.html}
}

@misc{raffelExploringLimitsTransfer2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2020-07-28},
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.10683},
  url = {http://arxiv.org/abs/1910.10683},
  urldate = {2022-07-27},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/raffelExploringLimitsTransfer2020.pdf;/Users/hugo/Zotero/storage/B9LCY5YJ/1910.html}
}

@unpublished{raflerGeneralizationConwayGame2011,
  title = {Generalization of {{Conway}}'s "{{Game}} of {{Life}}" to a Continuous Domain - {{SmoothLife}}},
  author = {Rafler, Stephan},
  date = {2011-11-07},
  eprint = {1111.1567},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1111.1567},
  urldate = {2019-04-22},
  abstract = {We present what we argue is the generic generalization of Conway’s ”Game of Life” to a continuous domain. We describe the theoretical model and the explicit implementation on a computer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/raflerGeneralizationConwayGame22.pdf}
}

@article{raghavanNeuralNetworksGrown,
  title = {Neural Networks Grown and Self-Organized by Noise},
  author = {Raghavan, Guruprasad and Thomson, Matt},
  pages = {11},
  abstract = {Living neural networks emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. Can we develop artificial computational devices that can grow and self-organize without human intervention? In this paper, we propose a biologically inspired developmental algorithm that can ‘grow’ a functional, layered neural network from a single initial cell. The algorithm organizes inter-layer connections to construct retinotopic pooling layers. Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes. The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that ‘learns’ the underlying activity pattern in the first layer. The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to successfully grow and self-organize pooling architectures of different pool-sizes and shapes. The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. We also demonstrate that networks grown from a single unit perform as well as hand-crafted networks on MNIST. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional ‘brains’ in-silico.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/RPC23PXP/raghavanNeuralNetworksGrown.pdf}
}

@article{rahmanCommunityGraphLinguistic,
  title = {Community Graph and Linguistic Analysis to Validate Relationships for Knowledge Base Population},
  author = {Rahman, Rashedur and Grau, Brigitte and Rosset, Sophie},
  pages = {11},
  abstract = {Relation extraction between entities from text plays an important role in information extraction and knowledge discovery related tasks. Relation extraction systems produce a large number of candidates where many of them are not correct. A relation validation method justifies a claimed relation based on the information provided by a system. In this paper, we propose some features by analyzing the community graphs of entities to account for some sort of world knowledge. The proposed features improve validation of relations significantly when they are combined with voting and some stateof-the-art linguistic features.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/rahmanCommunityGraphLinguistic2.pdf}
}

@article{raimbaultModelUrbanEvolution2020,
  title = {A Model of Urban Evolution Based on Innovation Diffusion},
  author = {Raimbault, Juste},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {500--508},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00283},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00283},
  urldate = {2020-07-27},
  abstract = {The dynamics of urban systems can be understood from an evolutionary perspective, in some sense extending biological and cultural evolution. Models for systems of cities implementing elementary evolutionary processes remain however to be investigated. We propose here such a model for urban dynamics at the macroscopic scale, in which the diffusion of innovations between cities captures transformation processes (mutations) and transmission processes (diffusion), using two coupled spatial interaction models. Explorations of the model on synthetic systems of cities show the role of spatial interaction and innovation diffusion ranges on measures of diversity and utility, and the existence of intermediate ranges yielding an optimal utility. Multi-objective optimization shows how the model produces a compromize between utility and diversity. This model paves the way towards more elaborated formalizations of urban evolution.},
  file = {/Users/hugo/Papers/pdf/raimbaultModelUrbanEvolution2020.pdf;/Users/hugo/Zotero/storage/8TK5MV6A/isal_a_00283.html}
}

@unpublished{ramasinghePeriodicityUnifyingFramework2022,
  title = {Beyond {{Periodicity}}: {{Towards}} a {{Unifying Framework}} for {{Activations}} in {{Coordinate-MLPs}}},
  shorttitle = {Beyond {{Periodicity}}},
  author = {Ramasinghe, Sameera and Lucey, Simon},
  date = {2022-03-17},
  eprint = {2111.15135},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.15135},
  urldate = {2022-04-20},
  abstract = {Coordinate-MLPs are emerging as an effective tool for modeling multidimensional continuous signals, overcoming many drawbacks associated with discrete grid-based approximations. However, coordinate-MLPs with ReLU activations, in their rudimentary form, demonstrate poor performance in representing signals with high fidelity, promoting the need for positional embedding layers. Recently, Sitzmann et al. proposed a sinusoidal activation function that has the capacity to omit positional embedding from coordinate-MLPs while still preserving high signal fidelity. Despite its potential, ReLUs are still dominating the space of coordinate-MLPs; we speculate that this is due to the hyper-sensitivity of networks -- that employ such sinusoidal activations -- to the initialization schemes. In this paper, we attempt to broaden the current understanding of the effect of activations in coordinate-MLPs, and show that there exists a broader class of activations that are suitable for encoding signals. We affirm that sinusoidal activations are only a single example in this class, and propose several non-periodic functions that empirically demonstrate more robust performance against random initializations than sinusoids. Finally, we advocate for a shift towards coordinate-MLPs that employ these non-traditional activation functions due to their high performance and simplicity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/ramasinghePeriodicityUnifyingFramework2022.pdf;/Users/hugo/Zotero/storage/T4KJMCRN/2111.html}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2022-07-22},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/rameshHierarchicalTextConditionalImage2022.pdf;/Users/hugo/Zotero/storage/VFMMVL9H/2204.html}
}

@misc{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  number = {arXiv:2102.12092},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.12092},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2022-07-22},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/rameshZeroShotTexttoImageGeneration2021.pdf;/Users/hugo/Zotero/storage/3ESSDPXU/2102.html}
}

@unpublished{ramsauerHopfieldNetworksAll2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-07-16},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2020-08-31},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/ramsauerHopfieldNetworksAll2020.pdf;/Users/hugo/Zotero/storage/KVBZU6S7/2008.html}
}

@unpublished{randazzoMPLPLearningMessage2020,
  title = {{{MPLP}}: {{Learning}} a {{Message Passing Learning Protocol}}},
  shorttitle = {{{MPLP}}},
  author = {Randazzo, Ettore and Niklasson, Eyvind and Mordvintsev, Alexander},
  date = {2020-07-03},
  eprint = {2007.00970},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.00970},
  urldate = {2020-07-21},
  abstract = {We present a novel method for learning the weights of an artificial neural network - a Message Passing Learning Protocol (MPLP). In MPLP, we abstract every operations occurring in ANNs as independent agents. Each agent is responsible for ingesting incoming multidimensional messages from other agents, updating its internal state, and generating multidimensional messages to be passed on to neighbouring agents. We demonstrate the viability of MPLP as opposed to traditional gradient-based approaches on simple feed-forward neural networks, and present a framework capable of generalizing to non-traditional neural network architectures. MPLP is meta learned using end-to-end gradientbased meta-optimisation. We further discuss the observed properties of MPLP and hypothesize its applicability on various fields of deep learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/randazzoMPLPLearningMessage2020.pdf}
}

@article{randazzoSelfclassifyingMNISTDigits2020,
  title = {Self-Classifying {{MNIST Digits}}},
  author = {Randazzo, Ettore and Mordvintsev, Alexander and Niklasson, Eyvind and Levin, Michael and Greydanus, Sam},
  date = {2020-08-27},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {5},
  number = {8},
  pages = {e00027.002},
  issn = {2476-0757},
  doi = {10.23915/distill.00027.002},
  url = {https://distill.pub/2020/selforg/mnist},
  urldate = {2020-09-28},
  abstract = {Training an end-to-end differentiable, self-organising cellular automata for classifying MNIST digits.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/MDMDCW34/mnist.html}
}

@unpublished{ratnerDataProgrammingCreating2016,
  title = {Data {{Programming}}: {{Creating Large Training Sets}}, {{Quickly}}},
  shorttitle = {Data {{Programming}}},
  author = {Ratner, Alexander and De Sa, Christopher and Wu, Sen and Selsam, Daniel and Ré, Christopher},
  date = {2016-05-25},
  eprint = {1605.07723},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1605.07723},
  urldate = {2018-04-16},
  abstract = {Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conflict. We show that by explicitly representing this training set labeling process as a generative model, we can "denoise" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and LSTMs. Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a state-of-the-art LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/X3N8SVMM/ratnerDataProgrammingCreating2016.pdf;/Users/hugo/Zotero/storage/WY7DWNHD/1605.html}
}

@article{ratnerSnorkelRapidTraining2017,
  title = {Snorkel: {{Rapid Training Data Creation}} with {{Weak Supervision}}},
  shorttitle = {Snorkel},
  author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
  date = {2017-11-01},
  journaltitle = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {3},
  eprint = {1711.10160},
  eprinttype = {arxiv},
  pages = {269--282},
  issn = {21508097},
  doi = {10.14778/3157794.3157797},
  url = {http://arxiv.org/abs/1711.10160},
  urldate = {2018-04-16},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8x faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8x speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/ratnerSnorkelRapidTraining22.pdf;/Users/hugo/Zotero/storage/VHIUM9XR/1711.html}
}

@article{rayEvolutionEcologyOptimization,
  title = {Evolution, {{Ecology}} and {{Optimization}} of {{Digital Organisms}}},
  author = {Ray, Thomas S},
  pages = {41},
  abstract = {Digital organisms have been synthesized based on a computer metaphor of organic life in which CPU time is the “energy” resource and memory is the “material” resource. Memory is organized into informational “genetic” patterns that exploit CPU time for self-replication. Mutation generates new forms, and evolution proceeds by natural selection as different “genotypes” compete for CPU time and memory space. In addition, new genotypes appear which exploit other “creatures” for informational or energetic resources.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/rayEvolutionEcologyOptimization2.pdf}
}

@unpublished{realLargeScaleEvolutionImage2017,
  title = {Large-{{Scale Evolution}} of {{Image Classifiers}}},
  author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
  date = {2017-06-11},
  eprint = {1703.01041},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.01041},
  urldate = {2019-12-13},
  abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,I.5.2},
  file = {/Users/hugo/Zotero/storage/94ZJMQEQ/realLargeScaleEvolutionImage2017.pdf;/Users/hugo/Zotero/storage/UW3FKDAU/1703.html}
}

@unpublished{realRegularizedEvolutionImage2019,
  title = {Regularized {{Evolution}} for {{Image Classifier Architecture Search}}},
  author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  date = {2019-02-16},
  eprint = {1802.01548},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1802.01548},
  urldate = {2019-12-13},
  abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,I.5.2},
  file = {/Users/hugo/Zotero/storage/YMVQCFW3/realRegularizedEvolutionImage2019.pdf;/Users/hugo/Zotero/storage/FDXCDNJ3/1802.html}
}

@inproceedings{rebuffiIcarlIncrementalClassifier2017,
  title = {Icarl: {{Incremental}} Classifier and Representation Learning},
  shorttitle = {Icarl},
  booktitle = {Proceedings of the {{IEEE}} Conference on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
  date = {2017},
  pages = {2001--2010},
  file = {/Users/hugo/Papers/pdf/rebuffiIcarlIncrementalClassifier2017.pdf;/Users/hugo/Zotero/storage/JUTEPPXE/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.html}
}

@article{reedGeneralistAgent2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  options = {useprefix=true},
  date = {2022-05-12},
  url = {https://arxiv.org/abs/2205.06175v2},
  urldate = {2022-07-26},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/reedGeneralistAgent2022.pdf;/Users/hugo/Zotero/storage/PLCQEBCU/2205.html}
}

@article{reFeatureEngineeringKnowledge2014,
  title = {Feature {{Engineering}} for {{Knowledge Base Construction}}},
  author = {Ré, Christopher and Sadeghian, Amir Abbas and Shan, Zifei and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce},
  date = {2014},
  pages = {1--14},
  url = {http://arxiv.org/abs/1407.6439},
  abstract = {Knowledge base construction (KBC) is the process of populating a knowledge base, i.e., a relational database together with inference rules, with information extracted from documents and structured sources. KBC blurs the distinction between two traditional database problems, information extraction and information integration. For the last several years, our group has been building knowledge bases with scientific collaborators. Using our approach, we have built knowledge bases that have comparable and sometimes better quality than those constructed by human volunteers. In contrast to these knowledge bases, which took experts a decade or more human years to construct, many of our projects are constructed by a single graduate student. Our approach to KBC is based on joint probabilistic inference and learning, but we do not see inference as either a panacea or a magic bullet: inference is a tool that allows us to be systematic in how we construct, debug, and improve the quality of such systems. In addition, inference allows us to construct these systems in a more loosely coupled way than traditional approaches. To support this idea, we have built the DeepDive system, which has the design goal of letting the user "think about features—not algorithms." We think of DeepDive as declarative in that one specifies what they want but not how to get it. We describe our approach with a focus on feature engineering, which we argue is an understudied problem relative to its importance to end-to-end quality.}
}

@unpublished{regnaultProgressesAnalysisStochastic2007,
  title = {Progresses in the {{Analysis}} of {{Stochastic 2D Cellular Automata}}: A {{Study}} of {{Asynchronous 2D Minority}}},
  shorttitle = {Progresses in the {{Analysis}} of {{Stochastic 2D Cellular Automata}}},
  author = {Regnault, Damien and Schabanel, Nicolas and Thierry, Éric},
  date = {2007-06-17},
  eprint = {0706.2479},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/0706.2479},
  urldate = {2019-05-09},
  abstract = {Cellular automata are often used to model systems in physics, social sciences, biology that are inherently asynchronous. Over the past 20 years, studies have demonstrated that the behavior of cellular automata drastically changed under asynchronous updates. Still, the few mathematical analyses of asynchronism focus on one-dimensional probabilistic cellular automata, either on single examples or on speci c classes. As for other classic dynamical systems in physics, extending known methods from one- to two-dimensional systems is a long lasting challenging problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Discrete Mathematics},
  file = {/Users/hugo/Zotero/storage/SHM7YKNL/regnaultProgressesAnalysisStochastic2007.pdf}
}

@unpublished{reinkeIntrinsicallyMotivatedDiscovery2020,
  title = {Intrinsically {{Motivated Discovery}} of {{Diverse Patterns}} in {{Self-Organizing Systems}}},
  author = {Reinke, Chris and Etcheverry, Mayalen and Oudeyer, Pierre-Yves},
  date = {2020-02-17},
  eprint = {1908.06663},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.06663},
  urldate = {2020-05-14},
  abstract = {In many complex dynamical systems, artificial or natural, one can observe selforganization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify “interesting” patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsicallymotivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the “interesting” features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/reinkeIntrinsicallyMotivatedDiscovery2020.pdf}
}

@incollection{rendellTuringUniversalityGame2002,
  title = {Turing Universality of the Game of Life},
  booktitle = {Collision-Based Computing},
  author = {Rendell, Paul},
  date = {2002},
  pages = {513--539},
  publisher = {{Springer}},
  file = {/Users/hugo/Zotero/storage/54VJPFQ5/978-1-4471-0129-1_18.html}
}

@article{reschkeEventExtractionUsing,
  title = {Event {{Extraction Using Distant Supervision}}},
  author = {Reschke, Kevin and Jankowiak, Martin and Surdeanu, Mihai and Manning, Christopher D and Jurafsky, Daniel},
  pages = {5},
  abstract = {Distant supervision is a successful paradigm that gathers training data for information extraction systems by automatically aligning vast databases of facts with text. Previous work has demonstrated its usefulness for the extraction of binary relations such as a person’s employer or a film’s director. Here, we extend the distant supervision approach to template-based event extraction, focusing on the extraction of passenger counts, aircraft types, and other facts concerning airplane crash events. We present a new publicly available dataset and event extraction task in the plane crash domain based on Wikipedia infoboxes and newswire text. Using this dataset, we conduct a preliminary evaluation of four distantly supervised extraction models which assign named entity mentions in text to entries in the event template. Our results indicate that joint inference over sequences of candidate entity mentions is beneficial. Furthermore, we demonstrate that the SEARN algorithm outperforms a linear-chain CRF and strong baselines with local inference.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/reschkeEventExtractionUsing2.pdf}
}

@unpublished{retsinasRecNetsChannelwiseRecurrent2019,
  title = {{{RecNets}}: {{Channel-wise Recurrent Convolutional Neural Networks}}},
  shorttitle = {{{RecNets}}},
  author = {Retsinas, George and Elafrou, Athena and Goumas, Georgios and Maragos, Petros},
  date = {2019-05-28},
  eprint = {1905.11910},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.11910},
  urldate = {2020-01-10},
  abstract = {In this paper, we introduce Channel-wise recurrent convolutional neural networks (RecNets), a family of novel, compact neural network architectures for computer vision tasks inspired by recurrent neural networks (RNNs). RecNets build upon Channel-wise recurrent convolutional (CRC) layers, a novel type of convolutional layer that splits the input channels into disjoint segments and processes them in a recurrent fashion. In this way, we simulate wide, yet compact models, since the number of parameters is vastly reduced via the parameter sharing of the RNN formulation. Experimental results on the CIFAR-10 and CIFAR-100 image classification tasks demonstrate the superior size-accuracy trade-off of RecNets compared to other compact state-of-the-art architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/E5WUFWR5/retsinasRecNetsChannelwiseRecurrent2019.pdf}
}

@article{richardsExtractingCellularAutomaton1990,
  title = {Extracting Cellular Automaton Rules Directly from Experimental Data},
  author = {Richards, Fred C. and Meyer, Thomas P. and Packard, Norman H.},
  date = {1990-09-02},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {45},
  number = {1},
  pages = {189--202},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(90)90182-O},
  url = {https://www.sciencedirect.com/science/article/pii/016727899090182O},
  urldate = {2022-11-16},
  abstract = {We outline a method for extracting two-dimensional cellular automaton (CA) rules directly from experimental data. The data consist of discrete-space patterns evolving in discrete time. We employ a learning algorithm, the genetic algorithm, to search efficiently through a space of probabilistic CA rules for a local rule that best reproduces the observed behavior of the data. Included are the results of an analysis of patterns generated by the dendritic solidification of NH4Br from a supersaturated acqueous solution.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/richardsExtractingCellularAutomaton1990.pdf;/Users/hugo/Zotero/storage/89JHK4JX/016727899090182O.html}
}

@article{richardsonMarkovLogicNetworks2006,
  title = {Markov Logic Networks},
  author = {Richardson, Matthew and Domingos, Pedro},
  date = {2006-02},
  journaltitle = {Machine Learning},
  volume = {62},
  number = {1-2},
  pages = {107--136},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-006-5833-1},
  url = {http://link.springer.com/10.1007/s10994-006-5833-1},
  urldate = {2018-06-20},
  abstract = {We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a firstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/richardsonMarkovLogicNetworks22.pdf}
}

@inproceedings{richardsonMCTestChallengeDataset2013,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open-Domain Machine Comprehension}} of {{Text}}},
  shorttitle = {{{MCTest}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Richardson, Matthew and Burges, Christopher J.C. and Renshaw, Erin},
  date = {2013-10},
  pages = {193--203},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, Washington, USA}},
  url = {https://aclanthology.org/D13-1020},
  urldate = {2022-02-23},
  eventtitle = {{{EMNLP}} 2013},
  file = {/Users/hugo/Papers/pdf/richardsonMCTestChallengeDataset2013.pdf}
}

@inproceedings{richardsonProbingNaturalLanguage2020,
  title = {Probing Natural Language Inference Models through Semantic Fragments},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Richardson, Kyle and Hu, Hai and Moss, Lawrence and Sabharwal, Ashish},
  date = {2020},
  volume = {34},
  number = {05},
  pages = {8713--8721},
  file = {/Users/hugo/Papers/pdf/richardsonProbingNaturalLanguage2020.pdf;/Users/hugo/Zotero/storage/HLZD7KJS/6397.html}
}

@unpublished{richardsonSurprisingEffectivenessLinear2020,
  title = {The {{Surprising Effectiveness}} of {{Linear Unsupervised Image-to-Image Translation}}},
  author = {Richardson, Eitan and Weiss, Yair},
  date = {2020-07-24},
  eprint = {2007.12568},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2007.12568},
  urldate = {2020-07-28},
  abstract = {Unsupervised image-to-image translation is an inherently ill-posed problem. Recent methods based on deep encoder-decoder architectures have shown impressive results, but we show that they only succeed due to a strong locality bias, and they fail to learn very simple nonlocal transformations (e.g. mapping upside down faces to upright faces). When the locality bias is removed, the methods are too powerful and may fail to learn simple local transformations. In this paper we introduce linear encoder-decoder architectures for unsupervised image to image translation. We show that learning is much easier and faster with these architectures and yet the results are surprisingly effective. In particular, we show a number of local problems for which the results of the linear methods are comparable to those of state-of-the-art architectures but with a fraction of the training time, and a number of nonlocal problems for which the state-ofthe-art fails while linear methods succeed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/richardsonSurprisingEffectivenessLinear2020.pdf}
}

@article{risiEvolvingPlasticNeural2010,
  title = {Evolving Plastic Neural Networks with Novelty Search},
  author = {Risi, Sebastian and Hughes, Charles E. and Stanley, Kenneth O.},
  date = {2010},
  journaltitle = {Adaptive Behavior},
  volume = {18},
  number = {6},
  pages = {470--491},
  publisher = {{Sage Publications Sage UK: London, England}},
  file = {/Users/hugo/Papers/pdf/risiEvolvingPlasticNeural2010.pdf;/Users/hugo/Zotero/storage/SXUFEQAV/1059712310379923.html}
}

@article{rivesBiologicalStructureFunction2021,
  title = {Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},
  author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
  date = {2021-04-13},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {118},
  number = {15},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2016239118},
  url = {https://www.pnas.org/content/118/15/e2016239118},
  urldate = {2021-04-20},
  abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
  langid = {english},
  keywords = {deep learning,generative biology,protein language model,representation learning,synthetic biology},
  file = {/Users/hugo/Papers/pdf/rivesBiologicalStructureFunction2021.pdf;/Users/hugo/Zotero/storage/NTP4SPGK/e2016239118.html}
}

@inproceedings{robinsCatastrophicForgettingNeural1993,
  title = {Catastrophic Forgetting in Neural Networks: The Role of Rehearsal Mechanisms},
  shorttitle = {Catastrophic Forgetting in Neural Networks},
  booktitle = {Proceedings 1993 {{The First New Zealand International Two-Stream Conference}} on {{Artificial Neural Networks}} and {{Expert Systems}}},
  author = {Robins, A.},
  date = {1993-11},
  pages = {65--68},
  doi = {10.1109/ANNES.1993.323080},
  abstract = {The author examines the problem of catastrophic forgetting-the overwriting of old information-in neural networks. He notes that R. Ratcliff's (1990) experiments with rehearsal regimes are a possible solution to catastrophic forgetting and describes sweep rehearsal-a much more effective regime. The use of sweep rehearsal, however, eventually encounters practical limits as the ability to recognize learned items begins to diminish. The author suggests that sweep rehearsal extends the approach of rehearsal mechanisms as far as is practicable, and exposes their eventual limitations.{$<>$}},
  eventtitle = {Proceedings 1993 {{The First New Zealand International Two-Stream Conference}} on {{Artificial Neural Networks}} and {{Expert Systems}}},
  keywords = {Computer science,Formal specifications,Information processing,Intelligent networks,Learning systems,Neural networks,Robustness,Stability,Supervised learning,Unsupervised learning},
  file = {/Users/hugo/Papers/pdf/robinsCatastrophicForgettingNeural1993.pdf;/Users/hugo/Zotero/storage/XIH79UCP/323080.html}
}

@inproceedings{robinsCatastrophicForgettingNeural1993a,
  title = {Catastrophic Forgetting in Neural Networks: The Role of Rehearsal Mechanisms},
  shorttitle = {Catastrophic Forgetting in Neural Networks},
  booktitle = {Proceedings 1993 {{The First New Zealand International Two-Stream Conference}} on {{Artificial Neural Networks}} and {{Expert Systems}}},
  author = {Robins, Anthony},
  date = {1993},
  pages = {65--68},
  publisher = {{IEEE}},
  file = {/Users/hugo/Zotero/storage/F2KKFALC/323080.html}
}

@unpublished{rolnickTacklingClimateChange2019,
  title = {Tackling {{Climate Change}} with {{Machine Learning}}},
  author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  date = {2019-11-05},
  eprint = {1906.05433},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05433},
  urldate = {2022-05-02},
  abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/rolnickTacklingClimateChange2019.pdf;/Users/hugo/Zotero/storage/BYA9DUSU/1906.html}
}

@article{rosaBADGERLearningLearn,
  title = {{{BADGER}}: {{Learning}} to ({{Learn}} [{{Learning Algorithms}}] through {{Multi-Agent Communication}})},
  author = {Rosa, Marek and Afanasjeva, Olga and Andersson, Simon and Davidson, Joseph and Guttenberg, Nicholas and Hlubu, Petr},
  journaltitle = {Agent Communication},
  pages = {15},
  abstract = {In this work, we propose a novel memory-based multi-agent meta-learning architecture and learning procedure that allows for learning of a shared communication policy that enables the emergence of rapid adaptation to new and unseen environments by learning to learn learning algorithms through communication. Behavior, adaptation and learning to adapt emerges from the interactions of homogeneous experts inside a single agent. The proposed architecture should allow for generalization beyond the level seen in existing methods, in part due to the use of a single policy shared by all experts within the agent as well as the inherent modularity of ‘Badger’.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/rosaBADGERLearningLearn2.pdf}
}

@article{rubnerEarthMoverDistance2000,
  title = {The {{Earth Mover}}'s {{Distance As}} a {{Metric}} for {{Image Retrieval}}},
  author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
  date = {2000-11},
  journaltitle = {Int. J. Comput. Vision},
  volume = {40},
  number = {2},
  pages = {99--121},
  issn = {0920-5691},
  doi = {10.1023/A:1026543900054},
  url = {https://doi.org/10.1023/A:1026543900054},
  urldate = {2019-01-01},
  abstract = {We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the EMD is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the EMD with that of other distances.},
  keywords = {color,Earth Mover's Distance,image retrieval,perceptual metrics,texture},
  file = {/Users/hugo/Papers/pdf/rubnerEarthMoverDistance22.pdf}
}

@article{ruckstiessExploringParameterSpace2010,
  title = {Exploring {{Parameter Space}} in {{Reinforcement Learning}}},
  author = {Rückstieß, Thomas and Sehnke, Frank and Schaul, Tom and Wierstra, Daan and Sun, Yi and Schmidhuber, Jürgen},
  date = {2010-03-01},
  journaltitle = {Paladyn, Journal of Behavioral Robotics},
  volume = {1},
  number = {1},
  pages = {14--24},
  publisher = {{De Gruyter}},
  doi = {10.2478/s13230-010-0002-4},
  url = {https://www.degruyter.com/view/journals/pjbr/1/1/article-p14.xml},
  urldate = {2020-06-18},
  abstract = {This paper discusses parameter-based exploration methods for reinforcement learning. Parameter-based methods perturb parameters of a general function approximator directly, rather than adding noise to the resulting actions. Parameter-based exploration unifies reinforcement learning and black-box optimization, and has several advantages over action perturbation. We review two recent parameter-exploring algorithms: Natural Evolution Strategies and Policy Gradients with Parameter-Based Exploration. Both outperform state-of-the-art algorithms in several complex high-dimensional tasks commonly found in robot control. Furthermore, we describe how a novel exploration method, State-Dependent Exploration, can modify existing algorithms to mimic exploration in parameter space.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/ruckstiessExploringParameterSpace22.pdf;/Users/hugo/Zotero/storage/AS5P9LXR/article-p14.html}
}

@unpublished{rudolphDynamicBernoulliEmbeddings2017,
  title = {Dynamic {{Bernoulli Embeddings}} for {{Language Evolution}}},
  author = {Rudolph, Maja and Blei, David},
  date = {2017-03-23},
  eprint = {1703.08052},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.08052},
  urldate = {2018-05-23},
  abstract = {Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/rudolphDynamicBernoulliEmbeddings22.pdf}
}

@report{rumelhartLearningInternalRepresentations1985,
  title = {Learning Internal Representations by Error Propagation},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1985},
  institution = {{California Univ San Diego La Jolla Inst for Cognitive Science}},
  file = {/Users/hugo/Papers/pdf/rumelhartLearningInternalRepresentations1985.pdf;/Users/hugo/Zotero/storage/YB4KYT27/ADA164453.html}
}

@article{rupeLocalCausalStates2018,
  title = {Local {{Causal States}} and {{Discrete Coherent Structures}}},
  author = {Rupe, Adam and Crutchfield, James P.},
  date = {2018-07},
  journaltitle = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  shortjournal = {Chaos},
  volume = {28},
  number = {7},
  eprint = {1801.00515},
  eprinttype = {arxiv},
  pages = {075312},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5021130},
  url = {http://arxiv.org/abs/1801.00515},
  urldate = {2020-06-16},
  abstract = {Coherent structures form spontaneously in nonlinear spatiotemporal systems and are found at all spatial scales in natural phenomena from laboratory hydrodynamic flows and chemical reactions to ocean, atmosphere, and planetary climate dynamics. Phenomenologically, they appear as key components that organize the macroscopic behaviors in such systems. Despite a century of effort, they have eluded rigorous analysis and empirical prediction, with progress being made only recently. As a step in this, we present a formal theory of coherent structures in fully-discrete dynamical field theories. It builds on the notion of structure introduced by computational mechanics, generalizing it to a local spatiotemporal setting. The analysis' main tool employs the \textbackslash localstates, which are used to uncover a system's hidden spatiotemporal symmetries and which identify coherent structures as spatially-localized deviations from those symmetries. The approach is behavior-driven in the sense that it does not rely on directly analyzing spatiotemporal equations of motion, rather it considers only the spatiotemporal fields a system generates. As such, it offers an unsupervised approach to discover and describe coherent structures. We illustrate the approach by analyzing coherent structures generated by elementary cellular automata, comparing the results with an earlier, dynamic-invariant-set approach that decomposes fields into domains, particles, and particle interactions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Mathematics - Dynamical Systems,Nonlinear Sciences - Cellular Automata and Lattice Gases,Nonlinear Sciences - Pattern Formation and Solitons},
  file = {/Users/hugo/Papers/pdf/rupeLocalCausalStates2018.pdf}
}

@article{ryanIntrinsicExtrinsicMotivations2000,
  title = {Intrinsic and {{Extrinsic Motivations}}: {{Classic Definitions}} and {{New Directions}}},
  shorttitle = {Intrinsic and {{Extrinsic Motivations}}},
  author = {Ryan, Richard M. and Deci, Edward L.},
  date = {2000-01-01},
  journaltitle = {Contemporary Educational Psychology},
  shortjournal = {Contemporary Educational Psychology},
  volume = {25},
  number = {1},
  pages = {54--67},
  issn = {0361-476X},
  doi = {10.1006/ceps.1999.1020},
  url = {https://www.sciencedirect.com/science/article/pii/S0361476X99910202},
  urldate = {2021-11-29},
  abstract = {Intrinsic and extrinsic types of motivation have been widely studied, and the distinction between them has shed important light on both developmental and educational practices. In this review we revisit the classic definitions of intrinsic and extrinsic motivation in light of contemporary research and theory. Intrinsic motivation remains an important construct, reflecting the natural human propensity to learn and assimilate. However, extrinsic motivation is argued to vary considerably in its relative autonomy and thus can either reflect external control or true self-regulation. The relations of both classes of motives to basic human needs for autonomy, competence and relatedness are discussed.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/MVH9MJ76/S0361476X99910202.html}
}

@article{sabathielEmergingRepresentationsCounting2020,
  title = {Emerging {{Representations}} for {{Counting}} in a {{Neural Network Agent Interacting}} with a {{Multimodal Environment}}},
  author = {Sabathiel, Silvester and McClelland, James L. and Solstad, Trygve},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {736--743},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00333},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00333},
  urldate = {2020-07-27},
  abstract = {Learning the procedure of counting represents a major step in children's development of the concept of the natural numbers. How children acquire generalized concepts of number and counting skills is still under debate. Here we investigate how a neural network agent develops representations for key concepts of counting while learning to perform several different counting tasks in a multimodal, interactive environment. We identify neural activity and connection patterns that realize a) a representation of the entity to count that was invariant to the task, b) a mapping from entity to number-word, and c) a representation of the number of entities that have been counted that was shared between tasks. The results support the notion that abstract representations of number can arise from integrating experiences across a range of number-related tasks.}
}

@misc{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  date = {2022-05-23},
  number = {arXiv:2205.11487},
  eprint = {2205.11487},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11487},
  url = {http://arxiv.org/abs/2205.11487},
  urldate = {2022-07-26},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/sahariaPhotorealisticTexttoImageDiffusion2022.pdf;/Users/hugo/Zotero/storage/XFDWZYI7/2205.html}
}

@article{salzbergComplexGeneticEvolution2004,
  title = {Complex Genetic Evolution of Artificial Self-Replicators in Cellular Automata},
  author = {Salzberg, Chris and Sayama, Hiroki},
  date = {2004},
  journaltitle = {Complex.},
  volume = {10},
  number = {2},
  pages = {33--39},
  doi = {10.1002/cplx.20060}
}

@unpublished{sanchez-gonzalezGraphNetworksLearnable2018,
  title = {Graph Networks as Learnable Physics Engines for Inference and Control},
  author = {Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
  date = {2018-06-04},
  eprint = {1806.01242},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.01242},
  urldate = {2018-12-28},
  abstract = {Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/sanchez-gonzalezGraphNetworksLearnable22.pdf;/Users/hugo/Zotero/storage/LVFDZF39/1806.html}
}

@misc{sanhDistilBERTDistilledVersion2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  date = {2020-02-29},
  number = {arXiv:1910.01108},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.01108},
  url = {http://arxiv.org/abs/1910.01108},
  urldate = {2022-07-22},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/sanhDistilBERTDistilledVersion2020.pdf;/Users/hugo/Zotero/storage/WMCSBMSD/1910.html}
}

@article{sanmiguelChallengesComplexSystems2012,
  title = {Challenges in Complex Systems Science},
  author = {San Miguel, M. and Johnson, J. H. and Kertesz, J. and Kaski, K. and Díaz-Guilera, A. and MacKay, R. S. and Loreto, V. and Érdi, P. and Helbing, D.},
  date = {2012-11-01},
  journaltitle = {The European Physical Journal Special Topics},
  shortjournal = {Eur. Phys. J. Spec. Top.},
  volume = {214},
  number = {1},
  pages = {245--271},
  issn = {1951-6401},
  doi = {10.1140/epjst/e2012-01694-y},
  url = {https://doi.org/10.1140/epjst/e2012-01694-y},
  urldate = {2022-10-12},
  abstract = {FuturICT foundations are social science, complex systems science, and ICT. The main concerns and challenges in the science of complex systems in the context of FuturICT are laid out in this paper with special emphasis on the Complex Systems route to Social Sciences. This include complex systems having: many heterogeneous interacting parts; multiple scales; complicated transition laws; unexpected or unpredicted emergence; sensitive dependence on initial conditions; path-dependent dynamics; networked hierarchical connectivities; interaction of autonomous agents; self-organisation; non-equilibrium dynamics; combinatorial explosion; adaptivity to changing environments; co-evolving subsystems; ill-defined boundaries; and multilevel dynamics. In this context, science is seen as the process of abstracting the dynamics of systems from data. This presents many challenges including: data gathering by large-scale experiment, participatory sensing and social computation, managing huge distributed dynamic and heterogeneous databases; moving from data to dynamical models, going beyond correlations to cause-effect relationships, understanding the relationship between simple and comprehensive models with appropriate choices of variables, ensemble modeling and data assimilation, modeling systems of systems of systems with many levels between micro and macro; and formulating new approaches to prediction, forecasting, and risk, especially in systems that can reflect on and change their behaviour in response to predictions, and systems whose apparently predictable behaviour is disrupted by apparently unpredictable rare or extreme events. These challenges are part of the FuturICT agenda.},
  langid = {english},
  keywords = {Complex World,European Physical Journal Special Topic,Extreme Event,Mobile Phone Data,Stock Market Crash},
  file = {/Users/hugo/Papers/pdf/sanmiguelChallengesComplexSystems2012.pdf}
}

@unpublished{santambrogioIntroductionOptimalTransport2010,
  title = {Introduction to {{Optimal Transport Theory}}},
  author = {Santambrogio, Filippo},
  date = {2010-09-20},
  eprint = {1009.3856},
  eprinttype = {arxiv},
  primaryclass = {math},
  url = {http://arxiv.org/abs/1009.3856},
  urldate = {2018-11-29},
  abstract = {These notes constitute a sort of Crash Course in Optimal Transport Theory. The different features of the problem of Monge-Kantorovitch are treated, starting from convex duality issues. The main properties of space of probability measures endowed with the distances Wp induced by optimal transport are detailed. The key tools to put in relation optimal transport and PDEs are provided.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Analysis of PDEs,Mathematics - Classical Analysis and ODEs,Mathematics - Differential Geometry,Mathematics - Probability},
  file = {/Users/hugo/Papers/pdf/santambrogioIntroductionOptimalTransport22.pdf}
}

@incollection{sapinResearchCellularAutomaton2003,
  title = {Research of a {{Cellular Automaton Simulating Logic Gates}} by {{Evolutionary Algorithms}}},
  booktitle = {Genetic {{Programming}}},
  author = {Sapin, Emmanuel and Bailleux, Olivier and Jean-Jacques, Chabrier},
  editor = {Ryan, Conor and Soule, Terence and Keijzer, Maarten and Tsang, Edward and Poli, Riccardo and Costa, Ernesto},
  options = {useprefix=true},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {2610},
  pages = {414--423},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36599-0_39},
  url = {http://link.springer.com/10.1007/3-540-36599-0_39},
  urldate = {2020-05-15},
  editorb = {Goos, G. and Hartmanis, J. and van Leeuwen, J.},
  editorbtype = {redactor},
  isbn = {978-3-540-00971-9 978-3-540-36599-0},
  file = {/Users/hugo/Papers/pdf/sapinResearchCellularAutomaton2003.pdf}
}

@inproceedings{sapinResearchComplexForms2004,
  title = {Research of {{Complex Forms}} in {{Cellular Automata}} by {{Evolutionary Algorithms}}},
  booktitle = {Artificial {{Evolution}}},
  author = {Sapin, Emmanuel and Bailleux, Olivier and Chabrier, Jean-Jacques},
  editor = {Liardet, Pierre and Collet, Pierre and Fonlupt, Cyril and Lutton, Evelyne and Schoenauer, Marc},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {357--367},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24621-3_29},
  abstract = {This paper presents an evolutionary approach for the search for new complex cellular automata. Two evolutionary algorithms are used: the first one discovers rules supporting gliders and periodic patterns, and the second one discovers glider guns in cellular automata. An automaton allowing us to simulate AND and NOT gates is discovered. The results are a step toward the general simulation of Boolean circuits by this automaton and show that the evolutionary approach is a promising technic for searching for cellular automata that support universal computation.},
  isbn = {978-3-540-24621-3},
  langid = {english},
  keywords = {Cellular Automaton,Evolutionary Algorithm,Logic Gate,Test Time,Transition Rule},
  file = {/Users/hugo/Papers/pdf/sapinResearchComplexForms2004.pdf}
}

@article{sayamaConstructionTheorySelfreplication2008,
  title = {Construction Theory, Self-Replication, and the Halting Problem},
  author = {Sayama, Hiroki},
  date = {2008-05},
  journaltitle = {Complexity},
  volume = {13},
  number = {5},
  pages = {16--22},
  issn = {10762787, 10990526},
  doi = {10.1002/cplx.20218},
  url = {http://doi.wiley.com/10.1002/cplx.20218},
  urldate = {2020-03-09},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/sayamaConstructionTheorySelfreplication2008.pdf}
}

@article{sayamaNewStructurallyDissolvable1999,
  title = {A New Structurally Dissolvable Self-Reproducing Loop Evolving in a Simple Cellular Automata Space},
  author = {Sayama, H.},
  date = {1999},
  journaltitle = {Artificial Life},
  shortjournal = {Artif. Life},
  volume = {5},
  number = {4},
  eprint = {10829086},
  eprinttype = {pmid},
  pages = {343--365},
  issn = {1064-5462},
  doi = {10.1162/106454699568818},
  abstract = {We constructed a simple evolutionary system, "evoloop," on a deterministic nine-state five-neighbor cellular automata (CA) space by improving the structurally dissolvable self-reproducing loop we had previously contrived [14] after Langton's self-reproducing loop [7]. The principal role of this improvement is to enhance the adaptability (a degree of the variety of situations in which structures in the CA space can operate regularly) of the self-reproductive mechanism of loops. The experiment with evoloop met with the intriguing result that, though no mechanism was explicitly provided to promote evolution, the loops varied through direct interaction of their phenotypes, smaller individuals were naturally selected thanks to their quicker self-reproductive ability, and the whole population gradually evolved toward the smallest ones. This result gives a unique example of evolution of self-replicators where genotypical variation is caused by precedent phenotypical variation. Such interrelation of genotype and phenotype would be one of the important factors driving the evolutionary process of primitive life forms that might have actually occurred in ancient times.},
  langid = {english},
  keywords = {Artificial Intelligence,Automation,Cells,Computer Simulation,Genotype,Models; Biological,Phenotype,Reproduction},
  file = {/Users/hugo/Papers/pdf/sayamaNewStructurallyDissolvable1999.pdf}
}

@article{sayamaQuantifyingEvolutionaryDynamics,
  title = {Quantifying {{Evolutionary Dynamics}} of {{Swarm Chemistry}}},
  author = {Sayama, Hiroki and Wong, Chun},
  pages = {3},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/sayamaQuantifyingEvolutionaryDynamics2.pdf}
}

@article{sayamaRealizationEvolvingEcosystem,
  title = {Toward the {{Realization}} of an {{Evolving Ecosystem}} on {{Cellular Automata}}},
  author = {Sayama, Hiroki},
  pages = {4},
  abstract = {We have contrived the evoloop5, 6, a new selfreproducing loop spontaneously evolving on a simple deterministic 9-state 5-neighbor cellular automata (CA) space. In this article, we examine the evolvability and adaptability of the evoloop through several experiments, the result of which brings us good prospects for the future implementation of an extraordinary large-scale arti cial ecosystem on a superparallel machine environment.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/sayamaRealizationEvolvingEcosystem2.pdf}
}

@inproceedings{sayamaSeekingOpenendedEvolution2011,
  title = {Seeking Open-Ended Evolution in {{Swarm Chemistry}}},
  booktitle = {2011 {{IEEE Symposium}} on {{Artificial Life}} ({{ALIFE}})},
  author = {Sayama, Hiroki},
  date = {2011-04},
  pages = {186--193},
  publisher = {{IEEE}},
  location = {{Paris, France}},
  doi = {10.1109/ALIFE.2011.5954667},
  url = {http://ieeexplore.ieee.org/document/5954667/},
  urldate = {2020-01-09},
  abstract = {This paper reports several new simulation results obtained with the revised Swarm Chemistry model. The model extensions included local transmission of recipes (kinetic rules) between particles, their stochastic differentiation, and competition and mutation of recipes. These extensions aimed to make the swarms capable of demonstrating open-ended evolution. The results indicated that “cooperation” among particles is essential for creating and maintaining macroscopic coherent structures, and that high mutation rates and dynamic environmental conditions may promote continuing evolutionary changes.},
  eventtitle = {2011 {{Ieee Symposium On Artificial Life}} - {{Part Of}} 17273 - 2011 {{Ssci}}},
  isbn = {978-1-61284-062-8},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/sayamaSeekingOpenendedEvolution22.pdf}
}

@article{schmicklHowLifelikeSystem2016,
  title = {How a Life-like System Emerges from a Simple Particle Motion Law},
  author = {Schmickl, Thomas and Stefanec, Martin and Crailsheim, Karl},
  date = {2016-11-30},
  journaltitle = {Scientific Reports},
  volume = {6},
  pages = {37969},
  issn = {2045-2322},
  doi = {10.1038/srep37969},
  url = {https://www.nature.com/articles/srep37969},
  urldate = {2019-05-02},
  abstract = {Self-structuring patterns can be observed all over the universe, from galaxies to molecules to living matter, yet their emergence is waiting for full understanding. We discovered a simple motion law for moving and interacting self-propelled particles leading to a self-structuring, self-reproducing and self-sustaining life-like system. The patterns emerging within this system resemble patterns found in living organisms. The emergent cells we found show a distinct life cycle and even create their own ecosystem from scratch. These structures grow and reproduce on their own, show self-driven behavior and interact with each other. Here we analyze the macroscopic properties of the emerging ecology, as well as the microscopic properties of the mechanism that leads to it. Basic properties of the emerging structures (size distributions, longevity) are analyzed as well as their resilience against sensor or actuation noise. Finally, we explore parameter space for potential other candidates of life. The generality and simplicity of the motion law provokes the thought that one fundamental rule, described by one simple equation yields various structures in nature: it may work on different time- and size scales, ranging from the self-structuring universe, to emergence of living beings, down to the emergent subatomic formation of matter.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/schmicklHowLifelikeSystem22.pdf;/Users/hugo/Zotero/storage/XKSFMSKM/srep37969.html}
}

@unpublished{schmidhuberComputerScientistView1999,
  title = {A {{Computer Scientist}}'s {{View}} of {{Life}}, the {{Universe}}, and {{Everything}}},
  author = {Schmidhuber, Juergen},
  date = {1999-04-13},
  eprint = {quant-ph/9904050},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/quant-ph/9904050},
  urldate = {2020-03-30},
  abstract = {Is the universe computable? If so, it may be much cheaper in terms of information requirements to compute all computable universes instead of just ours. I apply basic concepts of Kolmogorov complexity theory to the set of possible universes, and chat about perceived and true randomness, life, generalization, and learning in a given universe.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Computers and Society,Physics - Computational Physics,Physics - Popular Physics,Quantum Physics},
  file = {/Users/hugo/Papers/pdf/schmidhuberComputerScientistView1999.pdf;/Users/hugo/Zotero/storage/HZZY634Y/9904050.html}
}

@article{schmidhuberSequentialNeuralText1996,
  title = {Sequential Neural Text Compression},
  author = {Schmidhuber, J. and Heil, S.},
  date = {1996-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {7},
  number = {1},
  pages = {142--146},
  issn = {1045-9227},
  doi = {10.1109/72.478398},
  abstract = {The purpose of this paper is to show that neural networks may be promising tools for data compression without loss of information. We combine predictive neural nets and statistical coding techniques to compress text files. We apply our methods to certain short newspaper articles and obtain compression ratios exceeding those of the widely used Lempel-Ziv algorithms (which build the basis of the UNIX functions "compress" and "gzip"). The main disadvantage of our methods is that they are about three orders of magnitude slower than standard methods.},
  keywords = {Arithmetic,backpropagation,Character generation,Compression algorithms,data compression,Decoding,document handling,encoding,feedforward neural nets,feedforward neural networks,file organisation,History,Huffman coding,Hydrogen,linear predictive coding,Neural networks,predictive neural networks,probability,probability distribution,Probability distribution,sequential text compression,statistical coding,Table lookup},
  file = {/Users/hugo/Papers/pdf/schmidhuberSequentialNeuralText12.pdf;/Users/hugo/Zotero/storage/9TW6YS4Y/478398.html}
}

@unpublished{schmidtMinimizingFiniteSums2013,
  title = {Minimizing {{Finite Sums}} with the {{Stochastic Average Gradient}}},
  author = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
  date = {2013-09-10},
  eprint = {1309.2388},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1309.2388},
  urldate = {2018-11-22},
  abstract = {We propose the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from O(1/k\^\{1/2\}) to O(1/k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1/k) to a linear convergence rate of the form O(p\^k) for p \textbackslash textless\{\} 1. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/schmidtMinimizingFiniteSums22.pdf;/Users/hugo/Zotero/storage/K4RBCALR/1309.html}
}

@article{schopfEvidenceArcheanLife2007,
  title = {Evidence of {{Archean}} Life: {{Stromatolites}} and Microfossils},
  shorttitle = {Evidence of {{Archean}} Life},
  author = {Schopf, J. William and Kudryavtsev, Anatoliy B. and Czaja, Andrew D. and Tripathi, Abhishek B.},
  date = {2007-10},
  journaltitle = {Precambrian Research},
  shortjournal = {Precambrian Research},
  volume = {158},
  number = {3-4},
  pages = {141--155},
  issn = {03019268},
  doi = {10.1016/j.precamres.2007.04.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0301926807001209},
  urldate = {2021-05-10},
  langid = {english}
}

@inproceedings{schrauwenOverviewReservoirComputing2007,
  title = {An Overview of Reservoir Computing: Theory, Applications and Implementations},
  shorttitle = {An Overview of Reservoir Computing},
  booktitle = {Proceedings of the 15th European Symposium on Artificial Neural Networks. p. 471-482 2007},
  author = {Schrauwen, Benjamin and Verstraeten, David and Van Campenhout, Jan},
  date = {2007},
  pages = {471--482},
  file = {/Users/hugo/Papers/pdf/schrauwenOverviewReservoirComputing2007.pdf}
}

@unpublished{schulmanGradientEstimationUsing2016,
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  date = {2016-01-05},
  eprint = {1506.05254},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1506.05254},
  urldate = {2021-12-13},
  abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs—directed acyclic graphs that include both deterministic functions and conditional probability distributions—and describe how to easily and automatically derive an unbiased estimator of the loss function’s gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/77QFWJP9/Schulman et al. - 2016 - Gradient Estimation Using Stochastic Computation G.pdf}
}

@article{schulzeNewMonotonicCloneindependent2011,
  title = {A New Monotonic, Clone-Independent, Reversal Symmetric, and Condorcet-Consistent Single-Winner Election Method},
  author = {Schulze, Markus},
  date = {2011-02-01},
  journaltitle = {Social Choice and Welfare},
  shortjournal = {Soc Choice Welf},
  volume = {36},
  number = {2},
  pages = {267--303},
  issn = {1432-217X},
  doi = {10.1007/s00355-010-0475-4},
  url = {https://doi.org/10.1007/s00355-010-0475-4},
  urldate = {2021-12-09},
  abstract = {In recent years, the Pirate Party of Sweden, the Wikimedia Foundation, the Debian project, the “Software in the Public Interest” project, the Gentoo project, and many other private organizations adopted a new single-winner election method for internal elections and referendums. In this article, we will introduce this method, demonstrate that it satisfies, e.g., resolvability, Condorcet, Pareto, reversal symmetry, monotonicity, and independence of clones and present an O(C\^3) algorithm to calculate the winner, where C is the number of alternatives.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/schulzeNewMonotonicCloneindependent2011.pdf}
}

@inproceedings{schutzeWordSpace1993,
  title = {Word {{Space}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 5},
  author = {Schütze, Hinrich},
  date = {1993},
  pages = {895--902},
  publisher = {{Morgan Kaufmann}},
  abstract = {Representations for semantic information about words are necessary for many applications of neural networks in natural language processing. This paper describes an efficient, corpus-based method for inducing distributed semantic representations for a large number of words (50,000) from lexical coccurrence statistics by means of a large-scale linear regression. The representations are successfully applied to word sense disambiguation using a nearest neighbor method.},
  file = {/Users/hugo/Papers/pdf/schutzeWordSpace1993.pdf;/Users/hugo/Zotero/storage/N8VBGT6I/summary.html}
}

@inproceedings{sculleyWebscaleKmeansClustering2010,
  title = {Web-Scale k-Means Clustering},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web},
  author = {Sculley, D.},
  date = {2010-04-26},
  series = {{{WWW}} '10},
  pages = {1177--1178},
  publisher = {{Association for Computing Machinery}},
  location = {{Raleigh, North Carolina, USA}},
  doi = {10.1145/1772690.1772862},
  url = {https://doi.org/10.1145/1772690.1772862},
  urldate = {2020-03-26},
  abstract = {We present two modifications to the popular k-means clustering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast ε-accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml},
  isbn = {978-1-60558-799-8},
  keywords = {scalability,sparse solutions,unsupervised clustering}
}

@article{secretanPicbreederCaseStudy2011,
  title = {Picbreeder: A Case Study in Collaborative Evolutionary Exploration of Design Space},
  shorttitle = {Picbreeder},
  author = {Secretan, Jimmy and Beato, Nicholas and D'Ambrosio, David B. and Rodriguez, Adelein and Campbell, Adam and Folsom-Kovarik, Jeremiah T. and Stanley, Kenneth O.},
  date = {2011},
  journaltitle = {Evolutionary Computation},
  shortjournal = {Evol Comput},
  volume = {19},
  number = {3},
  eprint = {20964537},
  eprinttype = {pmid},
  pages = {373--403},
  issn = {1530-9304},
  doi = {10.1162/EVCO_a_00030},
  abstract = {For domains in which fitness is subjective or difficult to express formally, interactive evolutionary computation (IEC) is a natural choice. It is possible that a collaborative process combining feedback from multiple users can improve the quality and quantity of generated artifacts. Picbreeder, a large-scale online experiment in collaborative interactive evolution (CIE), explores this potential. Picbreeder is an online community in which users can evolve and share images, and most importantly, continue evolving others' images. Through this process of branching from other images, and through continually increasing image complexity made possible by the underlying neuroevolution of augmenting topologies (NEAT) algorithm, evolved images proliferate unlike in any other current IEC system. This paper discusses not only the strengths of the Picbreeder approach, but its challenges and shortcomings as well, in the hope that lessons learned will inform the design of future CIE systems.},
  langid = {english},
  keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Computer-Aided Design,Cooperative Behavior,Models; Theoretical,Search Engine},
  file = {/Users/hugo/Papers/pdf/secretanPicbreederCaseStudy2011.pdf}
}

@unpublished{seguyLargeScaleOptimalTransport2017,
  title = {Large-{{Scale Optimal Transport}} and {{Mapping Estimation}}},
  author = {Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, Rémi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
  date = {2017-11-06},
  eprint = {1711.02283},
  eprinttype = {arxiv},
  primaryclass = {stat},
  url = {http://arxiv.org/abs/1711.02283},
  urldate = {2018-11-22},
  abstract = {This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a \textbackslash textit\{Monge map\} as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/seguyLargeScaleOptimalTransport22.pdf;/Users/hugo/Zotero/storage/7J4MNYBG/1711.html}
}

@inproceedings{selvapeterCellularAutomataImage2009,
  title = {Cellular Automata for Image Noise Filtering},
  booktitle = {2009 {{World Congress}} on {{Nature}} \& {{Biologically Inspired Computing}} ({{NaBIC}})},
  author = {Selvapeter, P. Jebaraj and {Wim Hordijk}},
  date = {2009},
  pages = {193--197},
  publisher = {{IEEE}},
  location = {{Coimbatore, India}},
  doi = {10.1109/NABIC.2009.5393684},
  url = {http://ieeexplore.ieee.org/document/5393684/},
  urldate = {2020-03-07},
  abstract = {This paper presents an image noise filter based on cellular automata (CA), which can remove impulse noise from a noise corrupted image. Uniform cellular automata rules are constructed to filter impulse noise from both binary and gray scale images. Several modifications to the standard CA formulation are then applied to improve the filtering performance. For example, a random CA rule solves the noise propagation present in deterministic CA filters. A mirrored CA is used to solve the fixed boundary problem. The performance of this CA approach is compared with the classical median filter and different switching filters in terms of peak signal to noise ratio. This comparison shows that a filter based on cellular automata provides significant improvements over the standard filtering methods.},
  eventtitle = {2009 {{World Congress}} on {{Nature}} \& {{Biologically Inspired Computing}} ({{NaBIC}})},
  isbn = {978-1-4244-5053-4},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/selvapeterCellularAutomataImage2009.pdf}
}

@inreference{SemanticWeb2018,
  title = {Semantic {{Web}}},
  booktitle = {Wikipedia},
  date = {2018-05-28T10:28:31Z},
  url = {https://en.wikipedia.org/w/index.php?title=Semantic_Web&oldid=843319024},
  urldate = {2018-06-07},
  abstract = {The Semantic Web is an extension of the World Wide Web through standards by the World Wide Web Consortium (W3C). The standards promote common data formats and exchange protocols on the Web, most fundamentally the Resource Description Framework (RDF). According to the W3C, "The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries". The Semantic Web is therefore regarded as an integrator across different content, information applications and systems. The term was coined by Tim Berners-Lee for a web of data (or data web) that can be processed by machines—that is, one in which much of the meaning is machine-readable. While its critics have questioned its feasibility, proponents argue that applications in industry, biology and human sciences research have already proven the validity of the original concept. Berners-Lee originally expressed his vision of the Semantic Web as follows: I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A "Semantic Web", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The "intelligent agents" people have touted for ages will finally materialize. The 2001 Scientific American article by Berners-Lee, Hendler, and Lassila described an expected evolution of the existing Web to a Semantic Web. In 2006, Berners-Lee and colleagues stated that: "This simple idea…remains largely unrealized". In 2013, more than four million Web domains contained Semantic Web markup.},
  langid = {english},
  annotation = {Page Version ID: 843319024},
  file = {/Users/hugo/Zotero/storage/RK9TYD8L/index.html}
}

@article{serraDifferentialEquationsCellular2001,
  title = {Differential {{Equations}} and {{Cellular Automata Models}} of the {{Growth}} of {{Cell Cultures}} and {{Transformation Foci}}},
  author = {Serra, Roberto},
  date = {2001},
  journaltitle = {Complex Systems},
  pages = {34},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/4HYITDGL/Serra - 2001 - Differential Equations and Cellular Automata Model.pdf}
}

@article{shahShapeChangingRobots2020,
  title = {Shape {{Changing Robots}}: {{Bioinspiration}}, {{Simulation}}, and {{Physical Realization}}},
  shorttitle = {Shape {{Changing Robots}}},
  author = {Shah, Dylan and Yang, Bilige and Kriegman, Sam and Levin, Michael and Bongard, Josh and Kramer‐Bottiglio, Rebecca},
  date = {2020-09-21},
  journaltitle = {Advanced Materials},
  shortjournal = {Adv. Mater.},
  pages = {2002882},
  issn = {0935-9648, 1521-4095},
  doi = {10.1002/adma.202002882},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/adma.202002882},
  urldate = {2020-09-23},
  langid = {english}
}

@article{shaliziAutomaticFiltersDetection2006,
  title = {Automatic Filters for the Detection of Coherent Structure in Spatiotemporal Systems},
  author = {Shalizi, Cosma Rohilla and Haslinger, Robert and Rouquier, Jean-Baptiste and Klinkner, Kristina Lisa and Moore, Cristopher},
  date = {2006-03-02},
  journaltitle = {Physical Review E},
  volume = {73},
  number = {3},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.73.036104},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.73.036104},
  urldate = {2020-04-22},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/shaliziAutomaticFiltersDetection2006.pdf}
}

@article{shaliziQuantifyingSelfOrganizationOptimal2004,
  title = {Quantifying {{Self-Organization}} with {{Optimal Predictors}}},
  author = {Shalizi, Cosma Rohilla and Shalizi, Kristina Lisa and Haslinger, Robert},
  date = {2004-09-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {93},
  number = {11},
  eprint = {nlin/0409024},
  eprinttype = {arxiv},
  pages = {118701},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.93.118701},
  url = {http://arxiv.org/abs/nlin/0409024},
  urldate = {2020-04-22},
  abstract = {Despite broad interest in self-organizing systems, there are few quantitative, experimentally-applicable criteria for self-organization. The existing criteria all give counter-intuitive results for important cases. In this Letter, we propose a new criterion, namely an internally-generated increase in the statistical complexity, the amount of information required for optimal prediction of the system's dynamics. We precisely define this complexity for spatially-extended dynamical systems, using the probabilistic ideas of mutual information and minimal sufficient statistics. This leads to a general method for predicting such systems, and a simple algorithm for estimating statistical complexity. The results of applying this algorithm to a class of models of excitable media (cyclic cellular automata) strongly support our proposal.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematics - Statistics Theory,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Nonlinear Sciences - Cellular Automata and Lattice Gases,Physics - Data Analysis; Statistics and Probability},
  file = {/Users/hugo/Papers/pdf/shaliziQuantifyingSelfOrganizationOptimal2004.pdf}
}

@article{shanMorphWorldStateTransition2020,
  title = {{{MorphWorld}}: {{A State Transition Simulator}}},
  shorttitle = {{{MorphWorld}}},
  author = {Shan, Matthew and Moore, Jared M. and Clark, Anthony J.},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {747--749},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00253},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00253},
  urldate = {2020-07-27},
  abstract = {Digital simulation enables a wide variety of research and applications underlying the study of artificial life. In evolutionary robotics applications, the focus is often on maximizing performance of an animat for a specific task. Analyzing evolved behaviors can be challenging, however, given the complex coupling of morphology and brain. In this paper, we introduce a simulation environment built to investigate animats capable of smoothly transitioning between operating modes (e.g., from cautious to aggressive or from one physical form to another). The simulator provides functionality for logging sensory information as well as animat state enabling a deep analysis. Although more abstract than soft-body or rigid-body physics engines, it is lightweight and efficient, allowing for a high number of simulations in a small amount of time. The simulation supplements other more complex physics-based environments providing for greater inspection of sensor information and animat behavior. Furthermore, it is designed to provide an extensible test bed beyond just gait transitions to assess new artificial intelligence and evolutionary algorithms and more importantly the combination of these techniques.}
}

@book{shannonMathematicalTheoryCommunication1975,
  title = {The Mathematical Theory of Communication},
  author = {Shannon, Claude E. and Weaver, Warren},
  date = {1975},
  publisher = {{University of Illinois Press}},
  location = {{Urbana}},
  isbn = {978-0-252-72548-7},
  langid = {english},
  pagetotal = {125},
  annotation = {OCLC: ocm04243302}
}

@inproceedings{sharathkumarNearlinearTimeEapproximation2012,
  title = {A Near-Linear Time ε-Approximation Algorithm for Geometric Bipartite Matching},
  booktitle = {Proceedings of the 44th Symposium on {{Theory}} of {{Computing}} - {{STOC}} '12},
  author = {Sharathkumar, R. and Agarwal, Pankaj K.},
  date = {2012},
  pages = {385},
  publisher = {{ACM Press}},
  location = {{New York, New York, USA}},
  doi = {10.1145/2213977.2214014},
  url = {http://dl.acm.org/citation.cfm?doid=2213977.2214014},
  urldate = {2018-12-24},
  abstract = {For point sets A, B ⊂ Rd, |A| = |B| = n, and for a parameter ε {$>$} 0, we present a Monte Carlo algorithm that computes, in O(npoly(log n, 1/ε)) time, an ε-approximate perfect matching of A and B under any Lp-norm with high probability; the previously best known algorithm takes Ω(n3/2) time. We approximate the Lp-norm using a distance function, d(·, ·) based on a randomly shifted quad-tree. The algorithm iteratively generates an approximate minimum-cost augmenting path under d(·, ·) in time proportional, within a polylogarithmic factor, to the length of the path. We show that the total length of the augmenting paths generated by the algorithm is O((n/ε) log n), implying that the running time of our algorithm is O(npoly(log n, 1/ε)).},
  eventtitle = {The 44th Symposium},
  isbn = {978-1-4503-1245-5},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/sharathkumarNearlinearTimeEapproximation22.pdf}
}

@unpublished{shenOrderedNeuronsIntegrating2018,
  title = {Ordered {{Neurons}}: {{Integrating Tree Structures}} into {{Recurrent Neural Networks}}},
  shorttitle = {Ordered {{Neurons}}},
  author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron},
  date = {2018-10-22},
  eprint = {1810.09536},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1810.09536},
  urldate = {2019-05-07},
  abstract = {Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/shenOrderedNeuronsIntegrating22.pdf}
}

@unpublished{shiConvolutionalLSTMNetwork2015,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  shorttitle = {Convolutional {{LSTM Network}}},
  author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  date = {2015-09-19},
  eprint = {1506.04214},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1506.04214},
  urldate = {2020-11-05},
  abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-theart operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Zotero/storage/BG4QKTD6/Shi et al. - 2015 - Convolutional LSTM Network A Machine Learning App.pdf}
}

@inproceedings{shinContinualLearningDeep2017,
  title = {Continual {{Learning}} with {{Deep Generative Replay}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30: {{Annual Conference}} on {{Neural Information Processing Systems}} 2017, {{December}} 4-9, 2017, {{Long Beach}}, {{CA}}, {{USA}}},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  date = {2017},
  pages = {2990--2999},
  url = {https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html},
  urldate = {2022-03-04}
}

@misc{shoeybiMegatronLMTrainingMultiBillion2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  date = {2020-03-13},
  number = {arXiv:1909.08053},
  eprint = {1909.08053},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.08053},
  url = {http://arxiv.org/abs/1909.08053},
  urldate = {2022-07-26},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/shoeybiMegatronLMTrainingMultiBillion2020.pdf;/Users/hugo/Zotero/storage/868HFM76/1909.html}
}

@report{shorLempelZivNotes,
  title = {Lempel {{Ziv Notes}}},
  author = {Shor, Peter},
  file = {/Users/hugo/Papers/pdf/shorLempelZivNotes.pdf}
}

@misc{shusterLanguageModelsThat2022,
  title = {Language {{Models}} That {{Seek}} for {{Knowledge}}: {{Modular Search}} \& {{Generation}} for {{Dialogue}} and {{Prompt Completion}}},
  shorttitle = {Language {{Models}} That {{Seek}} for {{Knowledge}}},
  author = {Shuster, Kurt and Komeili, Mojtaba and Adolphs, Leonard and Roller, Stephen and Szlam, Arthur and Weston, Jason},
  date = {2022-03-29},
  number = {arXiv:2203.13224},
  eprint = {2203.13224},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.13224},
  url = {http://arxiv.org/abs/2203.13224},
  urldate = {2022-07-27},
  abstract = {Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine-{$>$}Knowledge-{$>$}Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/shusterLanguageModelsThat2022.pdf;/Users/hugo/Zotero/storage/K79H3BCL/2203.html}
}

@inproceedings{siegelmannComputationalPowerNeural1992,
  title = {On the Computational Power of Neural Nets},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
  date = {1992},
  pages = {440--449},
  file = {/Users/hugo/Papers/pdf/siegelmannComputationalPowerNeural1992.pdf;/Users/hugo/Zotero/storage/LPZMVTJZ/130385.html}
}

@article{silvermanConvolutionalNeuralNetworks2019,
  title = {Convolutional {{Neural Networks}} for {{Cellular Automata Classification}}},
  author = {Silverman, Eric},
  date = {2019-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {31},
  pages = {280--281},
  doi = {10.1162/isal_a_00175},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00175},
  urldate = {2020-01-09},
  abstract = {Wolfram famously developed a four-way classification of CA behaviour, with Class IV containing CAs that generate complex, localised structures. However, finding Class IV rules is far from straightforward, and can require extensive, time-consuming searches. This work presents a Convolutional Neural Network (CNN) that was trained on visual examples of CA behaviour, and learned to classify CA images with a high degree of accuracy. I propose that a refinement of this system could serve as a useful aid to CA research, automatically identifying possible candidates for Class IV behaviour and universality, and significantly reducing the time required to find interesting CA rules.},
  file = {/Users/hugo/Zotero/storage/DR9AJR4M/silvermanConvolutionalNeuralNetworks2019.pdf;/Users/hugo/Zotero/storage/DWMHFNP3/isal_a_00175.html}
}

@article{simonArchitectureComplexity1962,
  title = {The {{Architecture}} of {{Complexity}}},
  author = {Simon, Herbert A.},
  date = {1962},
  journaltitle = {Proceedings of the American Philosophical Society},
  volume = {106},
  number = {6},
  eprint = {985254},
  eprinttype = {jstor},
  pages = {467--482},
  publisher = {{American Philosophical Society}},
  issn = {0003-049X},
  file = {/Users/hugo/Papers/pdf/simonArchitectureComplexity1962.pdf}
}

@inproceedings{simsEvolvingVirtualCreatures1994,
  title = {Evolving Virtual Creatures},
  booktitle = {{{SIGGRAPH}}},
  author = {Sims, Karl},
  date = {1994},
  doi = {10.1145/192161.192167},
  abstract = {This paper describes a novel system for creating virtual creatures that move and behave in simulated three-dimensional physical worlds. The morphologies of creatures and the neural systems for controlling their muscle forces are both generated automatically using genetic algorithms. Different fitness evaluation functions are used to direct simulated evolutions towards specific behaviors such as swimming, walking, jumping, and following. A genetic language is presented that uses nodes and connections as its primitive elements to represent directed graphs, which are used to describe both the morphology and the neural circuitry of these creatures. This genetic language defines a hyperspace containing an indefinite number of possible creatures with behaviors, and when it is searched using optimization techniques, a variety of successful and interesting locomotion strategies emerge, some of which would be difficult to invent or built by design.},
  keywords = {Artificial neural network,Directed graph,Electronic circuit,Genetic algorithm,Mathematical morphology,Mathematical optimization,Simulation,Virtual world},
  file = {/Users/hugo/Papers/pdf/simsEvolvingVirtualCreatures12.pdf}
}

@article{sipperCoevolvingNonuniformCellular1996,
  title = {Co-Evolving Non-Uniform Cellular Automata to Perform Computations},
  author = {Sipper, Moshe},
  date = {1996-05-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {92},
  number = {3},
  pages = {193--208},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(95)00286-3},
  url = {https://www.sciencedirect.com/science/article/pii/0167278995002863},
  urldate = {2022-11-16},
  abstract = {A major impediment of cellular automata (CA) stems from the difficulty of utilizing their complex behavior to perform useful computations. Recent studies by Packard and Mitchell et al. have shown that CAs can be evolved to perform a computational task. In this paper non-uniform CAs are studied, where each cell may contain a different rule, in contrast to the original, uniform model. We describe experiments in which non-uniform CAs are evolved to perform the computational task using a local, co-evolutionary algorithm. For radius r = 3 we attain peak performance values of 0.92 comparable to those obtained for uniform CAs (0.93–0.95). This is notable considering the huge search spaces involved, much larger than the uniform case. Smaller radius CAs (previously unstudied in this context) attain performance values of 0.93–0.94. For r = 1 this is considerably higher than the maximal possible uniform CA performance of 0.83, suggesting that non-uniformity reduces connectivity requirements. We thus demonstrate that: (1) non-uniform CAs can attain high computational performance, and (2) such systems can be evolved rather than designed.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/M43AYT53/0167278995002863.html}
}

@unpublished{sitzmannImplicitNeuralRepresentations2020,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  date = {2020-06-17},
  eprint = {2006.09661},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2006.09661},
  urldate = {2020-06-19},
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal’s spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/hugo/Papers/pdf/sitzmannImplicitNeuralRepresentations2020.pdf}
}

@article{smithSimpleComputationUniversalCellular1971,
  title = {Simple {{Computation-Universal Cellular Spaces}}},
  author = {Smith, Alvy Ray},
  date = {1971-07-01},
  journaltitle = {Journal of the ACM},
  shortjournal = {J. ACM},
  volume = {18},
  number = {3},
  pages = {339--353},
  issn = {0004-5411},
  doi = {10.1145/321650.321652},
  url = {https://doi.org/10.1145/321650.321652},
  urldate = {2022-11-14},
  file = {/Users/hugo/Papers/pdf/smithSimpleComputationUniversalCellular1971.pdf}
}

@inproceedings{socherReasoningNeuralTensor2013,
  title = {Reasoning {{With Neural Tensor Networks}} for {{Knowledge Base Completion}}},
  booktitle = {Neural {{Information Processing Systems}} (2003)},
  author = {Socher, Richard and Chen, Danqi and Manning, Christopher and Chen, Danqi and Ng, Andrew},
  date = {2013},
  pages = {926--934},
  url = {https://nlp.stanford.edu/ socherr/SocherChenManningNg_NIPS2013.pdf http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.pdf%0Ahttp://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowle},
  abstract = {Abstract A common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities. The goal of this paper is to develop a more powerful neural network model suitable for ...},
  file = {/Users/hugo/Papers/pdf/socherReasoningNeuralTensor22.pdf}
}

@misc{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-11-18},
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2022-10-17},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/sohl-dicksteinDeepUnsupervisedLearning2015.pdf}
}

@article{soler-toscanoCalculatingKolmogorovComplexity2014,
  title = {Calculating {{Kolmogorov Complexity}} from the {{Output Frequency Distributions}} of {{Small Turing Machines}}},
  author = {Soler-Toscano, Fernando and Zenil, Hector and Delahaye, Jean-Paul and Gauvrit, Nicolas},
  date = {2014-05-08},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  number = {5},
  pages = {e96223},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0096223},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0096223},
  urldate = {2019-06-06},
  abstract = {Drawing on various notions from theoretical computer science, we present a novel numerical approach, motivated by the notion of algorithmic probability, to the problem of approximating the Kolmogorov-Chaitin complexity of short strings. The method is an alternative to the traditional lossless compression algorithms, which it may complement, the two being serviceable for different string lengths. We provide a thorough analysis for all binary strings of length and for most strings of length by running all Turing machines with 5 states and 2 symbols ( with reduction techniques) using the most standard formalism of Turing machines, used in for example the Busy Beaver problem. We address the question of stability and error estimation, the sensitivity of the continued application of the method for wider coverage and better accuracy, and provide statistical evidence suggesting robustness. As with compression algorithms, this work promises to deliver a range of applications, and to provide insight into the question of complexity calculation of finite (and short) strings. Additional material can be found at the Algorithmic Nature Group website at http://www.algorithmicnature.org. An Online Algorithmic Complexity Calculator implementing this technique and making the data available to the research community is accessible at http://www.complexitycalculator.com.},
  langid = {english},
  keywords = {Algorithms,Approximation methods,Computer and information sciences,Computer applications,Graph theory,Information theory,Internet,Probability distribution},
  file = {/Users/hugo/Zotero/storage/UXRZ5NGM/soler-toscanoCalculatingKolmogorovComplexity2014.pdf;/Users/hugo/Zotero/storage/MNFSG6EC/article.html}
}

@article{solomonConvolutionalWassersteinDistances2015,
  title = {Convolutional Wasserstein Distances: Efficient Optimal Transportation on Geometric Domains},
  shorttitle = {Convolutional Wasserstein Distances},
  author = {Solomon, Justin and de Goes, Fernando and Peyré, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
  options = {useprefix=true},
  date = {2015-07-27},
  journaltitle = {ACM Transactions on Graphics},
  volume = {34},
  number = {4},
  pages = {66:1-66},
  issn = {07300301},
  doi = {10.1145/2766963},
  url = {http://dl.acm.org/citation.cfm?doid=2809654.2766963},
  urldate = {2019-01-02},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/solomonConvolutionalWassersteinDistances22.pdf;/Users/hugo/Papers/pdf/solomonConvolutionalWassersteinDistances3.pdf}
}

@article{solomonoffFormalTheoryInductive1964,
  title = {A Formal Theory of Inductive Inference. {{Part I}}},
  author = {Solomonoff, R.J.},
  date = {1964-03},
  journaltitle = {Information and Control},
  volume = {7},
  number = {1},
  pages = {1--22},
  issn = {00199958},
  doi = {10.1016/S0019-9958(64)90223-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0019995864902232},
  urldate = {2019-09-11},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/I9PAPFBI/solomonoffFormalTheoryInductive1964.pdf}
}

@article{solomonoffFormalTheoryInductive1964a,
  title = {A Formal Theory of Inductive Inference. {{Part II}}},
  author = {Solomonoff, R.J.},
  date = {1964-06},
  journaltitle = {Information and Control},
  volume = {7},
  number = {2},
  pages = {224--254},
  issn = {00199958},
  doi = {10.1016/S0019-9958(64)90131-7},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0019995864901317},
  urldate = {2019-09-11},
  langid = {english}
}

@article{soltoggioEvolutionaryAdvantagesNeuromodulated2008,
  title = {Evolutionary {{Advantages}} of {{Neuromodulated Plasticity}} in {{Dynamic}}, {{Reward-based Scenarios}}},
  author = {Soltoggio, Andrea and Bullinaria, John A and Mattiussi, Claudio and Durr, Peter and Floreano, Dario},
  date = {2008},
  pages = {8},
  abstract = {Neuromodulation is considered a key factor for learning and memory in biological neural networks. Similarly, artificial neural networks could benefit from modulatory dynamics when facing certain types of learning problem. Here we test this hypothesis by introducing modulatory neurons to enhance or dampen neural plasticity at target neural nodes. Simulated evolution is employed to design neural control networks for T-maze learning problems, using both standard and modulatory neurons. The results show that experiments where modulatory neurons are enabled achieve better learning in comparison to those where modulatory neurons are disabled. We conclude that modulatory neurons evolve autonomously in the proposed learning tasks, allowing for increased learning and memory capabilities.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/soltoggioEvolutionaryAdvantagesNeuromodulated2008.pdf}
}

@unpublished{songMachineLearningModels2017,
  title = {Machine {{Learning Models}} That {{Remember Too Much}}},
  author = {Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly},
  date = {2017-09-22},
  eprint = {1709.07886},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1709.07886},
  urldate = {2020-12-02},
  abstract = {Machine learning (ML) is becoming a commodity. Numerous ML frameworks and services are available to data holders who are not ML experts but want to train predictive models on their data. It is important that ML models trained on sensitive inputs (e.g., personal images or documents) not leak too much information about the training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/QD8BUZHS/1709.07886.pdf}
}

@inproceedings{sorosIdentifyingNecessaryConditions2014,
  title = {Identifying {{Necessary Conditions}} for {{Open-Ended Evolution}} through the {{Artificial Life World}} of {{Chromaria}}},
  booktitle = {Artificial {{Life}} 14: {{Proceedings}} of the {{Fourteenth International Conference}} on the {{Synthesis}} and {{Simulation}} of {{Living Systems}}},
  author = {Soros, Lisa B. and Stanley, Kenneth},
  date = {2014-07-30},
  pages = {793--800},
  publisher = {{The MIT Press}},
  doi = {10.7551/978-0-262-32621-6-ch128},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/978-0-262-32621-6-ch128},
  urldate = {2019-06-21},
  abstract = {A full understanding of open-ended evolutionary dynamics remains elusive. While artificial life worlds have been proposed to study such dynamics and tests have been devised to try to detect them, no theory yet has enumerated the key conditions that are essential to inducing them. The aim of this paper is to further such an understanding by hypothesizing four conditions that are essential for open-ended evolution to prosper. Of course, any such conditions must be satisfied by nature (the clearest example of an open-ended domain), but we do not know the scope or range of possible worlds that could achieve similarly impressive results. To complement the hypothesized conditions, a new artificial life world called Chromaria is introduced that is designed explicitly for testing them. Chromaria, which is intended to deviate from Earth in key respects that highlight the breadth of possible worlds that can satisfy the four conditions, is shown in this paper to stagnate when one of the four conditions is not met. This initial controlled experiment thereby sets the stage for a broad research program and conversation on investigating and controlling for the key conditions for open-ended evolution.},
  eventtitle = {Artificial {{Life}} 14: {{International Conference}} on the {{Synthesis}} and {{Simulation}} of {{Living Systems}}},
  isbn = {978-0-262-32621-6},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/TTZLYTFG/sorosIdentifyingNecessaryConditions2014.pdf}
}

@online{sorosOpenendednessLastGrand2017,
  title = {Open-Endedness: {{The}} Last Grand Challenge You've Never Heard Of},
  shorttitle = {Open-Endedness},
  author = {Soros, Lisa and Stanley, Kenneth O. and Lehman, Joel},
  date = {2017-12-19T16:00:00-05:00},
  url = {https://www.oreilly.com/radar/open-endedness-the-last-grand-challenge-youve-never-heard-of/},
  urldate = {2021-05-10},
  abstract = {While open-endedness could be a force for discovering intelligence, it could also be a component of AI itself.},
  langid = {american},
  organization = {{O’Reilly Media}},
  file = {/Users/hugo/Zotero/storage/FH8LHHY8/open-endedness-the-last-grand-challenge-youve-never-heard-of.html}
}

@inproceedings{spectorDivisionBlocksOpenended2007,
  title = {Division {{Blocks}} and the {{Open-ended Evolution}} of {{Development}}, {{Form}}, and {{Behavior}}},
  booktitle = {Proceedings of the 9th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Spector, Lee and Klein, Jon and Feinstein, Mark},
  date = {2007},
  series = {{{GECCO}} '07},
  pages = {316--323},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1276958.1277019},
  url = {http://doi.acm.org/10.1145/1276958.1277019},
  urldate = {2019-06-21},
  abstract = {We present a new framework for artificial life involving physically simulated, three-dimensional blocks called Division Blocks. Division Blocks can grow and shrink, divide and form joints, exert forces on joints, and exchange resources. They are controlled by recurrent neural networks that evolve, along with the blocks, by natural selection. Division Blocks are simulated in an environment in which energy is approximately conserved, and in which all energy derives ultimately from a simulated sun via photosynthesis. In this paper we describe our implementation of Division Blocks and some of the ways that it can support experiments on the open-ended evolution of development, form, and behavior. We also present preliminary data from simulations, demonstrating the reliable emergence of cooperative resource transactions.},
  isbn = {978-1-59593-697-4},
  venue = {London, England},
  keywords = {artificial life,breve,development,division blocks,morphology,open-ended evolution,recurrent networks},
  file = {/Users/hugo/Zotero/storage/FUYBG3PA/spectorDivisionBlocksOpenended2007.pdf}
}

@article{spoererRecurrentConvolutionalNeural2017,
  title = {Recurrent {{Convolutional Neural Networks}}: {{A Better Model}} of {{Biological Object Recognition}}},
  shorttitle = {Recurrent {{Convolutional Neural Networks}}},
  author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.01551},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01551/full},
  urldate = {2020-01-10},
  abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and nonhuman primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, \textbackslash emph\{digit clutter\} (where multiple target digits occlude one another) and \textbackslash emph\{digit debris\} (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognising objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognise objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
  langid = {english},
  keywords = {Convolutional Neural Network,object recognition,occlusion,recurrent neural network,top-down processing},
  file = {/Users/hugo/Papers/pdf/spoererRecurrentConvolutionalNeural22.pdf}
}

@unpublished{springerItHardNeural2020,
  title = {It's {{Hard}} for {{Neural Networks To Learn}} the {{Game}} of {{Life}}},
  author = {Springer, Jacob M. and Kenyon, Garrett T.},
  date = {2020-09-02},
  eprint = {2009.01398},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.01398},
  urldate = {2020-10-01},
  abstract = {Efforts to improve the learning abilities of neural networks have focused mostly on the role of optimization methods rather than on weight initializations. Recent findings, however, suggest that neural networks rely on lucky random initial weights of subnetworks called “lottery tickets” that converge quickly to a solution [8]. To investigate how weight initializations affect performance, we examine small convolutional networks that are trained to predict n steps of the two-dimensional cellular automaton Conway’s Game of Life [3], the update rules of which can be implemented efficiently in a 2n + 1 layer convolutional network. We find that networks of this architecture trained on this task rarely converge. Rather, networks require substantially more parameters to consistently converge. In addition, nearminimal architectures are sensitive to tiny changes in parameters: changing the sign of a single weight can cause the network to fail to learn. Finally, we observe a critical value d0 such that training minimal networks with examples in which cells are alive with probability d0 dramatically increases the chance of convergence to a solution. We conclude that training convolutional neural networks to learn the input/output function represented by n steps of Game of Life exhibits many characteristics predicted by the lottery ticket hypothesis [8], namely, that the size of the networks required to learn this function are often significantly larger than the minimal network required to implement the function.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/springerItHardNeural2020.pdf}
}

@incollection{srayApproachSynthesisLife1991,
  title = {An Approach to the Synthesis of Life},
  booktitle = {Artificial {{Life II}}},
  author = {S Ray, Thomas},
  date = {1991-01-01},
  edition = {1},
  pages = {371--408},
  publisher = {{C. G. Langton et al. (Eds.) Addison-Wesley Publishing Co}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/srayApproachSynthesisLife12.pdf}
}

@inproceedings{srivastavaCompeteCompute2013a,
  title = {Compete to {{Compute}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26: 27th {{Annual Conference}} on {{Neural Information Processing Systems}} 2013. {{Proceedings}} of a Meeting Held {{December}} 5-8, 2013, {{Lake Tahoe}}, {{Nevada}}, {{United States}}},
  author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino J. and Schmidhuber, Jürgen},
  editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  date = {2013},
  pages = {2310--2318},
  url = {https://proceedings.neurips.cc/paper/2013/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html},
  urldate = {2022-03-04}
}

@unpublished{srivastavaHighwayNetworks2015,
  title = {Highway {{Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
  date = {2015-11-03},
  eprint = {1505.00387},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1505.00387},
  urldate = {2021-09-30},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  archiveprefix = {arXiv},
  keywords = {68T01,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6},
  file = {/Users/hugo/Papers/pdf/srivastavaHighwayNetworks2015.pdf;/Users/hugo/Zotero/storage/3XHNQWWH/1505.html}
}

@unpublished{standishOpenEndedArtificialEvolution2002,
  title = {Open-{{Ended Artificial Evolution}}},
  author = {Standish, Russell K.},
  date = {2002-10-15},
  eprint = {nlin/0210027},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/nlin/0210027},
  urldate = {2019-06-21},
  abstract = {Of all the issues discussed at \{\textbackslash em Alife VII: Looking Forward, Looking Backward\}, the issue of whether it was possible to create an artificial life system that exhibits \{\textbackslash em open-ended evolution\} of novelty is by far the biggest. Of the 14 open problems settled on as a result of debate at the conference, some 6 are directly, or indirectly related to this issue. Most people equate open-ended evolution with complexity growth, although a priori these seem to be different things. In this paper I report on experiments to measure the complexity of Tierran organisms, and show the results for a \{\textbackslash em size-neutral\} run of Tierra. In this run, no increase in organismal complexity was observed, although organism size did increase through the run. This result is discussed, offering some signposts on path to solving the issue of open ended evolution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Populations and Evolution},
  file = {/Users/hugo/Zotero/storage/VMJKKK34/standishOpenEndedArtificialEvolution2002.pdf}
}

@article{standishOpenendedArtificialEvolution2003,
  title = {Open-Ended Artificial Evolution},
  author = {Standish, Russell K.},
  date = {2003},
  journaltitle = {International Journal of Computational Intelligence and Applications},
  volume = {3},
  number = {02},
  pages = {167--175},
  publisher = {{World Scientific}},
  file = {/Users/hugo/Papers/pdf/standishOpenendedArtificialEvolution2003.pdf;/Users/hugo/Zotero/storage/UEJTXTDR/S1469026803000914.html}
}

@article{stanleyCompetitiveCoevolutionEvolutionary2004,
  title = {Competitive Coevolution through Evolutionary Complexification},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  date = {2004},
  journaltitle = {Journal of artificial intelligence research},
  volume = {21},
  pages = {63--100},
  file = {/Users/hugo/Zotero/storage/6FK3L2QB/stanleyCompetitiveCoevolutionEvolutionary2004.pdf}
}

@article{stanleyCompositionalPatternProducing2007,
  title = {Compositional Pattern Producing Networks: {{A}} Novel Abstraction of Development},
  shorttitle = {Compositional Pattern Producing Networks},
  author = {Stanley, Kenneth O.},
  date = {2007-06-06},
  journaltitle = {Genetic Programming and Evolvable Machines},
  volume = {8},
  number = {2},
  pages = {131--162},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-007-9028-8},
  url = {http://link.springer.com/10.1007/s10710-007-9028-8},
  urldate = {2020-05-15},
  abstract = {Natural DNA can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks (CPPNs), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, CPPNs map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with CPPNs through interactive evolution of two-dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/stanleyCompositionalPatternProducing22.pdf}
}

@inproceedings{stanleyEfficientReinforcementLearning2002,
  title = {Efficient Reinforcement Learning through Evolving Neural Network Topologies},
  booktitle = {Proceedings of the 4th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  date = {2002},
  pages = {569--577},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  file = {/Users/hugo/Zotero/storage/682Y28F2/1004508.html}
}

@article{stanleyEvolvingNeuralNetworks2002,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  date = {2002-06},
  journaltitle = {Evolutionary Computation},
  volume = {10},
  number = {2},
  pages = {99--127},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365602320169811},
  url = {http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
  urldate = {2019-12-13},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/7LF23LL9/stanleyEvolvingNeuralNetworks2002.pdf}
}

@article{stanleyHypercubeBasedIndirectEncoding,
  title = {A {{Hypercube-Based Indirect Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author = {Stanley, Kenneth O and D’Ambrosio, David and Gauci, Jason},
  pages = {39},
  abstract = {Research in neuroevolution, i.e. evolving artificial neural networks (ANNs) through evolutionary algorithms, is inspired by the evolution of biological brains. Because natural evolution discovered intelligent brains with billions of neurons and trillions of connections, perhaps neuroevolution can do the same. Yet while neuroevolution has produced successful results in a variety of domains, the scale of natural brains remains far beyond reach. This paper presents a method called Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective Compositional Pattern Producing Networks (connective CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. The advantage of this approach is that it can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/VX8T7FKQ/stanleyHypercubeBasedIndirectEncoding.pdf}
}

@article{stanleyTaxonomyArtificialEmbryogeny2003,
  title = {A Taxonomy for Artificial Embryogeny},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  date = {2003},
  journaltitle = {Artificial Life},
  volume = {9},
  number = {2},
  pages = {93--130},
  file = {/Users/hugo/Zotero/storage/FMJRW47S/stanleyTaxonomyArtificialEmbryogeny2003.pdf;/Users/hugo/Zotero/storage/WKR6ECMW/Stanley and Miikkulainen - 2003 - A taxonomy for artificial embryogeny.gz;/Users/hugo/Zotero/storage/98IQP3HV/106454603322221487.html}
}

@book{stanleyWhyGreatnessCannot2015,
  title = {Why Greatness Cannot Be Planned: The Myth of the Objective},
  shorttitle = {Why Greatness Cannot Be Planned},
  author = {Stanley, Kenneth O. and Lehman, Joel},
  date = {2015},
  publisher = {{Springer International Publishing}},
  location = {{Cham, Switzerland}},
  isbn = {978-3-319-15523-4},
  langid = {english},
  pagetotal = {141}
}

@article{stanleyWhyOpenEndednessMatters2019,
  title = {Why {{Open-Endedness Matters}}},
  author = {Stanley, Kenneth O.},
  date = {2019-08-01},
  journaltitle = {Artificial Life},
  shortjournal = {Artificial Life},
  volume = {25},
  number = {3},
  pages = {232--235},
  issn = {1064-5462},
  doi = {10.1162/artl_a_00294},
  url = {https://doi.org/10.1162/artl_a_00294},
  urldate = {2022-11-08},
  abstract = {Rather than acting as a review or analysis of the field, this essay focuses squarely on the motivations for investigating open-endedness and the opportunities it opens up. It begins by contemplating the awesome accomplishments of evolution in nature and the profound implications if such a process could be ignited on a computer. Some of the milestones in our understanding so far are then discussed, finally closing by highlighting the grand challenge of formalizing open-endedness as a computational process that can be encoded as an algorithm. The main contribution is to articulate why open-endedness deserves a place alongside artificial intelligence as one of the great computational challenges, and opportunities, of our time.},
  file = {/Users/hugo/Zotero/storage/4FBXDREW/Why-Open-Endedness-Matters.html}
}

@article{stepneyInnovationVariationEmergence2020,
  title = {Innovation, {{Variation}}, and {{Emergence}} in an {{Automata Chemistry}}},
  author = {Stepney, Susan and Hickinbotham, Simon},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {753--760},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00265},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00265},
  urldate = {2020-07-27},
  abstract = {Open-ended novelty is one of the goals of ALife. We use a recent definition of open-endedness, stated in terms of system models and meta-models, to demonstrate how the Stringmol Automata Chemistry achieves variation, innovation and emergence in a replicator-parasite system. We also show how Stringmol's self-modifying code allows certain of these novelties to be exploited within the system itself, while others are only externally observed.}
}

@incollection{stepneyNonclassicalComputationDynamical2012,
  title = {Nonclassical {{Computation}} — {{A Dynamical Systems Perspective}}},
  booktitle = {Handbook of {{Natural Computing}}},
  author = {Stepney, Susan},
  editor = {Rozenberg, Grzegorz and Bäck, Thomas and Kok, Joost N.},
  date = {2012},
  pages = {1979--2025},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-92910-9_59},
  url = {http://link.springer.com/10.1007/978-3-540-92910-9_59},
  urldate = {2020-06-04},
  isbn = {978-3-540-92909-3 978-3-540-92910-9},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/stepneyNonclassicalComputationDynamical2012.pdf}
}

@article{sternSupervisedLearningPhysical2021,
  title = {Supervised {{Learning}} in {{Physical Networks}}: {{From Machine Learning}} to {{Learning Machines}}},
  shorttitle = {Supervised {{Learning}} in {{Physical Networks}}},
  author = {Stern, Menachem and Hexner, Daniel and Rocks, Jason W. and Liu, Andrea J.},
  date = {2021-05-28},
  journaltitle = {Physical Review X},
  shortjournal = {Phys. Rev. X},
  volume = {11},
  number = {2},
  pages = {021045},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevX.11.021045},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.11.021045},
  urldate = {2022-03-15},
  abstract = {Materials and machines are often designed with particular goals in mind, so that they exhibit desired responses to given forces or constraints. Here we explore an alternative approach, namely physical coupled learning. In this paradigm, the system is not initially designed to accomplish a task, but physically adapts to applied forces to develop the ability to perform the task. Crucially, we require coupled learning to be facilitated by physically plausible learning rules, meaning that learning requires only local responses and no explicit information about the desired functionality. We show that such local learning rules can be derived for any physical network, whether in equilibrium or in steady state, with specific focus on two particular systems, namely disordered flow networks and elastic networks. By applying and adapting advances of statistical learning theory to the physical world, we demonstrate the plausibility of new classes of smart metamaterials capable of adapting to users’ needs in situ.},
  file = {/Users/hugo/Papers/pdf/sternSupervisedLearningPhysical2021.pdf;/Users/hugo/Zotero/storage/9Q4QL247/PhysRevX.11.html}
}

@article{stoneEvolutionCellularAutomata2009,
  title = {Evolution of Cellular Automata with Memory: {{The Density Classification Task}}},
  shorttitle = {Evolution of Cellular Automata with Memory},
  author = {Stone, Christopher and Bull, Larry},
  date = {2009-08-01},
  journaltitle = {Biosystems},
  shortjournal = {Biosystems},
  volume = {97},
  number = {2},
  pages = {108--116},
  issn = {0303-2647},
  doi = {10.1016/j.biosystems.2009.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0303264709000720},
  urldate = {2019-06-27},
  abstract = {The Density Classification Task is a well known test problem for two-state discrete dynamical systems. For many years researchers have used a variety of evolutionary computation approaches to evolve solutions to this problem. In this paper, we investigate the evolvability of solutions when the underlying Cellular Automaton is augmented with a type of memory based on the Least Mean Square algorithm. To obtain high performance solutions using a simple non-hybrid genetic algorithm, we design a novel representation based on the ternary representation used for Learning Classifier Systems. The new representation is found able to produce superior performance to the bit string traditionally used for representing Cellular automata. Moreover, memory is shown to improve evolvability of solutions and appropriate memory settings are able to be evolved as a component part of these solutions.},
  keywords = {Cellular automata,Density Classification,Genetic algorithm,Majority problem,Memory},
  file = {/Users/hugo/Papers/pdf/stoneEvolutionCellularAutomata22.pdf}
}

@article{storerDataCompressionTextual1982,
  title = {Data Compression via Textual Substitution},
  author = {Storer, James A. and Szymanski, Thomas G.},
  date = {1982-10},
  journaltitle = {Journal of the ACM (JACM)},
  volume = {29},
  number = {4},
  pages = {928--951},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/322344.322346},
  url = {http://dl.acm.org/doi/10.1145/322344.322346},
  urldate = {2020-03-04},
  langid = {english}
}

@article{suchanekPARISProbabilisticAlignment2011,
  title = {{{PARIS}}: {{Probabilistic Alignment}} of {{Relations}}, {{Instances}}, and {{Schema}}},
  author = {Suchanek, Fabian M. and Abiteboul, Serge and Senellart, Pierre},
  date = {2011},
  pages = {157--168},
  issn = {21508097},
  doi = {10.14778/2078331.2078332},
  url = {http://arxiv.org/abs/1111.7164},
  abstract = {One of the main challenges that the Semantic Web faces is the integration of a growing number of independently designed ontologies. In this work, we present PARIS, an approach for the automatic alignment of ontologies. PARIS aligns not only instances, but also relations and classes. Alignments at the instance level cross-fertilize with alignments at the schema level. Thereby, our system provides a truly holistic solution to the problem of ontology alignment. The heart of the approach is probabilistic, i.e., we measure degrees of matchings based on probability estimates. This allows PARIS to run without any parameter tuning. We demonstrate the efficiency of the algorithm and its precision through extensive experiments. In particular, we obtain a precision of around 90\% in experiments with some of the world's largest ontologies.},
  file = {/Users/hugo/Papers/pdf/suchanekPARISProbabilisticAlignment22.pdf}
}

@article{suchanekYAGOCoreSemantic2007,
  title = {{{YAGO}}: A Core of Semantic Knowledge},
  author = {Suchanek, Fabian M and Kasneci, Gjergji and Weikum, Gerhard},
  date = {2007},
  journaltitle = {Proceedings of the 16th international conference on World Wide Web},
  eprint = {19683066},
  eprinttype = {pmid},
  pages = {697--706},
  issn = {01695347},
  doi = {10.1145/1242572.1242667},
  url = {http://www2007.org/papers/paper391.pdf},
  abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as hasWonPrize). The facts have been automatically ex-tracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuris-tic methods described in this paper. The resulting knowl-edge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organiza-tions, products, etc. with their semantic relationships – and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact cor-rectness shows an accuracy of about 95\%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.},
  keywords = {General General Terms Knowledge Extraction,Ontologies Keywords Wikipedia,WordNet},
  file = {/Users/hugo/Papers/pdf/suchanekYAGOCoreSemantic22.pdf}
}

@unpublished{sudhakaranGrowing3DArtefacts2021,
  title = {Growing {{3D Artefacts}} and {{Functional Machines}} with {{Neural Cellular Automata}}},
  author = {Sudhakaran, Shyam and Grbic, Djordje and Li, Siyan and Katona, Adam and Najarro, Elias and Glanois, Claire and Risi, Sebastian},
  date = {2021-06-04},
  eprint = {2103.08737},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.08737},
  urldate = {2021-08-19},
  abstract = {Neural Cellular Automata (NCAs) have been proven effective in simulating morphogenetic processes, the continuous construction of complex structures from very few starting cells. Recent developments in NCAs lie in the 2D domain, namely reconstructing target images from a single pixel or infinitely growing 2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D convolutions in the proposed neural network architecture. Minecraft is selected as the environment for our automaton since it allows the generation of both static structures and moving machines. We show that despite their simplicity, NCAs are capable of growing complex entities such as castles, apartment blocks, and trees, some of which are composed of over 3,000 blocks. Additionally, when trained for regeneration, the system is able to regrow parts of simple functional machines, significantly expanding the capabilities of simulated morphogenetic systems. The code for the experiment in this paper can be found at: https://github.com/real-itu/ 3d-artefacts-nca.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/sudhakaranGrowing3DArtefacts2021.pdf}
}

@unpublished{sukhbaatarAdaptiveAttentionSpan2019,
  title = {Adaptive {{Attention Span}} in {{Transformers}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  date = {2019-08-08},
  eprint = {1905.07799},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.07799},
  urldate = {2019-11-27},
  abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/5RQNXSXQ/sukhbaatarAdaptiveAttentionSpan2019.pdf}
}

@article{sullivanSalvationDoug1993,
  title = {The Salvation of {{Doug}}},
  author = {Sullivan, William T.},
  date = {1993},
  journaltitle = {Generations},
  volume = {1},
  number = {3},
  pages = {1439--1443},
  file = {/Users/hugo/Zotero/storage/PIDMQPZF/dougandbill.html}
}

@article{sunRevealingPredictabilityIntrinsic2020,
  title = {Revealing the Predictability of Intrinsic Structure in Complex Networks},
  author = {Sun, Jiachen and Feng, Ling and Xie, Jiarong and Ma, Xiao and Wang, Dashun and Hu, Yanqing},
  date = {2020-01-29},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {1--10},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14418-6},
  url = {https://www.nature.com/articles/s41467-020-14418-6},
  urldate = {2020-01-29},
  abstract = {The likelihood of linking within a complex network is of importance to solve real-world problems, but it is challenging to predict. Sun et al. show that the link predictability limit can be well estimated by measuring the shortest compression length of a network without a need of prediction algorithm.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/5FWZUEMS/sunRevealingPredictabilityIntrinsic2020.pdf;/Users/hugo/Zotero/storage/MIZIILH5/s41467-020-14418-6.html}
}

@article{sussilloGeneratingCoherentPatterns2009,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  author = {Sussillo, David and Abbott, L.F.},
  date = {2009-08},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  volume = {63},
  number = {4},
  pages = {544--557},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.07.018},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627309005479},
  urldate = {2022-03-15},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/sussilloGeneratingCoherentPatterns2009.pdf}
}

@article{talaminiCommunicationDecisionMaking2020,
  title = {Communication in {{Decision Making}}: {{Competition}} Favors {{Inequality}}},
  shorttitle = {Communication in {{Decision Making}}},
  author = {Talamini, Jacopo and Medvet, Eric and Bartoli, Alberto and De Lorenzo, Andrea},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {570--577},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00248},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00248},
  urldate = {2020-07-27},
  abstract = {We consider a multi-agent system in which the individual goal is to collect resources, but where the amount of collected resources depends also on others decision. Agents can communicate and can take advantage of being communicated other agents’ plan: therefore they may develop more profitable strategies. We wonder if some kind of collective behaviour, with respect to communication, emerges in this system without being explicitly promoted. To investigate this aspect, we design three different scenarios, respectively a cooperative, a competitive, and a mixed one, in which agents’ behaviors are individually learned by means of reinforcement learning. We consider different strategies concerning communication and learning, including no-communication, always-communication, and optional-communication. Experimental results show that always-communication leads to a collective behaviour with the best results in terms of both overall earned resources and equality between agents. On the other hand optional-communication strategy leads to similar collective strategies in some of these scenarios, but in other scenarios some agents develop individual behaviours that oppose to the collective welfare and thus result in high inequality.},
  file = {/Users/hugo/Papers/pdf/talaminiCommunicationDecisionMaking2020.pdf;/Users/hugo/Zotero/storage/RWY4QXSY/isal_a_00248.html}
}

@article{tanakaRecentAdvancesPhysical2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  date = {2019-07-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608019300784},
  urldate = {2022-02-24},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  langid = {english},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing},
  file = {/Users/hugo/Papers/pdf/tanakaRecentAdvancesPhysical2019.pdf;/Users/hugo/Zotero/storage/LWYFUFGG/S0893608019300784.html}
}

@unpublished{tancikFourierFeaturesLet2020,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  date = {2020-06-18},
  eprint = {2006.10739},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.10739},
  urldate = {2020-06-20},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/tancikFourierFeaturesLet2020.pdf;/Users/hugo/Zotero/storage/AMYAPKQE/2006.html}
}

@unpublished{tancikLearnedInitializationsOptimizing2021,
  title = {Learned {{Initializations}} for {{Optimizing Coordinate-Based Neural Representations}}},
  author = {Tancik, Matthew and Mildenhall, Ben and Wang, Terrance and Schmidt, Divi and Srinivasan, Pratul P. and Barron, Jonathan T. and Ng, Ren},
  date = {2021-03-23},
  eprint = {2012.02189},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.02189},
  urldate = {2021-08-05},
  abstract = {Coordinate-based neural representations have shown significant promise as an alternative to discrete, array-based representations for complex low dimensional signals. However, optimizing a coordinate-based network from randomly initialized weights for each new signal is inefficient. We propose applying standard meta-learning algorithms to learn the initial weight parameters for these fully-connected networks based on the underlying class of signals being represented (e.g., images of faces or 3D models of chairs). Despite requiring only a minor change in implementation, using these learned initial weights enables faster convergence during optimization and can serve as a strong prior over the signal class being modeled, resulting in better generalization when only partial observations of a given signal are available. We explore these benefits across a variety of tasks, including representing 2D images, reconstructing CT scans, and recovering 3D shapes and scenes from 2D image observations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/tancikLearnedInitializationsOptimizing2021.pdf;/Users/hugo/Zotero/storage/E3S7987D/2012.html}
}

@incollection{taylorChapterCreativityEvolution2002,
  title = {Chapter 1 - {{Creativity}} in {{Evolution}}: {{Individuals}}, {{Interactions}}, and {{Environments}}},
  shorttitle = {Chapter 1 - {{Creativity}} in {{Evolution}}},
  booktitle = {Creative {{Evolutionary Systems}}},
  author = {Taylor, Tim},
  editor = {Bentley, Peter J and Corne, David W.},
  date = {2002-01-01},
  series = {The {{Morgan Kaufmann Series}} in {{Artificial Intelligence}}},
  pages = {79--108},
  publisher = {{Morgan Kaufmann}},
  location = {{San Francisco}},
  doi = {10.1016/B978-155860673-9/50037-9},
  url = {https://www.sciencedirect.com/science/article/pii/B9781558606739500379},
  urldate = {2022-11-08},
  abstract = {This chapter aims to discuss various issues concerning the design of artificial evolutionary systems and their capacity for creative evolution. The discussion emphasizes that it is necessary to consider not just the design of individuals, but also the sort of environment in which they live, and how individuals can interact with each other and with the physical (i.e., abiotic) environment. Much of this discussion is presented in relation to a hypothetical structure that would be suitable for acting as a robust initial seed for an open-ended, creative evolutionary process. The chapter also discusses how these issues should be integrated into a unifying framework in which the study of creative artificial evolutionary systems can be developed. The chapter highlights various open questions relating to issues that need to be addressed in future research.},
  isbn = {978-1-55860-673-9},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/4CHSTF7C/B9781558606739500379.html}
}

@inproceedings{taylorCrossdomainTransferReinforcement2007,
  title = {Cross-Domain Transfer for Reinforcement Learning},
  booktitle = {Machine {{Learning}}, {{Proceedings}} of the {{Twenty-Fourth International Conference}} ({{ICML}} 2007), {{Corvallis}}, {{Oregon}}, {{USA}}, {{June}} 20-24, 2007},
  author = {Taylor, Matthew E. and Stone, Peter},
  editor = {Ghahramani, Zoubin},
  date = {2007},
  series = {{{ACM International Conference Proceeding Series}}},
  volume = {227},
  pages = {879--886},
  publisher = {{ACM}},
  doi = {10.1145/1273496.1273607},
  file = {/Users/hugo/Papers/pdf/taylorCrossdomainTransferReinforcement2007.pdf}
}

@article{taylorImportanceOpenEndednessSake2020a,
  ids = {taylorImportanceOpenEndednessSake2020},
  title = {The {{Importance}} of {{Open-Endedness}} (for the {{Sake}} of {{Open-Endedness}})},
  author = {Taylor, Tim},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  eprint = {2006.03079},
  eprinttype = {arxiv},
  pages = {578--580},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00257},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00257},
  urldate = {2020-07-27},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/taylorImportanceOpenEndednessSake2020.pdf;/Users/hugo/Papers/pdf/taylorImportanceOpenEndednessSake2020a.pdf;/Users/hugo/Zotero/storage/YWFDY6IT/isal_a_00257.html}
}

@article{taylorOpenEndedEvolutionPerspectives2016,
  title = {Open-{{Ended Evolution}}: {{Perspectives}} from the {{OEE Workshop}} in {{York}}},
  shorttitle = {Open-{{Ended Evolution}}},
  author = {Taylor, Tim and Bedau, Mark and Channon, Alastair and Ackley, David and Banzhaf, Wolfgang and Beslon, Guillaume and Dolson, Emily and Froese, Tom and Hickinbotham, Simon and Ikegami, Takashi and McMullin, Barry and Packard, Norman and Rasmussen, Steen and Virgo, Nathaniel and Agmon, Eran and Clark, Edward and McGregor, Simon and Ofria, Charles and Ropella, Glen and Spector, Lee and Stanley, Kenneth O. and Stanton, Adam and Timperley, Christopher and Vostinar, Anya and Wiser, Michael},
  date = {2016-08},
  journaltitle = {Artificial Life},
  volume = {22},
  number = {3},
  pages = {408--423},
  issn = {1064-5462},
  doi = {10.1162/ARTL_a_00210},
  abstract = {We describe the content and outcomes of the First Workshop on Open-Ended Evolution: Recent Progress and Future Milestones (OEE1), held during the ECAL 2015 conference at the University of York, UK, in July 2015. We briefly summarize the content of the workshop's talks, and identify the main themes that emerged from the open discussions. Two important conclusions from the discussions are: (1) the idea of pluralism about OEE—it seems clear that there is more than one interesting and important kind of OEE; and (2) the importance of distinguishing observable behavioral hallmarks of systems undergoing OEE from hypothesized underlying mechanisms that explain why a system exhibits those hallmarks. We summarize the different hallmarks and mechanisms discussed during the workshop, and list the specific systems that were highlighted with respect to particular hallmarks and mechanisms. We conclude by identifying some of the most important open research questions about OEE that are apparent in light of the discussions. The York workshop provides a foundation for a follow-up OEE2 workshop taking place at the ALIFE XV conference in Cancún, Mexico, in July 2016. Additional materials from the York workshop, including talk abstracts, presentation slides, and videos of each talk, are available at http://alife.org/ws/oee1.},
  eventtitle = {Artificial {{Life}}},
  keywords = {adaptive evolution,dynamical hierarchies,major transitions,ongoing evolution,Open-ended evolution,perpetual novelty},
  file = {/Users/hugo/Papers/pdf/taylorOpenEndedEvolutionPerspectives2016.pdf}
}

@misc{taylorRequirementsOpenEndedEvolution2015,
  title = {Requirements for {{Open-Ended Evolution}} in {{Natural}} and {{Artificial Systems}}},
  author = {Taylor, Tim},
  date = {2015-07-27},
  number = {arXiv:1507.07403},
  eprint = {1507.07403},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1507.07403},
  url = {http://arxiv.org/abs/1507.07403},
  urldate = {2022-11-09},
  abstract = {Open-ended evolutionary dynamics remains an elusive goal for artificial evolutionary systems. Many ideas exist in the biological literature beyond the basic Darwinian requirements of variation, differential reproduction and inheritance. I argue that these ideas can be seen as aspects of five fundamental requirements for open-ended evolution: (1) robustly reproductive individuals, (2) a medium allowing the possible existence of a practically unlimited diversity of individuals and interactions, (3) individuals capable of producing more complex offspring, (4) mutational pathways to other viable individuals, and (5) drive for continued evolution. I briefly discuss implications of this view for the design of artificial systems with greater evolutionary potential.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Populations and Evolution},
  file = {/Users/hugo/Papers/pdf/taylorRequirementsOpenEndedEvolution2015.pdf;/Users/hugo/Zotero/storage/96EGESLH/1507.html}
}

@article{taylorTransferLearningInterTask2007,
  title = {Transfer {{Learning}} via {{Inter-Task Mappings}} for {{Temporal Difference Learning}}},
  author = {Taylor, Matthew E. and Stone, Peter and Liu, Yaxin},
  date = {2007},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {8},
  pages = {2125--2167},
  url = {http://dl.acm.org/citation.cfm?id=1314569},
  urldate = {2022-04-20}
}

@article{taylorTransferLearningReinforcement2009,
  title = {Transfer {{Learning}} for {{Reinforcement Learning Domains}}: {{A Survey}}},
  shorttitle = {Transfer {{Learning}} for {{Reinforcement Learning Domains}}},
  author = {Taylor, Matthew E. and Stone, Peter},
  date = {2009},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {10},
  pages = {1633--1685},
  url = {https://dl.acm.org/citation.cfm?id=1755839},
  urldate = {2022-04-20}
}

@unpublished{tayTransformerMemoryDifferentiable2022,
  title = {Transformer {{Memory}} as a {{Differentiable Search Index}}},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  date = {2022-02-16},
  eprint = {2202.06991},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.06991},
  urldate = {2022-02-17},
  abstract = {In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/tayTransformerMemoryDifferentiable2022.pdf}
}

@misc{thoppilanLaMDALanguageModels2022,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  date = {2022-02-10},
  number = {arXiv:2201.08239},
  eprint = {2201.08239},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.08239},
  urldate = {2022-07-26},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/thoppilanLaMDALanguageModels2022.pdf;/Users/hugo/Zotero/storage/E9N3ZNCC/2201.html}
}

@incollection{thrunLifelongLearningAlgorithms1998,
  title = {Lifelong Learning Algorithms},
  booktitle = {Learning to Learn},
  author = {Thrun, Sebastian},
  date = {1998},
  pages = {181--209},
  publisher = {{Springer}},
  file = {/Users/hugo/Zotero/storage/3GKVTX8N/978-1-4615-5529-2_8.html}
}

@article{toffoliEntropyHonest2016,
  title = {Entropy? {{Honest}}!},
  shorttitle = {Entropy?},
  author = {Toffoli, Tommaso},
  date = {2016-06-30},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {18},
  number = {7},
  eprint = {1705.02223},
  eprinttype = {arxiv},
  pages = {247},
  issn = {1099-4300},
  doi = {10.3390/e18070247},
  url = {http://arxiv.org/abs/1705.02223},
  urldate = {2021-11-30},
  abstract = {Here we deconstruct, and then in a reasoned way reconstruct, the concept of "entropy of a system," paying particular attention to where the randomness may be coming from. We start with the core concept of entropy as a COUNT associated with a DESCRIPTION; this count (traditionally expressed in logarithmic form for a number of good reasons) is in essence the number of possibilities---specific instances or "scenarios," that MATCH that description. Very natural (and virtually inescapable) generalizations of the idea of description are the probability distribution and of its quantum mechanical counterpart, the density operator. We track the process of dynamically updating entropy as a system evolves. Three factors may cause entropy to change: (1) the system's INTERNAL DYNAMICS; (2) unsolicited EXTERNAL INFLUENCES on it; and (3) the approximations one has to make when one tries to predict the system's future state. The latter task is usually hampered by hard-to-quantify aspects of the original description, limited data storage and processing resource, and possibly algorithmic inadequacy. Factors 2 and 3 introduce randomness into one's predictions and accordingly degrade them. When forecasting, as long as the entropy bookkeping is conducted in an HONEST fashion, this degradation will ALWAYS lead to an entropy increase. To clarify the above point we introduce the notion of HONEST ENTROPY, which coalesces much of what is of course already done, often tacitly, in responsible entropy-bookkeping practice. This notion, we believe, will help to fill an expressivity gap in scientific discourse. With its help we shall prove that ANY dynamical system---not just our physical universe---strictly obeys Clausius's original formulation of the second law of thermodynamics IF AND ONLY IF it is invertible. Thus this law is a TAUTOLOGICAL PROPERTY of invertible systems!},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {82-02,Physics - General Physics,Physics - History and Philosophy of Physics},
  file = {/Users/hugo/Zotero/storage/CXYEED5J/Toffoli - 2016 - Entropy Honest!.pdf}
}

@inproceedings{tongReservoirComputingUntrained2018,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Tong, Zhiqiang and Tanaka, Gouhei},
  date = {2018-08},
  pages = {1289--1294},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2018.8545471},
  abstract = {Reservoir computing has attracted much attention for its easy training process as well as its ability to deal with temporal data. A reservoir computing system consists of a reservoir part represented as a sparsely connected recurrent neural network and a readout part represented as a simple regression model. In machine learning tasks, the reservoir part is fixed and only the readout part is trained. Although reservoir computing has been mainly applied to time series prediction and recognition, it can be applied to image recognition as well by considering an image data as a sequence of pixel values. However, to achieve a high performance in image recognition with raw image data, a large-scale reservoir including a large number of neurons is required. This is a bottleneck in terms of computer memory and computational cost. To overcome this bottleneck, we propose a new method which combines reservoir computing with untrained convolutional neural networks. We use an untrained convolutional neural network to transform raw image data into a set of smaller feature maps in a preprocessing step of the reservoir computing. We demonstrate that our method achieves a high classification accuracy in an image recognition task with a much smaller number of trainable parameters compared with a previous study.},
  eventtitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  keywords = {Computational modeling,Convolution,Convolutional neural networks,Feature extraction,feedforward neural nets,image recognition,Image recognition,image recognition task,large-scale reservoir,learning (artificial intelligence),machine learning tasks,raw image data,readout part,recurrent neural nets,regression analysis,reservoir computing system,reservoir part,Reservoirs,sparsely connected recurrent neural network,time series,Training,untrained convolutional neural network},
  file = {/Users/hugo/Zotero/storage/TAFFV2L2/8545471.html}
}

@inproceedings{toutanovaRepresentingTextJoint2015,
  title = {Representing {{Text}} for {{Joint Embedding}} of {{Text}} and {{Knowledge Bases}}},
  author = {Toutanova, Kristina and Chen, Danqi and Pantel, Patrick and Poon, Hoifung and Choudhury, Pallavi and Gamon, Michael},
  date = {2015},
  pages = {1499--1509},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/D15-1174},
  url = {http://aclweb.org/anthology/D15-1174},
  urldate = {2018-04-13},
  abstract = {Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/3VVR9FQZ/toutanovaRepresentingTextJoint2015.pdf}
}

@unpublished{traskNeuralArithmeticLogic2018,
  title = {Neural {{Arithmetic Logic Units}}},
  author = {Trask, Andrew and Hill, Felix and Reed, Scott and Rae, Jack and Dyer, Chris and Blunsom, Phil},
  date = {2018-08-01},
  eprint = {1808.00508},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1808.00508},
  urldate = {2021-05-04},
  abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/traskNeuralArithmeticLogic2018.pdf}
}

@unpublished{trouillonComplexEmbeddingsSimple2016,
  title = {Complex {{Embeddings}} for {{Simple Link Prediction}}},
  author = {Trouillon, Théo and Welbl, Johannes and Riedel, Sebastian and Gaussier, Éric and Bouchard, Guillaume},
  date = {2016-06-20},
  eprint = {1606.06357},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1606.06357},
  urldate = {2018-04-16},
  abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/trouillonComplexEmbeddingsSimple22.pdf;/Users/hugo/Zotero/storage/2ISLMM8K/1606.html}
}

@article{tsalidesThreedimensionalCellularAutomata1989,
  title = {Three-Dimensional Cellular Automata and {{VLSI}} Applications},
  author = {Tsalides, Ph and Hicks, P. J. and York, T. A.},
  date = {1989-11-01},
  journaltitle = {IEE Proceedings E (Computers and Digital Techniques)},
  volume = {136},
  number = {6},
  pages = {490--495},
  publisher = {{IET Digital Library}},
  issn = {2053-7948},
  doi = {10.1049/ip-e.1989.0067},
  url = {https://digital-library.theiet.org/content/journals/10.1049/ip-e.1989.0067},
  urldate = {2022-11-07},
  abstract = {Finite, three-dimensional (3-D), N×(N×N) cellular automata with null boundary conditions are presented and discussed. It is shown that, depending on their local rule and the dimension N, these cellular automata exhibit group or semigroup algebraic structures similar to those in the one and two-dimensional (2-D) cases. The algebraic properties of these 3-D cellular automata are exploited in the implementation of integer modulo arithmetic units. Lower bounds on area A, time T, energy AT and AT2 complexity metrics of 3-D cellular automata-based modulo arithmetic units are also presented.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/9D2ZPYQB/ip-e.1989.html}
}

@article{tsertsvadzeStochasticAutomataProblem1964,
  title = {Stochastic Automata and the Problem of Constructing Reliable Automata from Unreliable Elements. i({{Reliable}} Finite Automata Design from Unreliable Elements, Using Stochastic Automaton Model)},
  author = {Tsertsvadze, G. N.},
  date = {1964},
  journaltitle = {Automation and Remote Control},
  volume = {25},
  pages = {198--210}
}

@article{turingChemicalBasisMorphogenesis1952,
  title = {The {{Chemical Basis}} of {{Morphogenesis}}},
  author = {Turing, A. M.},
  date = {1952},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
  volume = {237},
  number = {641},
  eprint = {92463},
  eprinttype = {jstor},
  pages = {37--72},
  publisher = {{The Royal Society}},
  issn = {0080-4622},
  abstract = {It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.},
  file = {/Users/hugo/Papers/pdf/turingChemicalBasisMorphogenesis1952.pdf}
}

@article{tutumAdaptingUnseenEnvironments2020,
  title = {Adapting to {{Unseen Environments}} through {{Explicit Representation}} of {{Context}}},
  author = {Tutum, Cem and Miikkulainen, Risto},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {581--588},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00313},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00313},
  urldate = {2020-07-27},
  abstract = {In order to deploy autonomous agents to domains such as autonomous driving, infrastructure management, health care, and finance, they must be able to adapt safely to unseen situations. The current approach in constructing such agents is to try to include as much variation into training as possible, and then generalize within the possible variations. This paper proposes a principled approach where a context module is coevolved with a skill module. The context module recognizes the variation and modulates the skill module so that the entire system performs well in unseen situations. The approach is evaluated in a challenging version of the Flappy Bird game where the effects of the actions vary over time. The Context+Skill approach leads to significantly more robust behavior in environments with previously unseen effects. Such a principled generalization ability is essential in deploying autonomous agents in real world tasks, and can serve as a foundation for continual learning as well.},
  file = {/Users/hugo/Papers/pdf/tutumAdaptingUnseenEnvironments2020.pdf;/Users/hugo/Zotero/storage/A3IQEXUK/isal_a_00313.html}
}

@inproceedings{ulamMathematicalProblemsConnected1962,
  title = {On Some Mathematical Problems Connected with Patterns of Growth of Figures},
  booktitle = {Proceedings of {{Symposia}} in {{Applied Mathematics}}},
  author = {Ulam, Stanislaw},
  date = {1962},
  volume = {14},
  pages = {215--224},
  publisher = {{Am. Math. Soc. Vol. 14, Providence}},
  file = {/Users/hugo/Zotero/storage/HSM72C8W/books.html}
}

@article{universidaddelcaucapopayancolombiaBehaviorClassificationTuring2017,
  title = {Behavior {{Classification}} for {{Turing Machines}}},
  author = {{Universidad del Cauca, Popayán, Colombia} and Diaz, Nestor},
  date = {2017-09-15},
  journaltitle = {Complex Systems},
  volume = {26},
  number = {3},
  pages = {283--294},
  issn = {08912513},
  doi = {10.25088/ComplexSystems.26.3.283},
  url = {http://www.complex-systems.com/abstracts/v26_i03_a04.html},
  urldate = {2019-09-04},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/SVEMIJZD/universidaddelcaucapopayancolombiaBehaviorClassificationTuring2017.pdf}
}

@unpublished{vahdatNVAEDeepHierarchical2020,
  title = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  shorttitle = {{{NVAE}}},
  author = {Vahdat, Arash and Kautz, Jan},
  date = {2020-07-08},
  eprint = {2007.03898},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.03898},
  urldate = {2020-07-13},
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\$\textbackslash times\$256 pixels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/vahdatNVAEDeepHierarchical2020.pdf;/Users/hugo/Zotero/storage/BEMLXVYG/2007.html}
}

@article{vakhrameevMeasuringAutonomyLifeLike2020,
  title = {Measuring {{Autonomy}} for {{Life-Like AI}}},
  author = {Vakhrameev, Demyan and Aguilera, Miguel and Barandiaran, Xabier E. and Bedia, Manuel},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {589--591},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00308},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00308},
  urldate = {2020-07-27},
  abstract = {Current success of Artificial Intelligence (particularly in the application of Deep Learning techniques) is bringing some of its methods closer to Artificial Life and re-opening old questions, social fears and envisioned applications. The concept of autonomy has long guided research and progress in Artificial Life. We explore how this concept can contribute to evaluate the autonomy of contemporary AI systems.},
  file = {/Users/hugo/Papers/pdf/vakhrameevMeasuringAutonomyLifeLike2020.pdf;/Users/hugo/Zotero/storage/4MG5HGHT/isal_a_00308.html}
}

@article{vandermaatenVisualizingDataUsing2008,
  ids = {maatenVisualizingDataUsing2008},
  title = {Visualizing Data Using T-{{SNE}}.},
  author = {Van der Maaten, Laurens and Hinton, Geoffrey},
  date = {2008},
  journaltitle = {Journal of machine learning research},
  volume = {9},
  number = {11},
  file = {/Users/hugo/Papers/pdf/vandermaatenVisualizingDataUsing2008.pdf;/Users/hugo/Zotero/storage/HFNB86J7/vandermaaten08a.html}
}

@incollection{varelaAutopoiesisOrganizationLiving1991,
  title = {Autopoiesis: {{The}} Organization of Living Systems, Its Characterization and a Model},
  shorttitle = {Autopoiesis},
  booktitle = {Facets of Systems Science},
  author = {Varela, Francisco G. and Maturana, Humberto R. and Uribe, Ricardo},
  date = {1991},
  pages = {559--569},
  publisher = {{Springer}},
  file = {/Users/hugo/Zotero/storage/Z5BU2Z7D/978-1-4899-0718-9_40.html}
}

@book{varelaPracticeAutonomousSystems1992,
  title = {Toward a {{Practice}} of {{Autonomous Systems}}: {{Proceedings}} of the {{First European Conference}} on {{Artificial Life}}},
  shorttitle = {Toward a {{Practice}} of {{Autonomous Systems}}},
  author = {Varela, Francisco J. and Bourgine, Paul},
  date = {1992-04-02},
  eprint = {pWsNJkdZ4tgC},
  eprinttype = {googlebooks},
  publisher = {{MIT Press}},
  abstract = {Artificial life embodies a recent and important conceptual step in modem science: asserting that the core of intelligence and cognitive abilities is the same as the capacity for living. The recent surge of interest in artificial life has pushed a whole range of engineering traditions, such as control theory and robotics, beyond classical notions of goal and planning into biologically inspired notions of viability and adaptation, situatedness and operational closure. These proceedings serve two important functions: they address bottom-up theories of artificial intelligence and explore what can be learned from simple models such as insects about the cognitive processes and characteristic autonomy of living organisms, while also engaging researchers and philosophers in an exciting examination of the epistemological basis of this new trend.TopicsArtificial Animals • Genetic Algorithms • Autonomous Systems • Emergent Behaviors • Artificial Ecologies • Immunologic Algorithms • Self-Adapting Systems • Emergent Structures • Emotion And Motivation • Neural Networks • Coevolution • Fitness Landscapes ContributorsH. Bersini, Domenico Parisi, Rodney A. Brooks, Christopher G. Langton, S. Kauffman, J.-L. Denenbourg, Pattie Maes, John Holland, T. Smithersm H. Swefel, H. Muhlenbein},
  isbn = {978-0-262-72019-9},
  langid = {english},
  pagetotal = {546},
  keywords = {Psychology / Cognitive Psychology & Cognition}
}

@unpublished{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-04-11},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/vaswaniAttentionAllYou2017.pdf;/Users/hugo/Zotero/storage/CXLW8VZD/1706.html}
}

@article{vecovenIntroducingNeuromodulationDeep2020,
  title = {Introducing Neuromodulation in Deep Neural Networks to Learn Adaptive Behaviours},
  author = {Vecoven, Nicolas and Ernst, Damien and Wehenkel, Antoine and Drion, Guillaume},
  date = {2020-01-27},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {15},
  number = {1},
  pages = {e0227922},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0227922},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0227922},
  urldate = {2020-06-15},
  abstract = {Animals excel at adapting their intentions, attention, and actions to the environment, making them remarkably efficient at interacting with a rich, unpredictable and ever-changing external world, a property that intelligent machines currently lack. Such an adaptation property relies heavily on cellular neuromodulation, the biological mechanism that dynamically controls intrinsic properties of neurons and their response to external stimuli in a context-dependent manner. In this paper, we take inspiration from cellular neuromodulation to construct a new deep neural network architecture that is specifically designed to learn adaptive behaviours. The network adaptation capabilities are tested on navigation benchmarks in a meta-reinforcement learning context and compared with state-of-the-art approaches. Results show that neuromodulation is capable of adapting an agent to different tasks and that neuromodulation-based approaches provide a promising way of improving adaptation of artificial systems.},
  langid = {english},
  keywords = {Convergent evolution,Evolutionary adaptation,Learning,Machine learning algorithms,Neural networks,Neuromodulation,Neuronal tuning,Neurons},
  file = {/Users/hugo/Papers/pdf/vecovenIntroducingNeuromodulationDeep2020.pdf;/Users/hugo/Zotero/storage/AGSBVL2B/article.html}
}

@article{veenstraHowDifferentEncodings2020,
  title = {How {{Different Encodings Affect Performance}} and {{Diversification}} When {{Evolving}} the {{Morphology}} and {{Control}} of {{2D Virtual Creatures}}},
  author = {Veenstra, Frank and Glette, Kyrre},
  date = {2020-07-01},
  journaltitle = {Artificial Life Conference Proceedings},
  shortjournal = {Artificial Life Conference Proceedings},
  volume = {32},
  pages = {592--601},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00295},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/isal_a_00295},
  urldate = {2020-07-27},
  abstract = {A challenge in evolutionary robotics is the in parallel adaptation of morphologies and controllers. Here, we considered encoding methods for morphogenesis of 2D virtual creatures that can be created from directed trees. Using an evolutionary algorithm, we optimized locomotion in these virtual creatures and compared a direct encoding, an L-System, and two types of encodings that produce neural networks—a Compositional Pattern Producing Network (CPPN) and a Cellular Encoding (CE). We evaluated these encodings based on performance and diversification, and we introduced an OpenAI gym environment as a computationally inexpensive benchmark for exploring morphological evolution. The direct encoding and L-System generated more fit solutions compared to the network strategies. Considering morphological diversity, the direct encoding finds solutions more locally in the morphological search space, the L-System made larger jumps across this search space, and both network approaches also make larger jumps though find fewer solutions in this space. With these results we show how encodings exhibit different characteristics as developmental approaches. Since the genotype-phenotype mapping plays a major role in evolutionary robotics, further modifications using more complex tasks and environments can lead to a better understanding of morphogenesis and thereby improve how morphologies and controllers of robots are evolved.},
  file = {/Users/hugo/Papers/pdf/veenstraHowDifferentEncodings2020.pdf;/Users/hugo/Zotero/storage/XBSVSW3C/isal_a_00295.html}
}

@unpublished{venessGatedLinearNetworks2020,
  title = {Gated {{Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Budden, David and Bhoopchand, Avishkar and Mattern, Christopher and Grabska-Barwinska, Agnieszka and Sezener, Eren and Wang, Jianan and Toth, Peter and Schmitt, Simon and Hutter, Marcus},
  date = {2020-06-11},
  eprint = {1910.01526},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1910.01526},
  urldate = {2020-06-16},
  abstract = {This paper presents a new family of backpropagation-free neural architectures, Gated Linear Networks (GLNs). What distinguishes GLNs from contemporary neural networks is the distributed and local nature of their credit assignment mechanism; each neuron directly predicts the target, forgoing the ability to learn feature representations in favor of rapid online learning. Individual neurons can model nonlinear functions via the use of data-dependent gating in conjunction with online convex optimization. We show that this architecture gives rise to universal learning capabilities in the limit, with effective model capacity increasing as a function of network size in a manner comparable with deep ReLU networks. Furthermore, we demonstrate that the GLN learning mechanism possesses extraordinary resilience to catastrophic forgetting, performing comparably to a MLP with dropout and Elastic Weight Consolidation on standard benchmarks. These desirable theoretical and empirical properties position GLNs as a complementary technique to contemporary offline deep learning methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/venessGatedLinearNetworks2020.pdf}
}

@unpublished{veniatEfficientContinualLearning2021,
  title = {Efficient {{Continual Learning}} with {{Modular Networks}} and {{Task-Driven Priors}}},
  author = {Veniat, Tom and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  date = {2021-02-12},
  eprint = {2012.12631},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2012.12631},
  urldate = {2021-10-15},
  abstract = {Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/veniatEfficientContinualLearning2021.pdf}
}

@article{verdegemDismantlingAICapitalism2022,
  title = {Dismantling {{AI}} Capitalism: The Commons as an Alternative to the Power Concentration of {{Big Tech}}},
  shorttitle = {Dismantling {{AI}} Capitalism},
  author = {Verdegem, Pieter},
  date = {2022-04-09},
  journaltitle = {AI \& SOCIETY},
  shortjournal = {AI \& Soc},
  issn = {1435-5655},
  doi = {10.1007/s00146-022-01437-8},
  url = {https://doi.org/10.1007/s00146-022-01437-8},
  urldate = {2022-05-02},
  abstract = {This article discusses the political economy of AI capitalism. It considers AI as a General Purpose Technology (GPT) and argues we need to investigate the power concentration of Big Tech. AI capitalism is characterised by the commodification of data, data extraction and a concentration in hiring of AI talent and compute capacity. This is behind Big Tech’s unstoppable drive for growth, which leads to monopolisation and enclosure under the winner takes all principle. If we consider AI as a GPT—technologies that alter society’s economic and social structures—we need to come up with alternatives in terms of ownership and governance. The commons is proposed as an alternative for thinking about how to organise AI development and how to distribute the value that can be derived from it. Using the commons framework is also a way of giving society a more prominent role in the debate about what we expect from AI and how we should approach it.},
  langid = {english},
  keywords = {AI capitalism,Artificial Intelligence (AI),Commodification,Commons,Extraction,Political economy},
  file = {/Users/hugo/Papers/pdf/verdegemDismantlingAICapitalism2022.pdf}
}

@article{vettelschossSelforganizedDynamicAttractors2020,
  title = {Self-Organized Dynamic Attractors in Recurrent Neural Networks},
  author = {Vettelschoss, Benedikt and Freiberger, Matthias and Dambre, Joni},
  date = {2020},
  journaltitle = {Computational Intelligence},
  pages = {6},
  abstract = {Recurrent neural networks usually rely on either transient or attractor dynamics to implement working memory, and some studies suggest that it requires a combination of the two. These studies introduce attractor states by the supervised training of a network’s feedback weights. In this work we report the creation of comparable memory states through unsupervised learning. We introduce attractor dynamics into an echo state network in a self-organized way by applying a differential Hebbian rule to it’s feedback weights. We find that this yields periodic and quasiperiodic attractors in most cases. We analyse the linearized system after the learning phase to understand the origin of these attractors, and connect these findings to other results concerning the dynamical changes induced by neural plasticity.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/vettelschossSelforganizedDynamicAttractors2020.pdf}
}

@article{vichniacSimulatingPhysicsCellular1984,
  title = {Simulating Physics with Cellular Automata},
  author = {Vichniac, Gérard Y.},
  date = {1984-01-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {10},
  number = {1},
  pages = {96--116},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(84)90253-7},
  url = {https://www.sciencedirect.com/science/article/pii/0167278984902537},
  urldate = {2022-05-02},
  abstract = {Cellular automata are dynamical systems where space, time, and variables are discrete. They are shown on two-dimensional examples to be capable of non-numerical simulations of physics. They are useful for faithful parallel processing of lattice models. At another level, they exhibit behaviours and illustrate concepts that are unmistakably physical, such as non-ergodicity and order parameters, frustration, relaxation to chaos through period doublings, a conspicuous arrow of time in reversible microscopic dynamics, causality and light-cone, and non-separability. In general, they constitute exactly computable models for complex phenomena and large-scale correlations that result from very simple short-range interactions. We study their space, time, and intrinsic symmetries and the corresponding conservation laws, with an emphasis on the conservation of information obeyed by reversible cellular automata.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/vichniacSimulatingPhysicsCellular1984.pdf;/Users/hugo/Zotero/storage/TILGG5FX/0167278984902537.html}
}

@inproceedings{vinyalsMatchingNetworksOne2016,
  title = {Matching {{Networks}} for {{One Shot Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29: {{Annual Conference}} on {{Neural Information Processing Systems}} 2016, {{December}} 5-10, 2016, {{Barcelona}}, {{Spain}}},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Tim and Kavukcuoglu, Koray and Wierstra, Daan},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and von Luxburg, Ulrike and Guyon, Isabelle and Garnett, Roman},
  date = {2016},
  pages = {3630--3638},
  url = {https://proceedings.neurips.cc/paper/2016/hash/90e1357833654983612fb05e3ec9148c-Abstract.html},
  urldate = {2022-04-19}
}

@inproceedings{virgolinScalableGeneticProgramming2017,
  title = {Scalable Genetic Programming by Gene-Pool Optimal Mixing and Input-Space Entropy-Based Building-Block Learning},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on - {{GECCO}} '17},
  author = {Virgolin, Marco and Alderliesten, Tanja and Witteveen, Cees and Bosman, Peter A. N.},
  date = {2017},
  pages = {1041--1048},
  publisher = {{ACM Press}},
  location = {{Berlin, Germany}},
  doi = {10.1145/3071178.3071287},
  url = {http://dl.acm.org/citation.cfm?doid=3071178.3071287},
  urldate = {2020-02-24},
  eventtitle = {The {{Genetic}} and {{Evolutionary Computation Conference}}},
  isbn = {978-1-4503-4920-8},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/virgolinScalableGeneticProgramming2017.pdf}
}

@incollection{voelkerLegendreMemoryUnits2019,
  title = {Legendre {{Memory Units}}: {{Continuous-Time Representation}} in {{Recurrent Neural Networks}}},
  shorttitle = {Legendre {{Memory Units}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Voelker, Aaron and Kajić, Ivana and Eliasmith, Chris},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textquotesingle Alché-Buc, F. and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {15544--15553},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf},
  urldate = {2019-12-11},
  file = {/Users/hugo/Zotero/storage/MKJ98975/voelkerLegendreMemoryUnits2019.pdf}
}

@unpublished{voitaInformationTheoreticProbingMinimum2020,
  title = {Information-{{Theoretic Probing}} with {{Minimum Description Length}}},
  author = {Voita, Elena and Titov, Ivan},
  date = {2020-03-27},
  eprint = {2003.12298},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.12298},
  urldate = {2020-07-28},
  abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/voitaInformationTheoreticProbingMinimum2020.pdf}
}

@article{vonneumannTheorySelfreproducingAutomata1966,
  title = {Theory of Self-Reproducing Automata},
  author = {Von Neumann, John and Burks, Arthur W.},
  date = {1966},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {1},
  pages = {3--14},
  url = {https://cba.mit.edu/events/03.11.ASE/docs/VonNeumann.pdf}
}

@article{vyuginAlgorithmicComplexityStochastic1999,
  title = {Algorithmic {{Complexity}} and {{Stochastic Properties}} of {{Finite Binary Sequences}}},
  author = {V'yugin, V. V.},
  date = {1999-04-01},
  journaltitle = {The Computer Journal},
  volume = {42},
  number = {4},
  pages = {294--317},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/42.4.294},
  url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/42.4.294},
  urldate = {2020-03-07},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/vyuginAlgorithmicComplexityStochastic1999.pdf}
}

@unpublished{wangEnhancedPOETOpenEnded2020,
  title = {Enhanced {{POET}}: {{Open-Ended Reinforcement Learning}} through {{Unbounded Invention}} of {{Learning Challenges}} and Their {{Solutions}}},
  shorttitle = {Enhanced {{POET}}},
  author = {Wang, Rui and Lehman, Joel and Rawal, Aditya and Zhi, Jiale and Li, Yulun and Clune, Jeff and Stanley, Kenneth O.},
  date = {2020-04-13},
  eprint = {2003.08536},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.08536},
  urldate = {2020-06-09},
  abstract = {Creating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/wangEnhancedPOETOpenEnded2020.pdf}
}

@article{wangGeneralizingFewExamples2020,
  title = {Generalizing from a Few Examples: {{A}} Survey on Few-Shot Learning},
  shorttitle = {Generalizing from a Few Examples},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  date = {2020},
  journaltitle = {ACM computing surveys (csur)},
  volume = {53},
  number = {3},
  pages = {1--34},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/hugo/Papers/pdf/wangGeneralizingFewExamples2020.pdf;/Users/hugo/Zotero/storage/2AFHB2F3/3386252.html}
}

@inproceedings{wangKnowledgeBaseCompletion2016,
  title = {Knowledge {{Base Completion}} via {{Coupled Path Ranking}}},
  author = {Wang, Quan and Liu, Jing and Luo, Yuanfei and Wang, Bin and Lin, Chin-Yew},
  date = {2016},
  pages = {1308--1318},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P16-1124},
  url = {http://aclweb.org/anthology/P16-1124},
  urldate = {2018-04-17},
  abstract = {Knowledge bases (KBs) are often greatly incomplete, necessitating a demand for KB completion. The path ranking algorithm (PRA) is one of the most promising approaches to this task. Previous work on PRA usually follows a single-task learning paradigm, building a prediction model for each relation independently with its own training data. It ignores meaningful associations among certain relations, and might not get enough training data for less frequent relations. This paper proposes a novel multi-task learning framework for PRA, referred to as coupled PRA (CPRA). It first devises an agglomerative clustering strategy to automatically discover relations that are highly correlated to each other, and then employs a multi-task learning strategy to effectively couple the prediction of such relations. As such, CPRA takes into account relation association and enables implicit data sharing among them. We empirically evaluate CPRA on benchmark data created from Freebase. Experimental results show that CPRA can effectively identify coherent clusters in which relations are highly correlated. By further coupling such relations, CPRA significantly outperforms PRA, in terms of both predictive accuracy and model interpretability.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/wangKnowledgeBaseCompletion22.pdf}
}

@book{wangKnowledgeGraphEmbedding2017,
  title = {Knowledge Graph Embedding: {{A}} Survey of Approaches and Applications},
  author = {Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
  date = {2017},
  volume = {29},
  number = {12},
  doi = {10.1109/TKDE.2017.2754499},
  abstract = {—Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.}
}

@inproceedings{wangKnowledgeGraphText2014,
  title = {Knowledge {{Graph}} and {{Text Jointly Embedding}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  date = {2014-10},
  pages = {1591--1601},
  publisher = {{Association for Computational Linguistics}},
  location = {{Doha, Qatar}},
  url = {http://www.aclweb.org/anthology/D14-1167},
  urldate = {2018-04-12},
  file = {/Users/hugo/Papers/pdf/wangKnowledgeGraphText22.pdf}
}

@unpublished{wangLearningReinforcementLearn2017,
  title = {Learning to Reinforcement Learn},
  author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  date = {2017-01-23},
  eprint = {1611.05763},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.05763},
  urldate = {2020-01-28},
  abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Zotero/storage/YSTR8W2E/wangLearningReinforcementLearn2017.pdf}
}

@inproceedings{wangPOETOpenendedCoevolution2019,
  title = {{{POET}}: Open-Ended Coevolution of Environments and Their Optimized Solutions},
  shorttitle = {{{POET}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}}},
  author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
  date = {2019-07-13},
  series = {{{GECCO}} '19},
  pages = {142--151},
  publisher = {{Association for Computing Machinery}},
  location = {{Prague, Czech Republic}},
  doi = {10.1145/3321707.3321799},
  url = {https://doi.org/10.1145/3321707.3321799},
  urldate = {2020-01-31},
  abstract = {How can progress in machine learning and reinforcement learning be automated to generate its own never-ending curriculum of challenges without human intervention? The recent emergence of quality diversity (QD) algorithms offers a glimpse of the potential for such continual open-ended invention. For example, novelty search showcases the benefits of explicit novelty pressure, MAP-Elites and Innovation Engines highlight the advantage of explicit elitism within niches in an otherwise divergent process, and minimal criterion coevolution (MCC) reveals that problems and solutions can coevolve divergently. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper combines these principles to produce a practical approach to generating an endless progression of diverse and increasingly challenging environments while at the same time explicitly optimizing their solutions. An intriguing implication is the opportunity to transfer solutions among environments, reflecting the view that innovation is a circuitous and unpredictable process. POET is tested in a 2-D obstacles course domain, where it generates diverse and sophisticated behaviors that create and solve a wide range of environmental challenges, many of which cannot be solved by direct optimization, or by a direct-path curriculum-building control algorithm. We hope that POET will inspire a new push towards open-ended discovery across many domains.},
  isbn = {978-1-4503-6111-8},
  keywords = {artificial life,coevolution,evolution strategies,novelty search,open-ended evolution},
  file = {/Users/hugo/Zotero/storage/FPIS585G/wangPOETOpenendedCoevolution2019.pdf}
}

@inproceedings{wangSemAttackNaturalTextual2022,
  title = {{{SemAttack}}: {{Natural Textual Attacks}} via {{Different Semantic Spaces}}},
  shorttitle = {{{SemAttack}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022, {{Seattle}}, {{WA}}, {{United States}}, {{July}} 10-15, 2022},
  author = {Wang, Boxin and Xu, Chejian and Liu, Xiangyu and Cheng, Yu and Li, Bo},
  editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Ruíz, Iván Vladimir Meza},
  date = {2022},
  pages = {176--205},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2022.findings-naacl.14},
  file = {/Users/hugo/Papers/pdf/wangSemAttackNaturalTextual2022.pdf}
}

@inproceedings{wangT3TreeAutoencoderConstrained2020,
  title = {T3: {{Tree-Autoencoder Constrained Adversarial Text Generation}} for {{Targeted Attack}}},
  shorttitle = {T3},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}, {{EMNLP}} 2020, {{Online}}, {{November}} 16-20, 2020},
  author = {Wang, Boxin and Pei, Hengzhi and Pan, Boyuan and Chen, Qian and Wang, Shuohang and Li, Bo},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  date = {2020},
  pages = {6134--6150},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2020.emnlp-main.495},
  file = {/Users/hugo/Papers/pdf/wangT3TreeAutoencoderConstrained2020.pdf}
}

@article{washingtonBenfordLawFibonacci1981,
  title = {Benford's {{Law}} for {{Fibonacci}} and {{Lucas Numbers}}},
  author = {Washington, L. C.},
  date = {1981},
  journaltitle = {The Fibonacci Quarterly},
  volume = {2},
  number = {19},
  pages = {175--177}
}

@article{weinanProposalMachineLearning2017,
  title = {A {{Proposal}} on {{Machine Learning}} via {{Dynamical Systems}}},
  author = {Weinan, E},
  date = {2017-03},
  journaltitle = {Communications in Mathematics and Statistics},
  shortjournal = {Commun. Math. Stat.},
  volume = {5},
  number = {1},
  pages = {1--11},
  issn = {2194-6701, 2194-671X},
  doi = {10.1007/s40304-017-0103-z},
  url = {http://link.springer.com/10.1007/s40304-017-0103-z},
  urldate = {2021-09-29},
  abstract = {We discuss the idea of using continuous dynamical systems to model general high-dimensional nonlinear functions used in machine learning. We also discuss the connection with deep learning.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/weinanProposalMachineLearning2017.pdf}
}

@inproceedings{weissProgrammingBiologicalCells1998,
  title = {Programming {{Biological Cells}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Weiss, Ron and Homsy, George and Nagpal, Radhika},
  date = {1998},
  pages = {1},
  eventtitle = {International {{Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/weissProgrammingBiologicalCells1998.pdf}
}

@unpublished{weissThinkingTransformers2021,
  title = {Thinking {{Like Transformers}}},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  date = {2021-06-13},
  eprint = {2106.06981},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.06981},
  urldate = {2021-06-16},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/weissThinkingTransformers2021.pdf}
}

@incollection{weissVivoDigitalCircuits2002,
  title = {Toward in Vivo {{Digital Circuits}}},
  booktitle = {Evolution as {{Computation}}},
  author = {Weiss, Ron and Homsy, George E. and Knight, Thomas F.},
  editor = {Landweber, Laura F. and Winfree, Erik},
  date = {2002},
  series = {Natural {{Computing Series}}},
  pages = {275--295},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-55606-7_14},
  url = {http://link.springer.com/10.1007/978-3-642-55606-7_14},
  urldate = {2022-11-14},
  editorb = {Rozenberg, G. and Bäck, Th. and Eiben, A. E. and Kok, J. N. and Spaink, H. P.},
  editorbtype = {redactor},
  isbn = {978-3-642-63081-1 978-3-642-55606-7},
  langid = {english}
}

@article{welchTechniqueHighPerformanceData1984,
  title = {A {{Technique}} for {{High-Performance Data Compression}}},
  author = {{Welch}},
  date = {1984-06},
  journaltitle = {Computer},
  volume = {17},
  number = {6},
  pages = {8--19},
  issn = {0018-9162},
  doi = {10.1109/MC.1984.1659158},
  url = {http://ieeexplore.ieee.org/document/1659158/},
  urldate = {2020-03-04}
}

@article{wellumEnergizingFinanceEnergy2020,
  title = {Energizing {{Finance}}: {{The Energy Crisis}}, {{Oil Futures}}, and {{Neoliberal Narratives}}},
  shorttitle = {Energizing {{Finance}}},
  author = {Wellum, Caleb},
  date = {2020-03},
  journaltitle = {Enterprise \& Society},
  shortjournal = {Enterp. Soc.},
  volume = {21},
  number = {1},
  pages = {2--37},
  issn = {1467-2227, 1467-2235},
  doi = {10.1017/eso.2019.26},
  url = {https://www.cambridge.org/core/product/identifier/S1467222719000260/type/journal_article},
  urldate = {2022-08-30},
  abstract = {This article examines the origins and development of oil futures trading in the United States to demonstrate the important role that energy concerns played in the financialization of the U.S. economy in the 1970s and 1980s. The article contextualizes the emergence of oil futures contracts by narrating the longer history of U.S. futures markets and financialization. It also explores the halting development of oil futures contracts, and analyzes the three kinds of legitimating narratives that accompanied oil futures trading: reason, the primacy of price, and power. As a whole, the article argues that energy crisis discourse contributed significantly to the financialization of the U.S. economy by framing futures markets as the only viable solution to the energy crisis. The much-celebrated oil futures contracts on the New York Mercantile Exchange supported and marked the emergent power of financial thinking as the United States entered a neoliberal era.},
  langid = {english}
}

@unpublished{wenOnlineInfluenceMaximization2016,
  title = {Online {{Influence Maximization}} under {{Independent Cascade Model}} with {{Semi-Bandit Feedback}}},
  author = {Wen, Zheng and Kveton, Branislav and Valko, Michal and Vaswani, Sharan},
  date = {2016-05-21},
  eprint = {1605.06593},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1605.06593},
  urldate = {2018-11-28},
  abstract = {We study the online influence maximization problem in social networks under the independent cascade model. Specifically, we aim to learn the set of “best influencers” in a social network online while repeatedly interacting with it. We address the challenges of (i) combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and (ii) limited feedback, since only the influenced portion of the network is observed. Under a stochastic semi-bandit feedback, we propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our bounds on the cumulative regret are polynomial in all quantities of interest, achieve near-optimal dependence on the number of interactions and reflect the topology of the network and the activation probabilities of its edges, thereby giving insights on the problem complexity. To the best of our knowledge, these are the first such results. Our experiments show that in several representative graph topologies, the regret of IMLinUCB scales as suggested by our upper bounds. IMLinUCB permits linear generalization and thus is both statistically and computationally suitable for large-scale problems. Our experiments also show that IMLinUCB with linear generalization can lead to low regret in real-world online influence maximization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/wenOnlineInfluenceMaximization22.pdf}
}

@inproceedings{westonAICompleteQuestionAnswering2016,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomás},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2016},
  url = {http://arxiv.org/abs/1502.05698},
  urldate = {2022-02-23}
}

@inproceedings{westonConnectingLanguageKnowledge2013,
  title = {Connecting {{Language}} and {{Knowledge Bases}} with {{Embedding Models}} for {{Relation Extraction}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Weston, Jason and Bordes, Antoine and Yakhnenko, Oksana and Usunier, Nicolas},
  date = {2013-10},
  pages = {1366--1371},
  publisher = {{Association for Computational Linguistics}},
  location = {{Seattle, Washington, USA}},
  url = {http://www.aclweb.org/anthology/D13-1136},
  urldate = {2018-04-12},
  file = {/Users/hugo/Papers/pdf/westonConnectingLanguageKnowledge22.pdf}
}

@article{whittingtonApproximationErrorBackpropagation2017,
  title = {An {{Approximation}} of the {{Error Backpropagation Algorithm}} in a {{Predictive Coding Network}} with {{Local Hebbian Synaptic Plasticity}}},
  author = {Whittington, James C. R. and Bogacz, Rafal},
  date = {2017-05},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {29},
  number = {5},
  pages = {1229--1262},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00949},
  url = {https://direct.mit.edu/neco/article/29/5/1229-1262/8261},
  urldate = {2021-06-14},
  abstract = {To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/TVYBXBC4/Whittington and Bogacz - 2017 - An Approximation of the Error Backpropagation Algo.pdf}
}

@inproceedings{wierstraModelingSystemsInternal2005,
  title = {Modeling Systems with Internal State Using Evolino},
  booktitle = {Proceedings of the 2005 Conference on {{Genetic}} and Evolutionary Computation  - {{GECCO}} '05},
  author = {Wierstra, Daan and Gomez, Faustino J. and Schmidhuber, Jürgen},
  date = {2005},
  pages = {1795},
  publisher = {{ACM Press}},
  location = {{Washington DC, USA}},
  doi = {10.1145/1068009.1068315},
  url = {http://portal.acm.org/citation.cfm?doid=1068009.1068315},
  urldate = {2019-12-12},
  abstract = {Existing Recurrent Neural Networks (RNNs) are limited in their ability to model dynamical systems with nonlinearities and hidden internal states. Here we use our general framework for sequence learning, EVOlution of recurrent systems with LINear Outputs (Evolino), to discover good RNN hidden node weights through evolution, while using linear regression to compute an optimal linear mapping from hidden state to output. Using the Long Short-Term Memory RNN Architecture, Evolino outperforms previous state-of-the-art methods on several tasks: 1) context-sensitive languages, 2) multiple superimposed sine waves.},
  eventtitle = {The 2005 Conference},
  isbn = {978-1-59593-010-1},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/TYCJ9SF2/wierstraModelingSystemsInternal2005.pdf}
}

@unpublished{wietingNoTrainingRequired2019,
  title = {No {{Training Required}}: {{Exploring Random Encoders}} for {{Sentence Classification}}},
  shorttitle = {No {{Training Required}}},
  author = {Wieting, John and Kiela, Douwe},
  date = {2019-01-29},
  eprint = {1901.10444},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1901.10444},
  urldate = {2020-10-08},
  abstract = {We explore various methods for computing sentence representations from pretrained word embeddings without any training, i.e., using nothing but random parameterizations. Our aim is to put sentence embeddings on more solid footing by 1) looking at how much modern sentence embeddings gain over random methods—as it turns out, surprisingly little; and by 2) providing the field with more appropriate baselines going forward—which are, as it turns out, quite strong. We also make important observations about proper experimental protocol for sentence classification evaluation, together with recommendations for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Zotero/storage/8GB4ACZ5/Wieting and Kiela - 2019 - No Training Required Exploring Random Encoders fo.pdf}
}

@inproceedings{williamsUsingNystromMethod2001,
  title = {Using the Nyström Method to Speed up Kernel Machines},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Williams, Christopher and Seeger, Matthias},
  editor = {Leen, T. and Dietterich, T. and Tresp, V.},
  date = {2001},
  volume = {13},
  pages = {682--688},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf}
}

@book{winogradReliableComputationPresence1963,
  title = {Reliable Computation in the Presence of Noise},
  author = {Winograd, Shmuel and Cowan, Jack D.},
  date = {1963},
  publisher = {{Mit Press Cambridge, Mass.}},
  file = {/Users/hugo/Papers/pdf/winogradReliableComputationPresence1963.pdf}
}

@article{wittenArithmeticCodingData1987,
  title = {Arithmetic Coding for Data Compression},
  author = {Witten, Ian H. and Neal, Radford M. and Cleary, John G.},
  date = {1987},
  journaltitle = {Communications of the ACM},
  volume = {30},
  number = {6},
  pages = {520--540},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/hugo/Papers/pdf/wittenArithmeticCodingData1987.pdf;/Users/hugo/Zotero/storage/ZTD8T673/214762.html}
}

@article{wolanskySemiDiscreteApproximationOptimal2015,
  title = {Semi-{{Discrete}} Approximation of {{Optimal Mass Transport}}},
  author = {Wolansky, Gershon},
  date = {2015-02-15},
  url = {https://arxiv.org/abs/1502.04309v1},
  urldate = {2018-11-22},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/wolanskySemiDiscreteApproximationOptimal22.pdf;/Users/hugo/Zotero/storage/XPT5A7AR/1502.html}
}

@book{wolframNewKindScience2002,
  title = {A New Kind of Science},
  author = {Wolfram, Stephen},
  date = {2002},
  publisher = {{Wolfram Media}},
  location = {{Champaign, IL}},
  url = {https://www.wolframscience.com/nks/},
  isbn = {978-1-57955-008-0},
  pagetotal = {1197},
  keywords = {Cellular automata,Computational complexity}
}

@book{wolframProjectFindFundamental2020,
  title = {A Project to Find the Fundamental Theory of Physics},
  author = {Wolfram, Stephen},
  date = {2020},
  publisher = {{Wolfram Media, Inc}},
  location = {{Champaign, IL}},
  abstract = {"The Wolfram Physics Project is a bold effort to find the fundamental theory of physics. It combines new ideas with the latest research in physics, mathematics and computation in the push to achieve this ultimate goal of science. Written with Stephen Wolfram's characteristic expository flair, this book provides a unique opportunity to learn about a historic initiative in science right as it is happening. A Project to Find the Fundamental Theory of Physics includes an accessible introduction to the project as well as core technical exposition and rich, never-before-seen visualizations"--},
  editora = {Wolfram Research (Firm)},
  editoratype = {collaborator},
  isbn = {978-1-57955-035-6},
  keywords = {Mathematical physics,Physics}
}

@article{wolframStatisticalMechanicsCellular1983,
  title = {Statistical Mechanics of Cellular Automata},
  author = {Wolfram, Stephen},
  date = {1983-07-01},
  journaltitle = {Reviews of Modern Physics},
  volume = {55},
  number = {3},
  pages = {601--644},
  issn = {0034-6861},
  doi = {10.1103/RevModPhys.55.601},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.55.601},
  urldate = {2019-05-28},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/wolframStatisticalMechanicsCellular12.pdf}
}

@article{wolframUniversalityComplexityCellular1984,
  title = {Universality and Complexity in Cellular Automata},
  author = {Wolfram, Stephen},
  date = {1984-01-01},
  journaltitle = {Physica D: Nonlinear Phenomena},
  shortjournal = {Physica D: Nonlinear Phenomena},
  volume = {10},
  number = {1},
  pages = {1--35},
  issn = {0167-2789},
  doi = {10.1016/0167-2789(84)90245-8},
  url = {http://www.sciencedirect.com/science/article/pii/0167278984902458},
  urldate = {2019-06-25},
  abstract = {Cellular automata are discrete dynamical systems with simple construction but complex self-organizing behaviour. Evidence is presented that all one-dimensional cellular automata fall into four distinct universality classes. Characterizations of the structures generated in these classes are discussed. Three classes exhibit behaviour analogous to limit points, limit cycles and chaotic attractors. The fourth class is probably capable of universal computation, so that properties of its infinite time behaviour are undecidable.},
  file = {/Users/hugo/Zotero/storage/8YZ3QLME/wolframUniversalityComplexityCellular1984.pdf;/Users/hugo/Zotero/storage/4NW8JVMU/0167278984902458.html}
}

@article{wolpertPositionalInformationSpatial1969,
  title = {Positional Information and the Spatial Pattern of Cellular Differentiation},
  author = {Wolpert, L.},
  date = {1969-10-01},
  journaltitle = {Journal of Theoretical Biology},
  shortjournal = {Journal of Theoretical Biology},
  volume = {25},
  number = {1},
  pages = {1--47},
  issn = {0022-5193},
  doi = {10.1016/S0022-5193(69)80016-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0022519369800160},
  urldate = {2022-11-14},
  abstract = {The problem of pattern is considered in terms of how genetic information can be translated in a reliable manner to give specific and different spatial patterns of cellular differentiation. Pattern formation thus differs from molecular differentiation which is mainly concerned with the control of synthesis of specific macromolecules within cells rather than the spatial arrangement of the cells. It is suggested that there may be a universal mechanism whereby the translation of genetic information into spatial patterns of differentiation is achieved. The basis of this is a mechanism whereby the cells in a developing system may have their position specified with respect to one or more points in the system. This specification of position is positional information. Cells which have their positional information specified with respect to the same set of points constitute a field. Positional information largely determines with respect to the cells' genome and developmental history the nature of its molecular differentiation. The specification of positional information in general precedes and is independent of molecular differentiation. The concept of positional information implies a co-ordinate system and polarity is defined as the direction in which positional information is specified or measured. Rules for the specification of positional information and polarity are discussed. Pattern regulation, which is the ability of the system to form the pattern even when parts are removed, or added, and to show size invariance as in the French Flag problem, is largely dependent on the ability of the cells to change their positional information and interpret this change. These concepts are applied in some detail to early sea urchin development, hydroid regeneration, pattern formation in the insect epidermis, and the development of the chick limb. It is concluded that these concepts provide a unifying framework within which a wide variety of patterns formed from fields may be discussed, and give new meaning to classical concepts such as induction, dominance and field. The concepts direct attention towards finding mechanisms whereby position and polarity are specified, and the nature of reference points and boundaries. More specifically, it is suggested that the mechanism is required to specify the position of about 50 cells in a line, relatively reliably, in about 10 hours. The size of embryonic fields is, surprisingly, usually less than 50 cells in any direction.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/2UGV7PH4/S0022519369800160.html}
}

@article{wongBlueNoiseSampling2017,
  title = {Blue Noise Sampling Using an {{N-body}} Simulation-Based Method},
  author = {Wong, Kin-Ming and Wong, Tien-Tsin},
  date = {2017-06-01},
  journaltitle = {The Visual Computer},
  shortjournal = {Vis Comput},
  volume = {33},
  number = {6},
  pages = {823--832},
  issn = {1432-2315},
  doi = {10.1007/s00371-017-1382-9},
  url = {https://doi.org/10.1007/s00371-017-1382-9},
  urldate = {2021-09-23},
  abstract = {We present a physically based blue noise sampling approach which can be evaluated efficiently by using the N-body simulation method. A set of sample points is modeled as electrically charged particles on an imaginary 2D plane where they self-organize by movement to minimize the electrostatic force that they each experience. The resulting particles’ positions at equilibrium exhibit an equidistant neighborhood characteristic that fulfills the essential requirement of a quality blue noise point set. We propose to use the Velocity Verlet algorithm commonly used in molecular dynamics simulation as our integration method, and we apply custom adaptation to improve the convergence rate for our purpose. Our method uses the magnitude of electrical charge of particles as an intuitive control parameter of the spectral behavior of the generated blue noise point sets. We are able to obtain high-quality blue noise point sets comparable to the state-of-the-art results, and we have also implemented a simple GPU application to evaluate our method on the image stippling application.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/wongBlueNoiseSampling2017.pdf}
}

@incollection{wongthanavasuCellularAutomataPattern2013,
  title = {Cellular {{Automata}} for {{Pattern Recognition}}},
  booktitle = {Emerging {{Applications}} of {{Cellular Automata}}},
  author = {Wongthanavasu, Sartra and Ponkaew, Jetsada},
  editor = {Salcido, Alejandro},
  date = {2013-05-08},
  publisher = {{InTech}},
  doi = {10.5772/52364},
  url = {http://www.intechopen.com/books/emerging-applications-of-cellular-automata/cellular-automata-for-pattern-recognition},
  urldate = {2020-01-10},
  isbn = {978-953-51-1101-6},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/wongthanavasuCellularAutomataPattern22.pdf}
}

@inproceedings{woolleyDeleteriousEffectsPriori2011,
  title = {On the Deleterious Effects of a Priori Objectives on Evolution and Representation},
  booktitle = {Proceedings of the 13th Annual Conference on {{Genetic}} and Evolutionary Computation},
  author = {Woolley, Brian G. and Stanley, Kenneth O.},
  date = {2011-07-12},
  series = {{{GECCO}} '11},
  pages = {957--964},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2001576.2001707},
  url = {https://doi.org/10.1145/2001576.2001707},
  urldate = {2022-06-06},
  abstract = {Evolutionary algorithms are often evaluated by measuring and comparing their ability to consistently reach objectives chosen a priori by researchers. Yet recent results from experiments without explicit a priori objectives, such as in Picbreeder and with the novelty search algorithm, raise the question of whether the very act of setting an objective is exacting a subtle price. Nature provides another hint that the reigning objective-based paradigm may be obfuscating evolutionary computation's true potential; after all, many of the greatest discoveries of natural evolution, such as flight and human-level intelligence, were not set as a priori objectives at the beginning of the search. The dangerous question is whether such triumphs only result because they were not objectives. To examine this question, this paper takes the unusual experimental approach of attempting to re-evolve images that were already once evolved on Picbreeder. In effect, images that were originally discovered serendipitously become a priori objectives for a new experiment with the same algorithm. Therefore, the resulting failure to reproduce the very same results cannot be blamed on the evolutionary algorithm, setting the stage for a contemplation of the price we pay for evaluating our algorithms only for their ability to achieve preconceived objectives.},
  isbn = {978-1-4503-0557-0},
  keywords = {deception,fitness,indirect encoding,non-objective search,representations,stepping stones},
  file = {/Users/hugo/Papers/pdf/woolleyDeleteriousEffectsPriori2011.pdf}
}

@online{WordNetLexicalDatabase,
  title = {{{WordNet}} | {{A Lexical Database}} for {{English}}},
  url = {https://wordnet.princeton.edu/},
  urldate = {2018-06-11},
  file = {/Users/hugo/Zotero/storage/AMJCQAPD/wordnet.princeton.edu.html}
}

@article{wuenscheClassifyingCellularAutomata1999,
  title = {Classifying Cellular Automata Automatically: {{Finding}} Gliders, Filtering, and Relating Space-Time Patterns, Attractor Basins, and the {{Z}} Parameter},
  shorttitle = {Classifying Cellular Automata Automatically},
  author = {Wuensche, Andrew},
  date = {1999},
  journaltitle = {Complexity},
  volume = {4},
  number = {3},
  pages = {47--66},
  issn = {1099-0526},
  doi = {10.1002/(SICI)1099-0526(199901/02)4:3<47::AID-CPLX9>3.0.CO;2-V},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-0526%28199901/02%294%3A3%3C47%3A%3AAID-CPLX9%3E3.0.CO%3B2-V},
  urldate = {2020-03-30},
  abstract = {Cellular automata (CA) rules can be classified automatically for a spectrum of ordered, complex, and chaotic dynamics by a measure of the variance of input-entropy over time. Rules that support interacting gliders and related complex dynamics can be identified, giving an unlimited source for further study. The distribution of rule classes in rule-space can be shown. A byproduct of the method allows the automatic “filtering” of CA space-time patterns to show up gliders and related emergent configurations more clearly. The classification seems to correspond to our subjective judgment of space-time dynamics. There are also approximate correlations with global measures on convergence in attractor basins, characterized by the distribution of in-degree sizes in their branching structure, and to the rule parameter, Z. Based on computer experiments using the software Discrete Dynamics Lab (DDLab), this article explains the methods and presents results for 1D CA. © 1999 John Wiley \& Sons, Inc.},
  langid = {english},
  keywords = {attractor basins,cellular automata,filtering,glider dynamics,Z parameter},
  file = {/Users/hugo/Papers/pdf/wuenscheClassifyingCellularAutomata1999.pdf;/Users/hugo/Zotero/storage/Q2RUEDH4/02)4347AID-CPLX93.0.html}
}

@book{wuenscheExploringDiscreteDynamics2011,
  title = {Exploring Discrete Dynamics},
  author = {Wuensche, Andrew},
  date = {2011},
  publisher = {{Luniver Press}},
  file = {/Users/hugo/Papers/pdf/wuenscheExploringDiscreteDynamics2011.pdf;/Users/hugo/Zotero/storage/8NUAK3H9/books.html}
}

@book{wuenscheGlobalDynamicsCellular1992,
  title = {Global Dynamics of Cellular Automata: {{An}} Atlas of Basin of Attraction Fields of One-Dimensional Cellular Automata},
  shorttitle = {Global Dynamics of Cellular Automata},
  author = {Wuensche, Andrew and Lesser, Mike and Lesser, Michael J.},
  date = {1992},
  volume = {1},
  publisher = {{Andrew Wuensche}},
  file = {/Users/hugo/Zotero/storage/T4RG937V/books.html}
}

@inproceedings{wulffLearningCellularAutomaton1993,
  title = {Learning Cellular Automaton Dynamics with Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wulff, N. H. and Hertz, J. A.},
  date = {1993},
  pages = {631--638},
  file = {/Users/hugo/Zotero/storage/XFIPA3TI/wulffLearningCellularAutomaton1993.pdf}
}

@unpublished{wuMemorizingTransformers2022,
  title = {Memorizing {{Transformers}}},
  author = {Wu, Yuhuai and Rabe, Markus N. and Hutchins, DeLesley and Szegedy, Christian},
  date = {2022-03-16},
  number = {arXiv:2203.08913},
  eprint = {2203.08913},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.08913},
  url = {http://arxiv.org/abs/2203.08913},
  urldate = {2022-05-24},
  abstract = {Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/wuMemorizingTransformers2022.pdf;/Users/hugo/Zotero/storage/5L9ZCSVU/2203.html}
}

@book{wyszeckiColorScienceConcepts2000,
  title = {Color Science: Concepts and Methods, Quantitative Data, and Formulae},
  shorttitle = {Color Science},
  author = {Wyszecki, Günter and Stiles, W. S.},
  date = {2000},
  series = {Wiley Classics Library},
  edition = {Wiley classics library ed},
  publisher = {{John Wiley \& Sons}},
  location = {{New York}},
  isbn = {978-0-471-39918-6},
  pagetotal = {950},
  keywords = {Color}
}

@unpublished{xieExploringRandomlyWired2019,
  title = {Exploring {{Randomly Wired Neural Networks}} for {{Image Recognition}}},
  author = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
  date = {2019-04-08},
  eprint = {1904.01569},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.01569},
  urldate = {2019-12-13},
  abstract = {Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets [11] and DenseNets [16] is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/IP6MJQXX/xieExploringRandomlyWired2019.pdf}
}

@unpublished{xiongNystrOmformerNystr2021,
  title = {Nystr\textbackslash "omformer: {{A Nystr}}\textbackslash "om-{{Based Algorithm}} for {{Approximating Self-Attention}}},
  shorttitle = {Nystr\textbackslash "omformer},
  author = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  date = {2021-02-07},
  eprint = {2102.03902},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.03902},
  urldate = {2021-02-10},
  abstract = {Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences – a topic being actively studied in the community. To address this limitation, we propose Nystro¨mformer – a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystro¨m method to approximate standard self-attention with O(n) complexity. The scalability of Nystro¨mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystro¨mformer performs comparably, or in a few cases, even slightly better, than standard Transformer. Our code is at https://github.com/mlpen/Nystromformer.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/xiongNystrOmformerNystr2021.pdf}
}

@unpublished{xuRepresentationLearningGraphs2018,
  title = {Representation {{Learning}} on {{Graphs}} with {{Jumping Knowledge Networks}}},
  author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  date = {2018-06-09},
  eprint = {1806.03536},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.03536},
  urldate = {2018-12-28},
  abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/xuRepresentationLearningGraphs22.pdf;/Users/hugo/Zotero/storage/AQ5WPCYI/1806.html}
}

@unpublished{xuShowAttendTell2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  date = {2015-02-10},
  eprint = {1502.03044},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1502.03044},
  urldate = {2018-08-21},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/xuShowAttendTell22.pdf}
}

@inproceedings{yaegerComputationalGeneticsPhysiology1994,
  title = {Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or {{Poly World}}: {{Life}} in a New Context},
  shorttitle = {Computational Genetics, Physiology, Metabolism, Neural Systems, Learning, Vision, and Behavior or {{Poly World}}},
  booktitle = {Santa {{Fe Institute}} Studies in the {{Sciences}} of {{Complexity}}},
  author = {Yaeger, Larry},
  date = {1994},
  volume = {17},
  pages = {263--263},
  publisher = {{Addison-Wesley Publishing Co}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.2435},
  file = {/Users/hugo/Zotero/storage/7I6S292L/yaegerComputationalGeneticsPhysiology1994.pdf;/Users/hugo/Zotero/storage/WGGHHFDL/yaegerComputationalGeneticsPhysiology1994.pdf}
}

@article{yangOneModelLearning2022,
  title = {One Model for the Learning of Language},
  author = {Yang, Yuan and Piantadosi, Steven T.},
  date = {2022-02-01},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc Natl Acad Sci USA},
  volume = {119},
  number = {5},
  pages = {e2021865119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021865119},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.2021865119},
  urldate = {2022-01-26},
  abstract = {A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,                                a                 n                              ,                                                                                                                        (                         a                         b                         )                                              n                                                                                       , and                                                                                                                        \{                         a                         ,                         b                         \}                                              +                                                                                       ), context-free (e.g.,                                                                                               a                       n                                                                 b                       n                                          ,                     \,                                            a                       n                                                                 b                                                n                         +                         m                                                                                                              , and                                                                        x                                            x                       R                                                                                       ), and context-sensitive (e.g.,                                                                                               a                       n                                                                 b                       n                                                                 c                       n                                          ,                     \,                                            a                       n                                                                 b                       m                                                                 c                       n                                                                 d                       m                                                                                       , and               xx               ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/yangOneModelLearning2022.pdf}
}

@article{yangSemanticPreservingAdversarialText2021,
  title = {Semantic-{{Preserving Adversarial Text Attacks}}},
  author = {Yang, Xinghao and Liu, Weifeng and Bailey, James and Zhu, Tianqing and Tao, Dacheng and Liu, Wei},
  date = {2021},
  journaltitle = {CoRR},
  volume = {abs/2108.10015},
  eprint = {2108.10015},
  eprinttype = {arxiv},
  url = {https://arxiv.org/abs/2108.10015},
  urldate = {2022-08-02},
  archiveprefix = {arXiv}
}

@misc{yangXLNetGeneralizedAutoregressive2020,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  date = {2020-01-02},
  number = {arXiv:1906.08237},
  eprint = {1906.08237},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.08237},
  url = {http://arxiv.org/abs/1906.08237},
  urldate = {2022-07-27},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/yangXLNetGeneralizedAutoregressive2020.pdf;/Users/hugo/Zotero/storage/5UI3M5U7/1906.html}
}

@unpublished{yaoDynamicWordEmbeddings2018,
  title = {Dynamic {{Word Embeddings}} for {{Evolving Semantic Discovery}}},
  author = {Yao, Zijun and Sun, Yifan and Ding, Weicong and Rao, Nikhil and Xiong, Hui},
  date = {2018},
  eprint = {1703.00607},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {673--681},
  doi = {10.1145/3159652.3159703},
  url = {http://arxiv.org/abs/1703.00607},
  urldate = {2018-05-23},
  abstract = {Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting “alignment problem”. This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/yaoDynamicWordEmbeddings22.pdf}
}

@unpublished{yaratsImprovingSampleEfficiency2019,
  title = {Improving Sample Efficiency in Model-Free Reinforcement Learning from Images},
  author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  date = {2019},
  eprint = {1910.01741},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  file = {/Users/hugo/Papers/pdf/yaratsImprovingSampleEfficiency2019.pdf;/Users/hugo/Zotero/storage/L2WJRRQJ/1910.html}
}

@inproceedings{yatesTextRunnerOpenInformation2007,
  title = {{{TextRunner}}: {{Open Information Extraction}} on the {{Web}}},
  shorttitle = {{{TextRunner}}},
  booktitle = {Proceedings of {{Human Language Technologies}}: {{The Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{NAACL-HLT}})},
  author = {Yates, Alexander and Banko, Michele and Broadhead, Matthew and Cafarella, Michael and Etzioni, Oren and Soderland, Stephen},
  date = {2007-04},
  pages = {25--26},
  publisher = {{Association for Computational Linguistics}},
  location = {{Rochester, New York, USA}},
  url = {http://www.aclweb.org/anthology/N/N07/N07-4013},
  urldate = {2018-04-12},
  file = {/Users/hugo/Papers/pdf/yatesTextRunnerOpenInformation22.pdf}
}

@inproceedings{yeNetworkDeconvolution2020,
  title = {Network {{Deconvolution}}},
  author = {Ye, Chengxi and Evanusa, Matthew and He, Hua and Mitrokhin, Anton and Goldstein, Tom and Yorke, James A and Fermüller, Cornelia and Aloimonos, Yiannis},
  date = {2020},
  pages = {20},
  abstract = {Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the center-surround structure found in biological neurons in the visual regions of the brain. Filtering with such kernels results in a sparse representation, a desired property that has been missing in the training of neural networks. Learning from the sparse representation promotes faster convergence and superior results without the use of batch normalization. We apply our network deconvolution operation to 10 modern neural network models by replacing batch normalization within each. Extensive experiments show that the network deconvolution operation is able to deliver performance improvement in all cases on the CIFAR-10, CIFAR-100, MNIST, Fashion-MNIST, Cityscapes, and ImageNet datasets.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/yeNetworkDeconvolution2020.pdf}
}

@unpublished{yilmazConnectionistSymbolicMachineIntelligence2015,
  title = {Connectionist-{{Symbolic Machine Intelligence}} Using {{Cellular Automata}} Based {{Reservoir-Hyperdimensional Computing}}},
  author = {Yilmaz, Ozgur},
  date = {2015-04-24},
  eprint = {1503.00851},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1503.00851},
  urldate = {2020-06-23},
  abstract = {We introduce a novel framework of reservoir computing, that is capable of both connectionist machine intelligence and symbolic computation. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long short-term memory and it requires orders of magnitude less computation compared to Echo State Networks. We prove that cellular automaton reservoir holds a distributed representation of attribute statistics, which provides a more effective computation than local representation. It is possible to estimate the kernel for linear cellular automata via metric learning, that enables a much more efficient distance computation in support vector machine framework. Also, binary reservoir feature vectors can be combined using Boolean operations as in hyperdimensional computing, paving a direct way for concept building and symbolic processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies},
  file = {/Users/hugo/Papers/pdf/yilmazConnectionistSymbolicMachineIntelligence2015.pdf;/Users/hugo/Zotero/storage/UI7NQ23Y/1503.html}
}

@unpublished{yilmazReservoirComputingUsing2014,
  title = {Reservoir {{Computing}} Using {{Cellular Automata}}},
  author = {Yilmaz, Ozgür},
  date = {2014-10-01},
  eprint = {1410.0162},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.0162},
  urldate = {2020-01-10},
  abstract = {We introduce a novel framework of reservoir computing. Cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is capable of long shortterm memory and it requires orders of magnitude less computation compared to Echo State Networks. Also, for additive cellular automaton rules, reservoir features can be combined using Boolean operations, which provides a direct way for concept building and symbolic processing, and it is much more efficient compared to state-of-the-art approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/yilmazReservoirComputingUsing22.pdf}
}

@article{yilmazSymbolicComputationUsing2015,
  title = {Symbolic {{Computation Using Cellular Automata-Based Hyperdimensional Computing}}},
  author = {Yilmaz, Ozgur},
  date = {2015-10-23},
  journaltitle = {Neural Computation},
  volume = {27},
  number = {12},
  pages = {2661--2692},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00787},
  url = {https://doi.org/10.1162/NECO_a_00787},
  urldate = {2020-06-23},
  abstract = {This letter introduces a novel framework of reservoir computing that is capable of both connectionist machine intelligence and symbolic computation. A cellular automaton is used as the reservoir of dynamical systems. Input is randomly projected onto the initial conditions of automaton cells, and nonlinear computation is performed on the input via application of a rule in the automaton for a period of time. The evolution of the automaton creates a space-time volume of the automaton state space, and it is used as the reservoir. The proposed framework is shown to be capable of long-term memory, and it requires orders of magnitude less computation compared to echo state networks. As the focus of the letter, we suggest that binary reservoir feature vectors can be combined using Boolean operations as in hyperdimensional computing, paving a direct way for concept building and symbolic processing. To demonstrate the capability of the proposed system, we make analogies directly on image data by asking, What is the automobile of air?},
  file = {/Users/hugo/Zotero/storage/I94YEN58/NECO_a_00787.html}
}

@article{yorkUnderstandingJevonsParadox2016,
  title = {Understanding the {{Jevons}} Paradox},
  author = {York, Richard and McGee, Julius Alexander},
  date = {2016-01-02},
  journaltitle = {Environmental Sociology},
  volume = {2},
  number = {1},
  pages = {77--87},
  issn = {2325-1042},
  doi = {10.1080/23251042.2015.1106060},
  url = {http://www.tandfonline.com/doi/full/10.1080/23251042.2015.1106060},
  urldate = {2020-04-06},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/yorkUnderstandingJevonsParadox2016.pdf}
}

@unpublished{youngMinAtarAtariInspiredTestbed2019,
  title = {{{MinAtar}}: {{An Atari-Inspired Testbed}} for {{Thorough}} and {{Reproducible Reinforcement Learning Experiments}}},
  shorttitle = {{{MinAtar}}},
  author = {Young, Kenny and Tian, Tian},
  date = {2019-06-06},
  eprint = {1903.03176},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1903.03176},
  urldate = {2021-09-07},
  abstract = {The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/youngMinAtarAtariInspiredTestbed2019.pdf;/Users/hugo/Zotero/storage/QSPA85QX/1903.html}
}

@unpublished{yuPlenoxelsRadianceFields2021,
  title = {Plenoxels: {{Radiance Fields}} without {{Neural Networks}}},
  shorttitle = {Plenoxels},
  author = {Yu, Alex and Fridovich-Keil, Sara and Tancik, Matthew and Chen, Qinhong and Recht, Benjamin and Kanazawa, Angjoo},
  date = {2021-12-09},
  eprint = {2112.05131},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2112.05131},
  urldate = {2021-12-29},
  abstract = {We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/hugo/Papers/pdf/yuPlenoxelsRadianceFields2021.pdf;/Users/hugo/Zotero/storage/ZX24MZRG/2112.html}
}

@inproceedings{yuSampleEfficientReinforcement2018,
  title = {Towards {{Sample Efficient Reinforcement Learning}}.},
  booktitle = {{{IJCAI}}},
  author = {Yu, Yang},
  date = {2018},
  pages = {5739--5743},
  file = {/Users/hugo/Papers/pdf/yuSampleEfficientReinforcement2018.pdf}
}

@article{zadorCritiquePureLearning2019,
  title = {A Critique of Pure Learning and What Artificial Neural Networks Can Learn from Animal Brains},
  author = {Zador, Anthony M.},
  date = {2019-08-21},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {1--7},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-11786-6},
  url = {https://www.nature.com/articles/s41467-019-11786-6},
  urldate = {2019-08-22},
  abstract = {Recent gains in artificial neural networks rely heavily on large amounts of training data. Here, the author suggests that for AI to learn from animal brains, it is important to consider that animal behaviour results from brain connectivity specified in the genome through evolution, and not due to unique learning algorithms.},
  langid = {english},
  file = {/home/hugo/Papers/pdf/zadorCritiquePureLearning2019.pdf;/Users/hugo/Zotero/storage/VS8BVV3W/s41467-019-11786-6.html}
}

@misc{zaheerBigBirdTransformers2021,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  shorttitle = {Big {{Bird}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  date = {2021-01-08},
  number = {arXiv:2007.14062},
  eprint = {2007.14062},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.14062},
  url = {http://arxiv.org/abs/2007.14062},
  urldate = {2022-07-22},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zaheerBigBirdTransformers2021.pdf;/Users/hugo/Zotero/storage/VIW8FFH3/2007.html}
}

@unpublished{zaheerDeepSets2018,
  title = {Deep {{Sets}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  date = {2018-04-14},
  eprint = {1703.06114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.06114},
  urldate = {2021-02-24},
  abstract = {We study the problem of designing models for machine learning tasks defined on \textbackslash emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \textbackslash cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams \textbackslash cite\{Jung15Exploration\}, to cosmology \textbackslash cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zaheerDeepSets2018.pdf;/Users/hugo/Zotero/storage/RX8WV5XR/1703.html}
}

@unpublished{zarembaRecurrentNeuralNetwork2015,
  title = {Recurrent {{Neural Network Regularization}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  date = {2015-02-19},
  eprint = {1409.2329},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1409.2329},
  urldate = {2020-06-24},
  abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/zarembaRecurrentNeuralNetwork2015.pdf;/Users/hugo/Zotero/storage/I86MRSLH/1409.html}
}

@article{zelenySelforganizationLivingSystems1977,
  title = {Self-Organization of Living Systems: {{A}} Formal Model of Autopoiesis},
  shorttitle = {Self-Organization of Living Systems},
  author = {Zeleny, Milan},
  date = {1977},
  journaltitle = {International journal of general system},
  volume = {4},
  number = {1},
  pages = {13--28},
  publisher = {{Taylor \& Francis}},
  file = {/Users/hugo/Papers/pdf/zelenySelforganizationLivingSystems1977.pdf;/Users/hugo/Zotero/storage/VSJPDJW3/03081077708960651.html}
}

@inproceedings{zengRelationClassificationConvolutional2014,
  title = {Relation {{Classification}} via {{Convolutional Deep Neural Network}}},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Zeng, Daojian and Liu, Kang and Lai, Siwei and Zhou, Guangyou and Zhao, Jun},
  date = {2014-08},
  pages = {2335--2344},
  publisher = {{Dublin City University and Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {http://www.aclweb.org/anthology/C14-1220},
  urldate = {2018-04-24},
  file = {/Users/hugo/Papers/pdf/zengRelationClassificationConvolutional22.pdf}
}

@unpublished{zenilAlgorithmicInformationDynamics2018,
  title = {Algorithmic {{Information Dynamics}} of {{Persistent Patterns}} and {{Colliding Particles}} in the {{Game}} of {{Life}}},
  author = {Zenil, Hector and Kiani, Narsis A. and Tegnér, Jesper},
  date = {2018-02-17},
  eprint = {1802.07181},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  url = {http://arxiv.org/abs/1802.07181},
  urldate = {2019-04-22},
  abstract = {Without loss of generalisation to other systems, including possibly non-deterministic ones, we demonstrate the application of methods drawn from algorithmic information dynamics to the characterisation and classification of emergent and persistent patterns, motifs and colliding particles in Conway’s Game of Life (GoL), a cellular automaton serving as a case study illustrating the way in which such ideas can be applied to a typical discrete dynamical system. We explore the issue of local observations of closed systems whose orbits may appear open because of inaccessibility to the global rules governing the overall system. We also investigate aspects of symmetry related to complexity in the distribution of patterns that occur with high frequency in GoL (which we thus call motifs) and analyse the distribution of these motifs with a view to tracking the changes in their algorithmic probability over time. We demonstrate how the tools introduced are an alternative to other computable measures that are unable to capture changes in emergent structures in evolving complex systems that are often too small or too subtle to be properly characterised by methods such as lossless compression and Shannon entropy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Dynamical Systems,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/Users/hugo/Papers/pdf/zenilAlgorithmicInformationDynamics22.pdf}
}

@article{zenilAlgorithmicityProgrammabilityNatural2015,
  title = {Algorithmicity and Programmability in Natural Computing with the {{Game}} of {{Life}} as {\emph{in Silico}} Case Study},
  author = {Zenil, Hector},
  date = {2015-01-02},
  journaltitle = {Journal of Experimental \& Theoretical Artificial Intelligence},
  volume = {27},
  number = {1},
  pages = {109--121},
  issn = {0952-813X, 1362-3079},
  doi = {10.1080/0952813X.2014.940686},
  url = {http://www.tandfonline.com/doi/abs/10.1080/0952813X.2014.940686},
  urldate = {2019-09-03},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/WIYPYUXT/zenilAlgorithmicityProgrammabilityNatural2015.pdf}
}

@article{zenilAsymptoticBehaviorRatios2013,
  title = {Asymptotic Behavior and Ratios of Complexity in Cellular Automata},
  author = {Zenil, Hector and Villarreal-Zapata, Elena},
  date = {2013-09-01},
  journaltitle = {International Journal of Bifurcation and Chaos},
  shortjournal = {Int. J. Bifurcation Chaos},
  volume = {23},
  number = {09},
  pages = {1350159},
  issn = {0218-1274},
  doi = {10.1142/S0218127413501599},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218127413501599},
  urldate = {2019-09-04},
  abstract = {We study the asymptotic behavior of symbolic computing systems, notably one-dimensional cellular automata (CA), in order to ascertain whether and at what rate the number of complex versus simple rules dominate the rule space for increasing neighborhood range and number of symbols (or colors), and how different behavior is distributed in the spaces of different cellular automata formalisms. Using two different measures, Shannon's block entropy and Kolmogorov complexity, the latter approximated by two different methods (lossless compressibility and block decomposition), we arrive at the same trend of larger complex behavioral fractions. We also advance a notion of asymptotic and limit behavior for individual rules, both over initial conditions and runtimes, and we provide a formalization of Wolfram's classification as a limit function in terms of Kolmogorov complexity.},
  file = {/Users/hugo/Zotero/storage/UQDWLNYW/zenilAsymptoticBehaviorRatios2013.pdf;/Users/hugo/Zotero/storage/5874KRE7/S0218127413501599.html}
}

@article{zenilCodingtheoremBehaviourEmergence2018,
  title = {Coding-Theorem like Behaviour and Emergence of the Universal Distribution from Resource-Bounded Algorithmic Probability},
  author = {Zenil, Hector and Badillo, Liliana and Hernández-Orozco, Santiago and Hernández-Quiroz, Francisco},
  date = {2018-04-06},
  journaltitle = {International Journal of Parallel, Emergent and Distributed Systems},
  issn = {10.1080/17445760.2018.1448932},
  url = {https://www.tandfonline.com/doi/abs/10.1080/17445760.2018.1448932},
  urldate = {2019-04-22},
  abstract = {(2019). Coding-theorem like behaviour and emergence of the universal distribution from resource-bounded algorithmic probability. International Journal of Parallel, Emergent and Distributed Systems: Vol. 34, No. 2, pp. 161-180.},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/L7Q4BKYJ/full.html}
}

@incollection{zenilComplexBehaviourNatural2020,
  title = {On the {{Complex Behaviour}} of {{Natural}} and {{Artificial Machines}} and {{Systems}}},
  booktitle = {Metrics of {{Sensory Motor Coordination}} and {{Integration}} in {{Robots}} and {{Animals}}: {{How}} to {{Measure}} the {{Success}} of {{Bioinspired Solutions}} with {{Respect}} to Their {{Natural Models}}, and {{Against More}} ‘{{Artificial}}’ {{Solutions}}?},
  author = {Zenil, H.},
  editor = {Bonsignorio, Fabio and Messina, Elena and del Pobil, Angel P. and Hallam, John},
  options = {useprefix=true},
  date = {2020},
  series = {Cognitive {{Systems Monographs}}},
  pages = {111--125},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-14126-4_6},
  url = {https://doi.org/10.1007/978-3-030-14126-4_6},
  urldate = {2019-06-10},
  abstract = {One of the most important aims of the fields of robotics, artificial intelligence and artificial life is the design and construction of systems and machines as versatile and as reliable as living organisms at performing high level human-like tasks. But how are we to evaluate artificial systems if we are not certain how to measure these capacities in living systems, let alone how to define life or intelligence? Here I survey a concrete metric towards measuring abstract properties of natural and artificial systems, such as the ability to react to the environment and to control one’s own behaviour.},
  isbn = {978-3-030-14126-4},
  langid = {english},
  keywords = {Artificial life,Compressibility,Controllability,Kolmogorov complexity,Natural computing,Programmability,Randomness,Robotics,Systems’ behaviour,Turing test},
  file = {/Users/hugo/Zotero/storage/BTABHQWX/zenilComplexBehaviourNatural2020.pdf}
}

@article{zenilCompressionBasedInvestigationDynamical2010,
  title = {Compression-{{Based Investigation}} of the {{Dynamical Properties}} of {{Cellular Automata}} and {{Other Systems}}},
  author = {Zenil, Hector},
  date = {2010},
  journaltitle = {Complex Systems},
  volume = {19},
  number = {1},
  url = {http://www.complex-systems.com/abstracts/v19_i01_a01.html},
  urldate = {2019-06-18},
  file = {/Users/hugo/Papers/pdf/zenilCompressionBasedInvestigationDynamical22.pdf}
}

@unpublished{zenilCompressionComprehensionUnreasonable2019,
  title = {Compression Is {{Comprehension}}, and the {{Unreasonable Effectiveness}} of {{Digital Computation}} in the {{Natural World}}},
  author = {Zenil, Hector},
  date = {2019-09-03},
  eprint = {1904.10258},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1904.10258},
  urldate = {2020-02-24},
  abstract = {Chaitin’s work, in its depth and breadth, encompasses many areas of scientific and philosophical interest. It helped establish the accepted mathematical concept of randomness, which in turn is the basis of tools that I have developed to justify and quantify what I think is clear evidence of the algorithmic nature of the world. To illustrate the concept I will establish novel upper bounds of algorithmic randomness for elementary cellular automata. I will discuss how the practice of science consists in conceiving a model that starts from certain initial values, running a computable instantiation, and awaiting a result in order to determine where the system may be in a future state—in a shorter time than the time taken by the actual unfolding of the phenomenon in question. If a model does not comply with all or some of these requirements it is traditionally considered useless or even unscientific, so the more precise and faster the better. A model is thus better if it can explain more with less, which is at the core of Chaitin’s ”compression is comprehension”. I will pursue these questions related to the random versus possibly algorithmic nature of the world in two directions, drawing heavily on the work of Chaitin. I will also discuss how the algorithmic approach is related to the success of science at producing models of the world, allowing computer simulations to better understand it and make more accurate predictions and interventions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory},
  file = {/Users/hugo/Papers/pdf/zenilCompressionComprehensionUnreasonable2019.pdf}
}

@article{zenilDemystifyingShannonEntropy2020,
  title = {Towards {{Demystifying Shannon Entropy}}, {{Lossless Compression}} and {{Approaches}} to {{Statistical Machine Learning}}},
  author = {Zenil, Hector},
  date = {2020},
  journaltitle = {Proceedings},
  volume = {47},
  number = {1},
  pages = {24},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/proceedings2020047024},
  url = {https://www.mdpi.com/2504-3900/47/1/24},
  urldate = {2020-07-25},
  abstract = {Current approaches in science, including most machine and deep learning methods, rely heavily at their core on traditional statistics and information theory, but these theories are known to fail to capture certain fundamental properties of data and the world related to recursive and computable phenomena, and they are ill-equipped to deal with high-level functions such as inference, abstraction, modelling and causation, being fragile and easily deceived. How is it that some of these approaches have (apparently) been successfully applied? We explore recent attempts to adopt more powerful, albeit more difficult methods, methods based on the theories of computability and algorithmic probability, which may eventually display and grasp these higher level elements of human intelligence. We propose that a fundamental question in science regarding how to find shortcuts for faster adoption of proven mathematical tools can be answered by shortening the adoption cycle and leaving behind old practices in favour of new ones. This is the case for randomness, where science continues to cling to purely statistical tools in disentangling randomness from meaning, and is stuck in a self-deluding pattern of still privileging regression and correlation despite the fact that mathematics has made important advances to better characterise randomness that have yet to be incorporated into scientific theory and practice.},
  issue = {1},
  langid = {english},
  keywords = {algorithmic complexity,Algorithmic Information Dynamics,causality vs. correlation,feasibility,Kolmogorov complexity,LZW,machine learning,Shannon entropy},
  file = {/Users/hugo/Papers/pdf/zenilDemystifyingShannonEntropy2020.pdf;/Users/hugo/Zotero/storage/2JGI4Z6C/24.html}
}

@article{zenilDynamicQualitativeBehavior2012,
  title = {On the {{Dynamic Qualitative Behavior}} of {{Universal Computation}}},
  author = {Zenil, Hector},
  date = {2012},
  journaltitle = {Complex Systems},
  pages = {13},
  langid = {english},
  file = {/Users/hugo/Zotero/storage/S4I7ZSWK/zenilDynamicQualitativeBehavior2012.pdf}
}

@article{zenilImageCharacterizationClassification2012,
  title = {Image Characterization and Classification by Physical Complexity},
  author = {Zenil, Hector and Delahaye, Jean-Paul and Gaucherel, Cédric},
  date = {2012},
  journaltitle = {Complexity},
  volume = {17},
  number = {3},
  pages = {26--42},
  issn = {1099-0526},
  doi = {10.1002/cplx.20388},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cplx.20388},
  urldate = {2019-09-04},
  abstract = {We present a method for estimating the complexity of an image based on Bennett's concept of logical depth. Bennett identified logical depth as the appropriate measure of organized complexity, and hence as being better suited to the evaluation of the complexity of objects in the physical world. Its use results in a different, and in some sense a finer characterization than is obtained through the application of the concept of Kolmogorov complexity alone. We use this measure to classify images by their information content. The method provides a means for classifying and evaluating the complexity of objects by way of their visual representations. To the authors' knowledge, the method and application inspired by the concept of logical depth presented herein are being proposed and implemented for the first time. © 2011 Wiley Periodicals, Inc. Complexity, 2011},
  langid = {english},
  keywords = {algorithmic complexity,algorithmic randomness,Bennett's logical depth,image classification,information content},
  file = {/Users/hugo/Zotero/storage/EJWF85FE/zenilImageCharacterizationClassification2012.pdf}
}

@article{zenilTwodimensionalKolmogorovComplexity2015,
  title = {Two-Dimensional {{Kolmogorov}} Complexity and an Empirical Validation of the {{Coding}} Theorem Method by Compressibility},
  author = {Zenil, Hector and Soler-Toscano, Fernando and Delahaye, Jean-Paul and Gauvrit, Nicolas},
  date = {2015-09-30},
  journaltitle = {PeerJ Computer Science},
  shortjournal = {PeerJ Comput. Sci.},
  volume = {1},
  pages = {e23},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.23},
  url = {https://peerj.com/articles/cs-23},
  urldate = {2019-09-10},
  abstract = {We propose a measure based upon the fundamental theoretical concept in algorithmic information theory that provides a natural approach to the problem of evaluating n-dimensional complexity by using an n-dimensional deterministic Turing machine. The technique is interesting because it provides a natural algorithmic process for symmetry breaking generating complex n-dimensional structures from perfectly symmetric and fully deterministic computational rules producing a distribution of patterns as described by algorithmic probability. Algorithmic probability also elegantly connects the frequency of occurrence of a pattern with its algorithmic complexity, hence effectively providing estimations to the complexity of the generated patterns. Experiments to validate estimations of algorithmic complexity based on these concepts are presented, showing that the measure is stable in the face of some changes in computational formalism and that results are in agreement with the results obtained using lossless compression algorithms when both methods overlap in their range of applicability. We then use the output frequency of the set of 2-dimensional Turing machines to classify the algorithmic complexity of the space-time evolutions of Elementary Cellular Automata.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/zenilTwodimensionalKolmogorovComplexity22.pdf;/Users/hugo/Zotero/storage/46VN72XG/cs-23.html}
}

@article{zenilWhatNatureLikeComputation2014,
  title = {What {{Is Nature-Like Computation}}? {{A Behavioural Approach}} and a {{Notion}} of {{Programmability}}},
  shorttitle = {What {{Is Nature-Like Computation}}?},
  author = {Zenil, Hector},
  date = {2014-09-01},
  journaltitle = {Philosophy \& Technology},
  shortjournal = {Philos. Technol.},
  volume = {27},
  number = {3},
  pages = {399--421},
  issn = {2210-5441},
  doi = {10.1007/s13347-012-0095-2},
  url = {https://doi.org/10.1007/s13347-012-0095-2},
  urldate = {2019-09-05},
  abstract = {The aim of this paper is to propose an alternative behavioural definition of computation (and of a computer) based simply on whether a system is capable of reacting to the environment—the input—as reflected in a measure of programmability. This definition is intended to have relevance beyond the realm of digital computers, particularly vis-à-vis natural systems. This will be done by using an extension of a phase transition coefficient previously defined in an attempt to characterise the dynamical behaviour of cellular automata and other systems. The transition coefficient measures the sensitivity of a system to external stimuli and will be used to define the susceptibility of a system to be (efficiently) programmed.},
  langid = {english},
  keywords = {Cellular automata,Compressibility,Natural computation,Philosophy of computation,Programmability,Turing universality},
  file = {/Users/hugo/Zotero/storage/EF735KXG/zenilWhatNatureLikeComputation2014.pdf}
}

@inproceedings{zenkeContinualLearningSynaptic2017,
  title = {Continual {{Learning Through Synaptic Intelligence}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2017, {{Sydney}}, {{NSW}}, {{Australia}}, 6-11 {{August}} 2017},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  editor = {Precup, Doina and Teh, Yee Whye},
  date = {2017},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {70},
  pages = {3987--3995},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v70/zenke17a.html},
  urldate = {2022-03-04}
}

@unpublished{zhaiAttentionFreeTransformer2021,
  title = {An {{Attention Free Transformer}}},
  author = {Zhai, Shuangfei and Talbott, Walter and Srivastava, Nitish and Huang, Chen and Goh, Hanlin and Zhang, Ruixiang and Susskind, Josh},
  date = {2021-05-28},
  eprint = {2105.14103},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.14103},
  urldate = {2021-06-14},
  abstract = {We introduce Attention Free Transformer (AFT), an efficient variant of Transformers [1] that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Zotero/storage/6VR6WD8E/Zhai et al. - 2021 - An Attention Free Transformer.pdf}
}

@misc{zhangBERTScoreEvaluatingText2020,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  shorttitle = {{{BERTScore}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  date = {2020-02-24},
  number = {arXiv:1904.09675},
  eprint = {1904.09675},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09675},
  url = {http://arxiv.org/abs/1904.09675},
  urldate = {2022-08-02},
  abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/zhangBERTScoreEvaluatingText2020.pdf;/Users/hugo/Zotero/storage/AUB5W4YN/1904.html}
}

@article{zhangCharacterlevelConvolutionalNetworks2015,
  title = {Character-Level Convolutional Networks for Text Classification},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  date = {2015},
  journaltitle = {Advances in neural information processing systems},
  volume = {28},
  file = {/Users/hugo/Papers/pdf/zhangCharacterlevelConvolutionalNetworks2015.pdf;/Users/hugo/Zotero/storage/ZH8TV8GD/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html}
}

@article{zhangDeepDiveDeclarativeKnowledge2017,
  title = {{{DeepDive}} : {{Declarative Knowledge Base Construction}}},
  author = {Zhang, Ce and Ré, Christopher and Cafarella, Michael and De Sa, Christopher and Ratner, Alex and Shin, Jaeho and Wang, Feiran and Wu, Sen},
  date = {2017},
  journaltitle = {Communications of the ACM},
  volume = {60},
  number = {5},
  eprint = {24655651},
  eprinttype = {pmid},
  pages = {93--102},
  issn = {0163-5808},
  doi = {10.1145/2949741.2949756},
  url = {http://cs.stanford.edu/people/chrismre/papers/deepdive_highlight.pdf},
  abstract = {The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from un-structured data sources including emails, webpages, and pdf re-ports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and inte-gration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learn-ing are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write proba-bilistic inference algorithms; instead, one interacts by defining fea-tures or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.},
  file = {/Users/hugo/Papers/pdf/zhangDeepDiveDeclarativeKnowledge2017.pdf}
}

@misc{zhangDialoGPTLargeScaleGenerative2020,
  title = {{{DialoGPT}}: {{Large-Scale Generative Pre-training}} for {{Conversational Response Generation}}},
  shorttitle = {{{DialoGPT}}},
  author = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
  date = {2020-05-02},
  number = {arXiv:1911.00536},
  eprint = {1911.00536},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.00536},
  url = {http://arxiv.org/abs/1911.00536},
  urldate = {2022-07-22},
  abstract = {We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zhangDialoGPTLargeScaleGenerative2020.pdf;/Users/hugo/Zotero/storage/RSPPF7NQ/1911.html}
}

@misc{zhangERNIEEnhancedLanguage2019,
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  shorttitle = {{{ERNIE}}},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  date = {2019-06-04},
  number = {arXiv:1905.07129},
  eprint = {1905.07129},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.07129},
  url = {http://arxiv.org/abs/1905.07129},
  urldate = {2022-07-26},
  abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/zhangERNIEEnhancedLanguage2019.pdf;/Users/hugo/Zotero/storage/7KZLKGTV/1905.html}
}

@article{zhangEvolvingFeedforwardArtificial2019,
  title = {Evolving Feedforward Artificial Neural Networks Using a Two-Stage Approach},
  author = {Zhang, Li and Li, Hong and Kong, Xian-Guang},
  date = {2019-09-30},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {360},
  pages = {25--36},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.03.097},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231219309191},
  urldate = {2020-01-29},
  abstract = {This paper presents a two-stage approach, denoted as CBRDE-LM, to evolve the architecture and weights of feedforward artificial neural networks. In the first stage, a collaborative binary-real differential evolution (CBRDE) is used to optimize simultaneously network architecture and connection weights of an ANN by a specific individual representation and evolutionary scheme, in which the structure is indirectly represented as binary coding and connection weights are directly encoded by real-valued coding. In the second stage, based on the resulting architecture and weights of an ANN, Levenberg-Marquardt (LM) backpropagation algorithm is adopted for fine-tuning ANN weights. The performance of the two-stage approach has been evaluated on several benchmarks. The results demonstrate that the two-stage approach can fast produce compact ANNs with good generalization ability at low computational cost.},
  langid = {english},
  keywords = {Artificial neural network,Differential evolution,Evolutionary neural network,Generalization ability,Levenberg-Marquardt method},
  file = {/Users/hugo/Zotero/storage/BZW3LW7M/zhangEvolvingFeedforwardArtificial2019.pdf;/Users/hugo/Zotero/storage/C7ZJBTUQ/zhangEvolvingFeedforwardArtificial2019.pdf;/Users/hugo/Zotero/storage/LR4JRNGU/S0925231219309191.html;/Users/hugo/Zotero/storage/VEQQUYG3/S0925231219309191.html}
}

@misc{zhangOPTOpenPretrained2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  date = {2022-06-21},
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.01068},
  urldate = {2022-07-26},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zhangOPTOpenPretrained2022.pdf;/Users/hugo/Zotero/storage/PA6QRFEU/2205.html}
}

@misc{zhangPEGASUSPretrainingExtracted2020,
  title = {{{PEGASUS}}: {{Pre-training}} with {{Extracted Gap-sentences}} for {{Abstractive Summarization}}},
  shorttitle = {{{PEGASUS}}},
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  date = {2020-07-10},
  number = {arXiv:1912.08777},
  eprint = {1912.08777},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1912.08777},
  urldate = {2022-07-26},
  abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/hugo/Papers/pdf/zhangPEGASUSPretrainingExtracted2020.pdf;/Users/hugo/Zotero/storage/F6Q2UQ45/1912.html}
}

@article{zhangReversibleFluorescentDNA2012,
  title = {A Reversible Fluorescent {{DNA}} Logic Gate Based on Graphene Oxide and Its Application for Iodide Sensing},
  author = {Zhang, Min and Ye, Bang-Ce},
  date = {2012-03-14},
  journaltitle = {Chemical Communications},
  shortjournal = {Chem. Commun.},
  volume = {48},
  number = {30},
  pages = {3647--3649},
  publisher = {{The Royal Society of Chemistry}},
  issn = {1364-548X},
  doi = {10.1039/C2CC17906G},
  url = {https://pubs.rsc.org/en/content/articlelanding/2012/cc/c2cc17906g},
  urldate = {2022-03-15},
  abstract = {A simple and reliable fluorescent DNA logic gate is developed by utilizing graphene oxide as a signal transducer and mercury ions and iodide as mechanical activators.},
  langid = {english},
  file = {/Users/hugo/Papers/pdf/zhangReversibleFluorescentDNA2012.pdf;/Users/hugo/Zotero/storage/Y95V6ZIQ/c2cc17906g.html}
}

@unpublished{zhangSelfAttentionGenerativeAdversarial2018,
  title = {Self-{{Attention Generative Adversarial Networks}}},
  author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  date = {2018-05-21},
  eprint = {1805.08318},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.08318},
  urldate = {2018-12-02},
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zhangSelfAttentionGenerativeAdversarial22.pdf;/Users/hugo/Zotero/storage/MS3SB3YP/1805.html}
}

@unpublished{zhangSensitivityAnalysisPractitioners2015,
  title = {A {{Sensitivity Analysis}} of (and {{Practitioners}}' {{Guide}} to) {{Convolutional Neural Networks}} for {{Sentence Classification}}},
  author = {Zhang, Ye and Wallace, Byron},
  date = {2015-10-13},
  eprint = {1510.03820},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1510.03820},
  urldate = {2019-03-22},
  abstract = {Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/zhangSensitivityAnalysisPractitioners22.pdf}
}

@inproceedings{zhangUnderstandingDeepLearning2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2017, {{Toulon}}, {{France}}, {{April}} 24-26, 2017, {{Conference Track Proceedings}}},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=Sy8gdB9xx},
  urldate = {2022-03-07}
}

@article{zhongAligningKnowledgeText2015,
  title = {Aligning {{Knowledge}} and {{Text Embeddings}} by {{Entity Descriptions}}},
  author = {Zhong, Huaping and Zhang, Jianwen and Wang, Zhen and Wan, Hai and Chen, Zheng},
  date = {2015},
  journaltitle = {Emnlp},
  eprint = {1845553},
  eprinttype = {pmid},
  pages = {267--272},
  abstract = {We study the problem of jointly em-bedding a knowledge base and a text corpus. The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia an-chors, making the applicable scope quite limited. In this paper we propose a new alignment model based on text descrip-tions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consis-tently performs comparably or even better than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information.},
  issue = {September},
  file = {/Users/hugo/Papers/pdf/zhongAligningKnowledgeText2015.pdf}
}

@unpublished{zhongBlockQNNEfficientBlockwise2018,
  title = {{{BlockQNN}}: {{Efficient Block-wise Neural Network Architecture Generation}}},
  shorttitle = {{{BlockQNN}}},
  author = {Zhong, Zhao and Yang, Zichen and Deng, Boyang and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
  date = {2018-08-16},
  eprint = {1808.05584},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1808.05584},
  urldate = {2020-01-26},
  abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35\% top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0\% top-1 and 96.0\% top-5 on ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zhongBlockQNNEfficientBlockwise22.pdf;/Users/hugo/Zotero/storage/93A95KD2/1808.html}
}

@inproceedings{zhuUnpairedImageToImageTranslation2017,
  title = {Unpaired {{Image-To-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2017},
  pages = {2223--2232},
  url = {http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html},
  urldate = {2019-01-18},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/hugo/Papers/pdf/zhuUnpairedImageToImageTranslation22.pdf;/Users/hugo/Zotero/storage/KA2BLTV8/1703.html;/Users/hugo/Zotero/storage/RX52B7U6/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html}
}

@article{zivCompressionIndividualSequences1978,
  title = {Compression of Individual Sequences via Variable-Rate Coding},
  author = {Ziv, J. and Lempel, A.},
  date = {1978-09},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {24},
  number = {5},
  pages = {530--536},
  issn = {0018-9448},
  doi = {10.1109/TIT.1978.1055934},
  url = {http://ieeexplore.ieee.org/document/1055934/},
  urldate = {2020-03-04},
  langid = {english},
  file = {/home/hugo/Papers/pdf/zivCompressionIndividualSequences1978.pdf}
}

@article{zivUniversalAlgorithmSequential1977,
  title = {A Universal Algorithm for Sequential Data Compression},
  author = {Ziv, J. and Lempel, A.},
  date = {1977-05},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {23},
  number = {3},
  pages = {337--343},
  issn = {0018-9448},
  doi = {10.1109/TIT.1977.1055714},
  url = {http://ieeexplore.ieee.org/document/1055714/},
  urldate = {2020-03-04},
  langid = {english},
  file = {/home/hugo/Papers/pdf/zivUniversalAlgorithmSequential1977.pdf}
}

@unpublished{zophLearningTransferableArchitectures2018,
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  date = {2018-04-11},
  eprint = {1707.07012},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.07012},
  urldate = {2019-12-11},
  abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/hugo/Papers/pdf/zophLearningTransferableArchitectures2018.pdf;/Users/hugo/Zotero/storage/ZJL5MC6F/1707.html}
}

@unpublished{zophNeuralArchitectureSearch2017,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  author = {Zoph, Barret and Le, Quoc V.},
  date = {2017-02-15},
  eprint = {1611.01578},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.01578},
  urldate = {2019-12-11},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/hugo/Papers/pdf/zophNeuralArchitectureSearch22.pdf;/Users/hugo/Zotero/storage/URCDHGJ8/1611.html}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }
